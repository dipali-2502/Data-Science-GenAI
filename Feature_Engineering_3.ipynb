{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
        "\n",
        "ans. **Min-Max Scaling** is a data preprocessing technique used to normalize the range of features or variables in a dataset. It rescales the data to a fixed range—usually **\\[0, 1]**—by transforming the original values linearly.\n",
        "\n",
        "\n",
        "\n",
        "### What is Min-Max Scaling?\n",
        "\n",
        "The formula for Min-Max scaling is:\n",
        "\n",
        "$$\n",
        "X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
        "$$\n",
        "\n",
        "* $X$ = original value\n",
        "* $X_{\\min}$ = minimum value in the dataset (feature-wise)\n",
        "* $X_{\\max}$ = maximum value in the dataset (feature-wise)\n",
        "* $X_{\\text{scaled}}$ = scaled value, which will be between 0 and 1\n",
        "\n",
        "\n",
        "\n",
        "### Why Use Min-Max Scaling?\n",
        "\n",
        "* **Normalizes data:** Brings all features to the same scale, preventing features with large ranges from dominating the learning process.\n",
        "* **Improves algorithm performance:** Many ML algorithms (like KNN, neural networks) perform better with normalized data.\n",
        "* **Keeps data distribution:** It preserves the relationships and distribution of the original data (unlike some other scaling methods like standardization which centers the data).\n",
        "\n",
        "\n",
        "\n",
        "### Example:\n",
        "\n",
        "Suppose you have a dataset with one feature (height in cm) for 5 people:\n",
        "\n",
        "| Person | Height (cm) |\n",
        "| ------ | ----------- |\n",
        "| A      | 150         |\n",
        "| B      | 160         |\n",
        "| C      | 170         |\n",
        "| D      | 180         |\n",
        "| E      | 190         |\n",
        "\n",
        "* Minimum height, $X_{\\min} = 150$\n",
        "* Maximum height, $X_{\\max} = 190$\n",
        "\n",
        "Apply Min-Max scaling for person C (170 cm):\n",
        "\n",
        "$$\n",
        "X_{\\text{scaled}} = \\frac{170 - 150}{190 - 150} = \\frac{20}{40} = 0.5\n",
        "$$\n",
        "\n",
        "Similarly, scaled values for all:\n",
        "\n",
        "| Person | Height (cm) | Scaled Height       |\n",
        "| ------ | ----------- | ------------------- |\n",
        "| A      | 150         | (150-150)/40 = 0.0  |\n",
        "| B      | 160         | (160-150)/40 = 0.25 |\n",
        "| C      | 170         | 0.5                 |\n",
        "| D      | 180         | 0.75                |\n",
        "| E      | 190         | 1.0                 |\n",
        "\n"
      ],
      "metadata": {
        "id": "r_vhyC8_AVK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
        "\n",
        "\n",
        "ans.\n",
        "\n",
        "## What is Unit Vector Scaling?\n",
        "\n",
        "**Unit Vector scaling** rescales a feature vector so that its length (or magnitude) is 1. This is often done by dividing each component of the vector by its **Euclidean norm** (L2 norm).\n",
        "\n",
        "Mathematically, for a feature vector $\\mathbf{x} = [x_1, x_2, ..., x_n]$, the unit vector scaled form $\\mathbf{x}_{\\text{scaled}}$ is:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_{\\text{scaled}} = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2} = \\frac{\\mathbf{x}}{\\sqrt{x_1^2 + x_2^2 + ... + x_n^2}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## How is it used in Feature Scaling?\n",
        "\n",
        "* It scales the whole vector (all features together), not feature-wise independently.\n",
        "* Common in text mining, image processing, and other cases where the direction of the data vector matters more than its magnitude.\n",
        "* Used when we want each sample to have unit length, focusing on the *direction* rather than scale.\n",
        "\n",
        "\n",
        "\n",
        "## How is Unit Vector different from Min-Max Scaling?\n",
        "\n",
        "| Aspect    | Min-Max Scaling                                                    | Unit Vector Scaling                                        |\n",
        "| --------- | ------------------------------------------------------------------ | ---------------------------------------------------------- |\n",
        "| Purpose   | Rescales each feature independently to a range (e.g., 0 to 1)      | Scales the whole vector to have length 1                   |\n",
        "| Operation | Feature-wise: $\\frac{x - \\min}{\\max - \\min}$                       | Vector-wise: divide by vector magnitude (L2 norm)          |\n",
        "| Result    | Each feature lies in a fixed range \\[0,1]                          | Vector magnitude = 1; features keep relative ratios        |\n",
        "| Use cases | When features have different ranges and you want to normalize each | When direction of vector matters (e.g., cosine similarity) |\n",
        "\n",
        "\n",
        "\n",
        "## Example:\n",
        "\n",
        "Suppose a sample vector with three features:\n",
        "\n",
        "$$\n",
        "\\mathbf{x} = [3, 4, 0]\n",
        "$$\n",
        "\n",
        "* Compute Euclidean norm:\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{x}\\|_2 = \\sqrt{3^2 + 4^2 + 0^2} = \\sqrt{9 + 16 + 0} = \\sqrt{25} = 5\n",
        "$$\n",
        "\n",
        "* Unit Vector scaled:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_{\\text{scaled}} = \\frac{1}{5}[3, 4, 0] = [0.6, 0.8, 0]\n",
        "$$\n",
        "\n",
        "Check length of scaled vector:\n",
        "\n",
        "$$\n",
        "\\sqrt{0.6^2 + 0.8^2 + 0^2} = \\sqrt{0.36 + 0.64} = \\sqrt{1} = 1\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### In contrast, if we apply Min-Max scaling feature-wise (assuming min=0, max=5 for each feature for example):\n",
        "\n",
        "* For feature 1: $\\frac{3 - 0}{5 - 0} = 0.6$\n",
        "* For feature 2: $\\frac{4 - 0}{5 - 0} = 0.8$\n",
        "* For feature 3: $\\frac{0 - 0}{5 - 0} = 0$\n",
        "\n",
        "This example yields the same values here, but if the min and max differ per feature, Min-Max scales features independently, while unit vector scales the whole vector together to length 1.\n"
      ],
      "metadata": {
        "id": "kgkx0mjDAlIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
        "\n",
        "ans.\n",
        "##  PCA (Principal Component Analysis)?\n",
        "\n",
        "**Principal Component Analysis (PCA)** is a statistical technique used to **reduce the dimensionality** of a dataset while preserving as much of the original variance (information) as possible.\n",
        "\n",
        "* PCA transforms the original features into a new set of variables called **principal components**.\n",
        "* These components are linear combinations of the original variables.\n",
        "* The components are ordered by the amount of variance they capture from the data:\n",
        "\n",
        "  * The 1st principal component captures the most variance.\n",
        "  * The 2nd principal component captures the next most, and so on.\n",
        "* By keeping only the top $k$ components, you reduce the number of features while retaining most of the data’s important information.\n",
        "\n",
        "\n",
        "\n",
        "## How does PCA work?\n",
        "\n",
        "1. **Standardize the data** (mean=0, variance=1 for each feature).\n",
        "2. **Calculate the covariance matrix** of the features.\n",
        "3. **Compute eigenvalues and eigenvectors** of the covariance matrix.\n",
        "4. **Sort eigenvectors** by eigenvalues (variance explained).\n",
        "5. **Project the original data** onto the top $k$ eigenvectors to get a reduced representation.\n",
        "\n",
        "\n",
        "\n",
        "## Why use PCA for dimensionality reduction?\n",
        "\n",
        "* To reduce computational cost.\n",
        "* To remove noise and redundancy in data.\n",
        "* To visualize high-dimensional data in 2D or 3D.\n",
        "* To improve model performance by eliminating irrelevant features.\n",
        "\n",
        "\n",
        "\n",
        "## Example:\n",
        "\n",
        "Suppose you have a dataset with 2 features:\n",
        "\n",
        "| Sample | Feature 1 (X) | Feature 2 (Y) |\n",
        "| ------ | ------------- | ------------- |\n",
        "| 1      | 2.5           | 2.4           |\n",
        "| 2      | 0.5           | 0.7           |\n",
        "| 3      | 2.2           | 2.9           |\n",
        "| 4      | 1.9           | 2.2           |\n",
        "| 5      | 3.1           | 3.0           |\n",
        "\n",
        "* After standardizing the data and calculating covariance, PCA finds the directions (principal components) along which the data varies most.\n",
        "* The first principal component might point roughly along the line $Y = X$, capturing the main variance.\n",
        "* Instead of representing data in terms of (X, Y), we can use just the first principal component (a single dimension) which explains most variance.\n",
        "* This reduces the 2D data into 1D, simplifying the dataset while keeping most information.\n",
        "\n",
        "\n",
        "\n",
        "### Visual intuition:\n",
        "\n",
        "If points lie roughly along a line in 2D, PCA finds that line and projects all points onto it, reducing 2D data to 1D.\n",
        "\n"
      ],
      "metadata": {
        "id": "HaxzZTOQBJlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
        "\n",
        "ans.\n",
        "\n",
        "## What is Feature Extraction?\n",
        "\n",
        "* **Feature Extraction** is the process of transforming raw data into a set of new features (or variables) that are more informative, non-redundant, and useful for modeling.\n",
        "* Instead of using the original features directly, you create new features that capture the essential information.\n",
        "* It helps in reducing dimensionality, improving model performance, and handling noisy or correlated data.\n",
        "\n",
        "\n",
        "\n",
        "## Relationship Between PCA and Feature Extraction\n",
        "\n",
        "* **PCA is a popular feature extraction technique.**\n",
        "* It **extracts new features** called **principal components** by combining the original features.\n",
        "* These components capture the **most important patterns/variance** in the data.\n",
        "* PCA reduces dimensionality by keeping only a few principal components, which act as new, **informative features** for further use in machine learning or analysis.\n",
        "\n",
        "\n",
        "\n",
        "## How PCA is used for Feature Extraction?\n",
        "\n",
        "* PCA computes linear combinations of original features to form principal components.\n",
        "* Each principal component is a new feature.\n",
        "* These new features are **uncorrelated** and ordered by their importance (variance explained).\n",
        "* By selecting the top $k$ components, PCA extracts the most relevant features from the original set.\n",
        "\n",
        "\n",
        "\n",
        "## Example:\n",
        "\n",
        "Suppose you have a dataset with 3 correlated features: $X_1, X_2, X_3$.\n",
        "\n",
        "| Sample | $X_1$ | $X_2$ | $X_3$ |\n",
        "| ------ | ----- | ----- | ----- |\n",
        "| 1      | 2.0   | 2.1   | 1.9   |\n",
        "| 2      | 3.5   | 3.6   | 3.7   |\n",
        "| 3      | 1.2   | 1.1   | 1.3   |\n",
        "\n",
        "* These features might be correlated (similar information).\n",
        "* PCA analyzes the covariance among $X_1, X_2, X_3$ and creates 3 principal components $PC_1, PC_2, PC_3$.\n",
        "* Suppose the first two components explain 95% of the variance.\n",
        "* You can **extract** and use only $PC_1$ and $PC_2$ as new features, reducing dimensionality from 3 to 2.\n",
        "* These extracted features capture most of the meaningful variation in the data, while reducing redundancy.\n",
        "\n"
      ],
      "metadata": {
        "id": "GfINbKYEBgz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
        "\n",
        "\n",
        "ans.\n",
        "\n",
        "\n",
        "### Why Use Min-Max Scaling Here?\n",
        "\n",
        "* These features are on **different scales**:\n",
        "\n",
        "  * Price could range from ₹50 to ₹1000\n",
        "  * Rating might be from 1 to 5\n",
        "  * Delivery time could be from 10 to 90 minutes\n",
        "* If you feed these raw features into many machine learning algorithms, features with larger scales (like price) can **dominate** the learning process.\n",
        "* Min-Max scaling rescales all features to the **same range \\[0, 1]**, making them comparable and ensuring fair influence on the model.\n",
        "\n",
        "\n",
        "\n",
        "### How to Use Min-Max Scaling:\n",
        "\n",
        "For each feature $X$, apply:\n",
        "\n",
        "$$\n",
        "X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Step-by-Step Example:\n",
        "\n",
        "Suppose for the **price** feature:\n",
        "\n",
        "* $X_{\\min} = 50$\n",
        "* $X_{\\max} = 1000$\n",
        "\n",
        "A price of ₹200 would be scaled as:\n",
        "\n",
        "$$\n",
        "\\frac{200 - 50}{1000 - 50} = \\frac{150}{950} \\approx 0.158\n",
        "$$\n",
        "\n",
        "Similarly, for **rating** (min=1, max=5):\n",
        "\n",
        "* Rating 4 scaled as:\n",
        "\n",
        "$$\n",
        "\\frac{4 - 1}{5 - 1} = \\frac{3}{4} = 0.75\n",
        "$$\n",
        "\n",
        "For **delivery time** (min=10 min, max=90 min):\n",
        "\n",
        "* Delivery time 30 minutes scaled as:\n",
        "\n",
        "$$\n",
        "\\frac{30 - 10}{90 - 10} = \\frac{20}{80} = 0.25\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Benefits:\n",
        "\n",
        "* Helps algorithms converge faster and perform better.\n",
        "* Prevents bias toward features with larger numeric ranges.\n",
        "* Keeps the data consistent and interpretable.\n",
        "\n"
      ],
      "metadata": {
        "id": "jYtMn44RB0qK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
        "\n",
        "ans.\n",
        "\n",
        "\n",
        "### Problem Context:\n",
        "\n",
        "* You have many features like company financials (e.g., revenue, profit, debt ratios) and market trends (e.g., indices, volumes, sentiment scores).\n",
        "* These features might be **high-dimensional**, and many could be **correlated or redundant**.\n",
        "* Too many features can cause:\n",
        "\n",
        "  * Increased computational cost.\n",
        "  * Overfitting.\n",
        "  * Difficulty in interpreting the model.\n",
        "\n",
        "\n",
        "\n",
        "### How to Use PCA for Dimensionality Reduction:\n",
        "\n",
        "1. **Standardize the Data:**\n",
        "   Scale features so each has zero mean and unit variance. PCA is sensitive to the scale of data.\n",
        "\n",
        "2. **Calculate Covariance Matrix:**\n",
        "   Understand how features vary with each other.\n",
        "\n",
        "3. **Compute Eigenvalues & Eigenvectors:**\n",
        "   Identify directions (principal components) where variance is maximized.\n",
        "\n",
        "4. **Select Principal Components:**\n",
        "\n",
        "   * Sort components by explained variance.\n",
        "   * Choose top $k$ components that capture most variance (e.g., 90-95%).\n",
        "\n",
        "5. **Transform Data:**\n",
        "   Project the original dataset onto the selected principal components.\n",
        "   This creates a **new feature set** with reduced dimensions but most of the important information retained.\n",
        "\n",
        "\n",
        "\n",
        "### Benefits of PCA in Stock Price Prediction:\n",
        "\n",
        "* **Reduces Noise:** Eliminates redundant or less important features.\n",
        "* **Improves Speed:** Fewer features mean faster training and prediction.\n",
        "* **Mitigates Multicollinearity:** PCA components are uncorrelated.\n",
        "* **Simplifies Model:** Easier to interpret and less prone to overfitting.\n",
        "\n",
        "\n",
        "\n",
        "### Example:\n",
        "\n",
        "* Suppose you start with 50 financial and market features.\n",
        "* PCA reveals the first 10 components capture 92% of the total variance.\n",
        "* You keep these 10 principal components as your new features instead of all 50.\n",
        "* These 10 features summarize the original data efficiently, making your stock price model faster and possibly more accurate.\n",
        "\n"
      ],
      "metadata": {
        "id": "5pEvd3-DDIwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
        "\n",
        "\n",
        "ans.\n",
        "\n",
        "\n",
        "### Min-Max Scaling formula for custom range $[a, b]$:\n",
        "\n",
        "$$\n",
        "X_{\\text{scaled}} = a + \\frac{(X - X_{\\min})(b - a)}{X_{\\max} - X_{\\min}}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $X$ = original value\n",
        "* $X_{\\min} = 1$ (minimum value in the dataset)\n",
        "* $X_{\\max} = 20$ (maximum value in the dataset)\n",
        "* $a = -1$ (new min)\n",
        "* $b = 1$ (new max)\n",
        "\n",
        "\n",
        "### Step-by-step calculation for each value:\n",
        "\n",
        "$$\n",
        "X_{\\text{scaled}} = -1 + \\frac{(X - 1)(1 - (-1))}{20 - 1} = -1 + \\frac{(X - 1) \\times 2}{19}\n",
        "$$\n",
        "\n",
        "* For $X = 1$:\n",
        "\n",
        "$$\n",
        "-1 + \\frac{(1 - 1) \\times 2}{19} = -1 + 0 = -1\n",
        "$$\n",
        "\n",
        "* For $X = 5$:\n",
        "\n",
        "$$\n",
        "-1 + \\frac{(5 - 1) \\times 2}{19} = -1 + \\frac{4 \\times 2}{19} = -1 + \\frac{8}{19} \\approx -1 + 0.421 = -0.579\n",
        "$$\n",
        "\n",
        "* For $X = 10$:\n",
        "\n",
        "$$\n",
        "-1 + \\frac{(10 - 1) \\times 2}{19} = -1 + \\frac{9 \\times 2}{19} = -1 + \\frac{18}{19} \\approx -1 + 0.947 = -0.053\n",
        "$$\n",
        "\n",
        "* For $X = 15$:\n",
        "\n",
        "$$\n",
        "-1 + \\frac{(15 - 1) \\times 2}{19} = -1 + \\frac{14 \\times 2}{19} = -1 + \\frac{28}{19} \\approx -1 + 1.474 = 0.474\n",
        "$$\n",
        "\n",
        "* For $X = 20$:\n",
        "\n",
        "$$\n",
        "-1 + \\frac{(20 - 1) \\times 2}{19} = -1 + \\frac{19 \\times 2}{19} = -1 + 2 = 1\n",
        "$$\n",
        "\n",
        "\n",
        "### Final scaled values in $[-1,1]$ range:\n",
        "\n",
        "$$\n",
        "[-1.0, -0.579, -0.053, 0.474, 1.0]\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GDTrMXHsDhpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
        "\n",
        "ans.\n",
        "\n",
        "**\\[height, weight, age, gender, blood pressure]**\n",
        "\n",
        "\n",
        "\n",
        "### Step 1: Consider the features\n",
        "\n",
        "* **height, weight, age, blood pressure** are **numerical** features.\n",
        "* **gender** is typically a **categorical** feature (e.g., male/female), which should be encoded (e.g., 0/1) before PCA.\n",
        "\n",
        "\n",
        "\n",
        "### Step 2: Preprocess the data\n",
        "\n",
        "* Encode **gender** numerically.\n",
        "* **Standardize** all features (mean=0, variance=1), because PCA is sensitive to feature scales.\n",
        "\n",
        "\n",
        "\n",
        "### Step 3: Apply PCA\n",
        "\n",
        "* PCA will analyze the covariance among these 5 features and generate 5 principal components.\n",
        "* Each principal component is a linear combination of the original features.\n",
        "\n",
        "\n",
        "\n",
        "### Step 4: How many principal components to retain?\n",
        "\n",
        "* Usually, choose the number of components that **explain a high percentage of the variance** in the data, commonly **90-95%**.\n",
        "* For example:\n",
        "\n",
        "  * If the first 2 principal components explain 92% variance, keep 2 components.\n",
        "  * If you need more detailed representation, keep 3 or more.\n",
        "* This choice balances **dimensionality reduction** with **information preservation**.\n",
        "\n",
        "\n",
        "\n",
        "### Why reduce dimensions?\n",
        "\n",
        "* To **remove redundancy** or correlated information.\n",
        "* To **simplify** the model and reduce computational cost.\n",
        "* To potentially **improve model performance** by reducing noise.\n",
        "\n",
        "\n",
        "\n",
        "### Practical note:\n",
        "\n",
        "* After fitting PCA, look at the **explained variance ratio** output (usually from libraries like `sklearn`).\n",
        "* Plot a **scree plot** or cumulative variance plot to decide the ideal number of components.\n",
        "\n",
        "\n",
        "\n",
        "### So, for this dataset:\n",
        "\n",
        "* Start by computing explained variance.\n",
        "* Likely keep **2 or 3 principal components** to retain most info while reducing dimension from 5 to 2 or 3.\n",
        "\n"
      ],
      "metadata": {
        "id": "u-bPahnREBVM"
      }
    }
  ]
}