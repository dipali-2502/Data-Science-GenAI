{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Theoretical"
      ],
      "metadata": {
        "id": "SA-yoqBXquUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques1.  What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "\n",
        "\n",
        "ans.   Logistic Regression is a classification algorithm used to predict the probability of a binary outcome (like yes/no, 0/1, pass/fail).\n",
        "\n",
        "Instead of predicting a continuous value like linear regression, it predicts the probability that a given input belongs to a particular class (usually class 1).\n",
        "\n",
        "It uses the logistic (sigmoid) function to squeeze the output of a linear equation into a range between 0 and 1.\n",
        "\n",
        "Main Differences Between Logistic and Linear Regression:\n",
        "Logistic Regression and Linear Regression are both supervised learning algorithms but are used for different types of problems. Linear Regression is used for predicting continuous numerical values by modeling a straight-line relationship between the dependent and independent variables. It assumes that the output can take any real value, positive or negative. In contrast, Logistic Regression is used for classification problems where the output is categorical, typically binary (such as 0 or 1, yes or no). Instead of predicting a direct numerical value, it estimates the probability that a given input belongs to a particular class using the sigmoid (logistic) function, which restricts the output to a range between 0 and 1. While Linear Regression uses Mean Squared Error (MSE) as its loss function, Logistic Regression uses Log Loss (Cross-Entropy). Additionally, Logistic Regression models the relationship between inputs and the log-odds of the outcome, rather than the outcome itself. Thus, the main difference lies in the type of output they predict and the mathematical approach they use to model the data."
      ],
      "metadata": {
        "id": "GWJGz7GMqxCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques2 .What is the mathematical equation of Logistic Regression.\n",
        "\n",
        "ans.\n",
        "Logistic Regression is a statistical model used to predict the probability that a given input belongs to a particular class, usually one of two possible outcomes like \"yes\" or \"no\", \"true\" or \"false\". It starts by combining all the input features and their respective weights, along with a constant term called the bias. This combination produces a single value that can be any real number. However, since probabilities must be between 0 and 1, this value is passed through a special mathematical function called the sigmoid or logistic function. This function transforms the result into a number between 0 and 1, which represents the probability of the input belonging to the positive class. The final output of the model is this probability, which can then be converted into a class label based on a chosen threshold, usually 0.5.\n"
      ],
      "metadata": {
        "id": "c1XXmNaJr4hR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques3.  Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "ans.   We use the sigmoid function in Logistic Regression because it transforms the output of a linear equation into a value between 0 and 1, which is ideal for representing probabilities.\n",
        "\n",
        "Here’s why it’s important:\n",
        "\n",
        "Probability Output: Logistic Regression is used for classification, especially binary classification (like yes/no, 0/1). Probabilities must lie between 0 and 1, and the sigmoid function ensures that.\n",
        "\n",
        "Smooth Transition: The sigmoid function gives a smooth and gradual transition from 0 to 1, making it easier to interpret the output as the likelihood of belonging to a certain class.\n",
        "\n",
        "Decision Boundary: Because the sigmoid output is between 0 and 1, we can easily apply a threshold (commonly 0.5) to classify whether the input belongs to class 0 or class 1.\n",
        "\n",
        "Differentiable: The sigmoid function is smooth and differentiable, which is necessary for optimizing the model using gradient-based methods like gradient descent.\n"
      ],
      "metadata": {
        "id": "a6honVBZsT3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques4.  What is the cost function of Logistic Regression?\n",
        "\n",
        "ans. In Logistic Regression, the cost function used is called the Log Loss or Cross-Entropy Loss.\n",
        "\n",
        "This cost function measures how well the predicted probabilities match the actual class labels (0 or 1). Instead of using Mean Squared Error like in Linear Regression, Logistic Regression uses Log Loss because it's more suitable for classification and works well with probabilities.\n",
        "\n",
        "Here's what the cost function does in simple terms:\n",
        "\n",
        "If the model predicts a probability close to the true label (like predicting 0.9 when the actual label is 1), the cost is low.\n",
        "\n",
        "If the model predicts a probability far from the true label (like predicting 0.1 when the actual label is 1), the cost is high.\n",
        "\n",
        "The function penalizes wrong predictions more severely when the model is confident but wrong.\n",
        "\n",
        "This helps the model learn to give more accurate probabilities and make better classifications over time.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e4Vg44oesl2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ques5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "ans. Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the cost function. Overfitting happens when the model learns not just the general pattern but also the noise in the training data, leading to poor performance on new, unseen data.\n",
        "\n",
        "Why is Regularization Needed?\n",
        "Prevents Overfitting: Without regularization, the model may become too complex and fit the training data too well, including its noise and outliers.\n",
        "\n",
        "Simplifies the Model: Regularization encourages the model to keep the weights (coefficients) smaller, which results in a simpler and more generalizable model.\n",
        "\n",
        "Improves Generalization: It helps the model perform better on test data by not relying too heavily on specific features.\n",
        "\n",
        "Types of Regularization in Logistic Regression:\n",
        "L1 Regularization (Lasso): Adds the sum of the absolute values of the coefficients to the cost function. It can shrink some coefficients to zero, effectively selecting features.\n",
        "\n",
        "L2 Regularization (Ridge): Adds the sum of the squared values of the coefficients. It reduces the impact of less important features without eliminating them completely.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DxOWTKsSs4iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "quest6. Explain the difference between Lasso, Ridge, and Elastic Net regression?\n",
        "\n",
        "ans.  Lasso, Ridge, and Elastic Net are all forms of regularized linear regression techniques that help prevent overfitting by adding a penalty term to the cost function. Here’s how they differ:\n",
        "\n",
        "1. Lasso Regression (L1 Regularization)\n",
        "Penalty term: Lasso adds the sum of the absolute values of the coefficients to the cost function. This is known as L1 regularization.\n",
        "\n",
        "Effect on coefficients: Lasso can shrink some coefficients to exactly zero, effectively performing feature selection. This means it can help identify and keep only the most important features while eliminating the irrelevant ones.\n",
        "\n",
        "Use case: It’s useful when we suspect that only a few features are relevant to the model and want to perform automatic feature selection.\n",
        "\n",
        "2. Ridge Regression (L2 Regularization)\n",
        "Penalty term: Ridge adds the sum of the squared values of the coefficients to the cost function. This is known as L2 regularization.\n",
        "\n",
        "Effect on coefficients: Ridge shrinks the coefficients but never sets them to zero. It reduces the impact of less important features but keeps all features in the model.\n",
        "\n",
        "Use case: Ridge is useful when we want to keep all the features in the model but prevent the model from overfitting by controlling large coefficients.\n",
        "\n",
        "3. Elastic Net Regression (Combination of L1 and L2)\n",
        "Penalty term: Elastic Net combines both L1 and L2 regularization. The penalty term is a mix of the sum of the absolute values and the sum of the squared values of the coefficients.\n",
        "\n",
        "Effect on coefficients: Elastic Net can perform both feature selection (like Lasso) and shrinkage (like Ridge). It tends to be a good balance between the two methods and can handle situations where there are correlated features.\n",
        "\n",
        "Use case: Elastic Net is useful when there are many correlated features or when you want the benefits of both Lasso and Ridge.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cqa4oiLatQQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ques7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "ans.\n",
        "\n",
        "ans. we should consider using Elastic Net instead of Lasso or Ridge when:\n",
        "\n",
        "1. When there are many correlated features:\n",
        "Lasso may struggle when there are highly correlated features. It might arbitrarily select one feature and ignore the others, which is not ideal when these features are equally important.\n",
        "\n",
        "Elastic Net, however, performs better in such cases by keeping the correlated features together and assigning them similar coefficients.\n",
        "\n",
        "2. When you need a balance between feature selection and coefficient shrinkage:\n",
        "Lasso is excellent for feature selection because it can shrink some coefficients to zero, but it may not perform well when you have many features or when the features are correlated.\n",
        "\n",
        "Ridge is useful when you want to shrink coefficients but do not need feature selection. It keeps all features, just reducing their impact.\n",
        "\n",
        "Elastic Net combines both Lasso's feature selection and Ridge's shrinkage, giving you a balance between reducing overfitting and selecting important features.\n",
        "\n",
        "3. When the number of features is larger than the number of observations:\n",
        "In such high-dimensional problems, Elastic Net is preferable because Lasso can sometimes fail to select a meaningful subset of features, especially when the number of features is much higher than the number of observations.\n",
        "\n",
        "Elastic Net handles this well by combining the benefits of both Lasso and Ridge, ensuring better model stability.\n",
        "\n",
        "4. When you need a more flexible model:\n",
        "Elastic Net allows you to tune the mix between L1 (Lasso) and L2 (Ridge) penalties. This flexibility makes it more adaptable to different situations, particularly when you're unsure which regularization technique is best for your data.\n"
      ],
      "metadata": {
        "id": "YwQPNWwutihO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 8.  What is the impact of the regularization parameter (λ) in Logistic Regression.\n",
        "\n",
        "ans.The regularization parameter (λ) in Logistic Regression plays a crucial role in controlling the strength of the regularization applied to the model. It influences the trade-off between fitting the data well and avoiding overfitting by penalizing large coefficients. Here's the impact of λ:\n",
        "\n",
        "1. When λ is small (close to 0):\n",
        "Impact: The regularization effect is weak, and the model behaves similarly to unregularized logistic regression. It focuses more on fitting the training data, potentially leading to overfitting.\n",
        "\n",
        "Outcome: The model might have large coefficients, which could lead to poor generalization on new, unseen data.\n",
        "\n",
        "2. When λ is large (high value):\n",
        "Impact: The regularization effect is strong, and the model’s coefficients are heavily penalized. This causes the model to underfit the data, as it will try to keep the coefficients small, possibly leading to a less flexible model.\n",
        "\n",
        "Outcome: The model becomes too simple and may not capture important patterns in the data, resulting in poorer performance, especially on the training set.\n",
        "\n",
        "3. Optimal λ (moderate value):\n",
        "Impact: A moderate value of λ strikes a balance between fitting the data well and avoiding overfitting by preventing the model from relying too heavily on any particular feature.\n",
        "\n",
        "Outcome: This leads to a model that generalizes better, with smaller coefficients that don't overfit the noise in the data. It improves performance on unseen data while still capturing the key relationships.\n",
        "\n",
        "Summary of Impact:\n",
        "Small λ: The model is more likely to overfit, as the regularization effect is weak.\n",
        "\n",
        "Large λ: The model may underfit, as the regularization effect is too strong and the coefficients become very small.\n",
        "\n",
        "Optimal λ: Balances bias and variance, leading to a model that generalizes well.\n",
        "\n",
        "How to choose the best λ:\n",
        "Cross-validation: Typically, the value of λ is tuned using cross-validation. By evaluating the model’s performance for different values of λ, you can find the one that gives the best generalization to new data.\n",
        "\n",
        "In short, λ helps control the complexity"
      ],
      "metadata": {
        "id": "EHW5k58_t5Xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        "ans. Key Assumptions of Logistic Regression\n",
        "\n",
        "1. **Binary or Categorical Dependent Variable**  \n",
        "   - The response variable should be binary (e.g., 0 or 1) in binary logistic regression. Multinomial logistic regression handles multiple categories.\n",
        "\n",
        "2. **Linear Relationship Between Predictors and Logit**  \n",
        "   - Logistic regression assumes a linear relationship between the independent variables and the log odds of the dependent variable.  \n",
        "     Example:  \n",
        "     \\[\n",
        "     \\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_nX_n\n",
        "     \\]\n",
        "\n",
        "3. **Independence of Observations**  \n",
        "   - Each observation should be independent. Violations (e.g., repeated measures or time series data) can lead to incorrect inferences.\n",
        "\n",
        "4. **No Multicollinearity**  \n",
        "   - The independent variables should not be too highly correlated with each other. Multicollinearity can make the model unstable and coefficients unreliable.\n",
        "\n",
        "5. **Sufficiently Large Sample Size**  \n",
        "   - Logistic regression performs better with a large dataset, especially when the outcome is rare. Small sample sizes may lead to overfitting or poor generalization.\n",
        "\n",
        "6. **No Strong Outliers or Influential Points**  \n",
        "   - Outliers can distort the estimated coefficients and the fit of the model. Proper diagnostics should be run to identify and manage them.\n",
        "\n",
        "7. **Independent Variables Can Be Categorical or Continuous**  \n",
        "   - Logistic regression accepts both types of predictors, but categorical variables should be encoded correctly (e.g., one-hot or dummy encoding).\n"
      ],
      "metadata": {
        "id": "dPYYOl9o3Wb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "ans. ### 1. **Decision Trees**\n",
        "- Simple, interpretable models that split data based on feature values.\n",
        "- Good for non-linear relationships and mixed data types.\n",
        "- Prone to overfitting (can be mitigated with pruning).\n",
        "\n",
        "### 2. **Random Forests**\n",
        "- Ensemble of decision trees (bagging method).\n",
        "- Handles overfitting better than a single decision tree.\n",
        "- Works well with both numerical and categorical data.\n",
        "\n",
        "### 3. **Support Vector Machines (SVM)**\n",
        "- Finds the optimal boundary between classes.\n",
        "- Effective in high-dimensional spaces and with clear margins.\n",
        "- Can be extended with kernel trick for non-linear classification.\n",
        "\n",
        "### 4. **k-Nearest Neighbors (k-NN)**\n",
        "- Non-parametric, instance-based learning algorithm.\n",
        "- Makes predictions based on the closest k training examples.\n",
        "- Simple and intuitive, but can be slow with large datasets.\n",
        "\n",
        "### 5. **Naive Bayes**\n",
        "- Based on Bayes’ theorem with strong (naive) independence assumptions.\n",
        "- Works well with high-dimensional data, especially text (e.g., spam detection).\n",
        "- Fast and scalable.\n",
        "\n",
        "### 6. **Gradient Boosting Machines (e.g., XGBoost, LightGBM, CatBoost)**\n",
        "- Builds models in a stage-wise fashion by optimizing loss.\n",
        "- Often achieves top performance in classification tasks.\n",
        "- More complex and sensitive to hyperparameters.\n",
        "\n",
        "### 7. **Neural Networks (Deep Learning)**\n",
        "- Suitable for large and complex datasets (images, text, etc.).\n",
        "- Can model complex non-linear relationships.\n",
        "- Requires more data, computational resources, and tuning.\n",
        "\n",
        "### 8. **Linear Discriminant Analysis (LDA)**\n",
        "- Assumes normally distributed features and equal class covariances.\n",
        "- Projects data to a space that maximizes class separation.\n",
        "- Works well for linearly separable classes.\n"
      ],
      "metadata": {
        "id": "QUXc4e39d5jo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics?\n",
        "\n",
        "ans.\n",
        "### **Classification Evaluation Metrics**\n",
        "\n",
        "#### 1. **Accuracy**\n",
        "- **Definition**: The proportion of total correct predictions.\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "  \\]\n",
        "- **Limitation**: Can be misleading for imbalanced datasets.\n",
        "\n",
        "\n",
        "\n",
        "#### 2. **Precision**\n",
        "- **Definition**: The proportion of correctly predicted positive observations to total predicted positives.\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
        "  \\]\n",
        "- **Use Case**: Important when **false positives** are costly (e.g., spam filters).\n",
        "\n",
        "\n",
        "#### 3. **Recall (Sensitivity or True Positive Rate)**\n",
        "- **Definition**: The proportion of correctly predicted positive observations to all actual positives.\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{Recall} = \\frac{TP}{TP + FN}\n",
        "  \\]\n",
        "- **Use Case**: Important when **false negatives** are costly (e.g., disease detection).\n",
        "\n",
        "\n",
        "\n",
        "#### 4. **F1 Score**\n",
        "- **Definition**: The harmonic mean of Precision and Recall.\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "  \\]\n",
        "- **Use Case**: Best when there's an uneven class distribution and you want a balance between precision and recall.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **Confusion Matrix**\n",
        "- **Definition**: A table showing TP, TN, FP, FN values.\n",
        "- Helps visualize model performance and error types.\n",
        "  ```\n",
        "            Predicted\n",
        "             0    1\n",
        "         ----------\n",
        "      0 |  TN | FP |\n",
        "  Actual\n",
        "      1 |  FN | TP |\n",
        "  ```\n",
        "\n",
        "\n",
        "#### 6. **ROC Curve and AUC (Area Under the Curve)**\n",
        "- **ROC Curve**: Plots **True Positive Rate** vs **False Positive Rate**.\n",
        "- **AUC**: A single number summary of ROC curve performance.\n",
        "  - Closer to 1 → better classifier.\n",
        "\n",
        "\n",
        "#### 7. **Log Loss (Logarithmic Loss)**\n",
        "- **Definition**: Measures the uncertainty of predictions.\n",
        "- Penalizes confident wrong answers more heavily.\n",
        "- Lower log loss is better.\n"
      ],
      "metadata": {
        "id": "TWfOQQtkeSKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  How does class imbalance affect Logistic Regression?\n",
        "\n",
        "ans.   Class imbalance can significantly impact the performance of **Logistic Regression** and other classification models.\n",
        "\n",
        "### **Effects of Class Imbalance on Logistic Regression**\n",
        "\n",
        "#### 1. **Biased Predictions Toward the Majority Class**\n",
        "- Logistic Regression may **favor the majority class** during training.\n",
        "- This leads to high **accuracy**, but poor **recall** or **precision** for the minority class.\n",
        "- Example: In a 95:5 class ratio, predicting all as majority gives 95% accuracy but fails to capture the minority class.\n",
        "\n",
        "\n",
        "#### 2. **Poor Decision Boundary**\n",
        "- With imbalanced classes, the model struggles to **find a boundary** that separates the minority class well.\n",
        "- The result is often a **high false negative rate**—failing to detect the minority class.\n",
        "\n",
        "\n",
        "#### 3. **Misleading Performance Metrics**\n",
        "- **Accuracy** becomes a poor measure of performance.\n",
        "- You may need to rely on:\n",
        "  - **Precision**\n",
        "  - **Recall**\n",
        "  - **F1 Score**\n",
        "  - **ROC-AUC**\n",
        "  - **Confusion Matrix**\n",
        "\n",
        "\n",
        "\n",
        "#### 4. **Poor Probability Calibration**\n",
        "- Logistic Regression outputs probabilities, but in imbalanced data, these probabilities can become **skewed**.\n",
        "- The model is often **overconfident** in majority class predictions.\n",
        "\n",
        "\n",
        "### **How to Handle Class Imbalance**\n",
        "\n",
        "1. **Resampling Techniques**\n",
        "   - **Oversampling** the minority class (e.g., SMOTE)\n",
        "   - **Undersampling** the majority class\n",
        "\n",
        "2. **Use Class Weights**\n",
        "   - Adjust the penalty for misclassifying classes using `class_weight='balanced'` in scikit-learn.\n",
        "\n",
        "3. **Change Evaluation Metrics**\n",
        "   - Use **F1 Score**, **Precision-Recall Curve**, **AUC-ROC**, not just accuracy.\n",
        "\n",
        "4. **Try Alternative Models**\n",
        "   - Tree-based models like **Random Forests** or **Gradient Boosting** often handle imbalance better.\n"
      ],
      "metadata": {
        "id": "UeG4Hg6gesn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "ans.    Hyperparameter tuning is the process of selecting the best combination of external configuration settings (hyperparameters) that are not learned from the data but instead manually set or optimized before training a logistic regression model.\n",
        "\n",
        "###Key Hyperparameters in Logistic Regression\n",
        "Regularization Type (penalty)\n",
        "\n",
        "Controls the type of regularization:\n",
        "\n",
        "'l1': Lasso (sparse solutions)\n",
        "\n",
        "'l2': Ridge (smooth solutions)\n",
        "\n",
        "'elasticnet': Combination of L1 and L2\n",
        "\n",
        "'none': No regularization\n",
        "\n",
        "Regularization Strength (C)\n",
        "\n",
        "Inverse of regularization strength.\n",
        "\n",
        "Smaller C → stronger regularization.\n",
        "\n",
        "Larger C → weaker regularization.\n",
        "\n",
        "Balances model simplicity vs. overfitting.\n",
        "\n",
        "Solver (solver)\n",
        "\n",
        "Algorithm used for optimization:\n",
        "\n",
        "'liblinear' (good for small datasets)\n",
        "\n",
        "'saga' (supports L1, L2, and elastic net for large datasets)\n",
        "\n",
        "'lbfgs' (fast for L2 regularization)\n",
        "\n",
        "'newton-cg', 'sag' (useful for different data scales and regularization)\n",
        "\n",
        "Maximum Iterations (max_iter)\n",
        "\n",
        "Number of iterations for the solver to converge.\n",
        "\n",
        "Increase this if your model is not converging.\n",
        "\n",
        "Elastic Net Mixing Parameter (l1_ratio)\n",
        "\n",
        "Only used when penalty='elasticnet'.\n",
        "\n",
        "Range: 0 (Ridge) to 1 (Lasso).\n",
        "\n",
        "Determines balance between L1 and L2 regularization.\n",
        "\n",
        "\n",
        "####How to Perform Hyperparameter Tuning\n",
        "1. Manual Search\n",
        "\n",
        "Manually test various combinations.\n",
        "\n",
        "2. Grid Search (GridSearchCV)\n",
        "\n",
        "Tests all combinations of specified hyperparameter values.\n",
        "\n",
        "3. Randomized Search (RandomizedSearchCV)\n",
        "\n",
        "Randomly selects combinations from a specified range.\n",
        "\n",
        "4. Bayesian Optimization / Optuna\n",
        "\n",
        "Smarter, probabilistic approaches to efficiently search the hyperparameter space.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xhKvc896fUNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used?\n",
        " ans. Got it — here’s a clean version without emojis:\n",
        "\n",
        "\n",
        "### Solvers in Logistic Regression (scikit-learn)\n",
        "\n",
        "Logistic Regression uses different solvers to optimize its cost function. The choice of solver affects speed, accuracy, and compatibility with different penalties (like L1, L2). Here's a breakdown of the main solvers available:\n",
        "\n",
        "\n",
        "### 1. `liblinear`\n",
        "- **Algorithm**: Coordinate Descent\n",
        "- **Supports**: L1 and L2 penalties\n",
        "- **Best for**: Small datasets and binary classification\n",
        "- **Advantages**:\n",
        "  - Works with sparse data\n",
        "  - Supports L1 regularization (useful for feature selection)\n",
        "- **Limitations**:\n",
        "  - Slower with large datasets\n",
        "\n",
        "**When to use**: If your dataset is small and you want L1 regularization.\n",
        "\n",
        "\n",
        "\n",
        "### 2. `newton-cg`\n",
        "- **Algorithm**: Newton’s Method\n",
        "- **Supports**: L2 penalty only\n",
        "- **Best for**: Multiclass classification and large datasets\n",
        "- **Advantages**:\n",
        "  - Good convergence for multiclass problems\n",
        "- **Limitations**:\n",
        "  - Does not support L1 regularization\n",
        "\n",
        "**When to use**: For multiclass problems where L2 penalty is acceptable.\n",
        "\n",
        "\n",
        "### 3. `lbfgs` (default)\n",
        "- **Algorithm**: Quasi-Newton method\n",
        "- **Supports**: L2 penalty only\n",
        "- **Best for**: Medium to large datasets, especially for multiclass\n",
        "- **Advantages**:\n",
        "  - Fast convergence\n",
        "  - Robust and works well in most cases\n",
        "\n",
        "**When to use**: This is often the best general-purpose choice.\n",
        "\n",
        "\n",
        "### 4. `sag` (Stochastic Average Gradient)\n",
        "- **Algorithm**: Stochastic optimization\n",
        "- **Supports**: L2 penalty only\n",
        "- **Best for**: Very large datasets with many samples\n",
        "- **Advantages**:\n",
        "  - Faster than `lbfgs` for large datasets\n",
        "  - Supports sparse input\n",
        "\n",
        "**When to use**: For large datasets when you only need L2 regularization.\n",
        "\n",
        "\n",
        "\n",
        "### 5. `saga`\n",
        "- **Algorithm**: Stochastic optimization (variant of `sag`)\n",
        "- **Supports**: L1, L2, elasticnet\n",
        "- **Best for**: Large datasets, sparse data, and mixed regularization\n",
        "- **Advantages**:\n",
        "  - Supports all penalties\n",
        "  - Works well with sparse and large data\n",
        "\n",
        "**When to use**: If you need flexibility with regularization or have very large/sparse data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RdwR5zJshBsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "\n",
        "ans. In binary classification, **Logistic Regression** models the probability that an instance belongs to a particular class (say, class 1). For **multiclass classification**, logistic regression can be **extended using strategies** that break the problem into multiple binary classification problems.\n",
        "\n",
        "\n",
        "\n",
        "### 1. **One-vs-Rest (OvR)** – also called One-vs-All\n",
        "- This is the **default** strategy used in scikit-learn’s `LogisticRegression`.\n",
        "- For **K classes**, the model trains **K separate binary classifiers**.\n",
        "  - Each classifier distinguishes one class vs. the rest.\n",
        "- During prediction, the class whose classifier has the **highest probability** is selected.\n",
        "\n",
        "**Example**: For 3 classes (A, B, C), you train:\n",
        "- Classifier 1: A vs. not-A  \n",
        "- Classifier 2: B vs. not-B  \n",
        "- Classifier 3: C vs. not-C\n",
        "\n",
        "**Pros**:\n",
        "- Simple and efficient for most cases.\n",
        "- Works well if classes are well-separated.\n",
        "\n",
        "**Cons**:\n",
        "- Can struggle if classes overlap significantly.\n",
        "\n",
        "\n",
        "\n",
        "### 2. **Multinomial (Softmax Regression)**\n",
        "- This approach treats all classes simultaneously using the **softmax function**.\n",
        "- The model directly estimates the **probabilities of all classes together**, rather than one-vs-rest.\n",
        "- The cost function is different: it uses **cross-entropy loss** over all classes.\n",
        "- Requires solvers that support it (`lbfgs`, `newton-cg`, `saga` in scikit-learn).\n",
        "\n",
        "**Pros**:\n",
        "- More principled and accurate for true multiclass problems.\n",
        "- Captures relationships between all classes at once.\n",
        "\n",
        "**Cons**:\n",
        "- Computationally more expensive than OvR.\n",
        "- Requires more careful tuning and regularization.\n"
      ],
      "metadata": {
        "id": "1VembffRjM9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "ans.  advantages and disadvantages of Logistic Regression:\n",
        "\n",
        "\n",
        "## **Advantages of Logistic Regression**\n",
        "\n",
        "### 1. **Simple and Interpretable**\n",
        "- Easy to implement and understand.\n",
        "- Coefficients directly show the effect of features on the log-odds of the outcome.\n",
        "\n",
        "### 2. **Efficient to Train**\n",
        "- Requires less computational power compared to more complex models (e.g., neural networks).\n",
        "- Fast convergence, especially on small to medium datasets.\n",
        "\n",
        "### 3. **Works Well with Linearly Separable Data**\n",
        "- Performs well when the classes can be separated with a linear decision boundary.\n",
        "\n",
        "### 4. **Probabilistic Output**\n",
        "- Outputs class probabilities, not just hard classifications.\n",
        "- Useful for applications where you want to measure uncertainty.\n",
        "\n",
        "### 5. **Regularization Support**\n",
        "- Supports **L1**, **L2**, and **ElasticNet** regularization to prevent overfitting and handle multicollinearity.\n",
        "\n",
        "### 6. **Multiclass Extensions Available**\n",
        "- Can be extended for multiclass classification using One-vs-Rest or Softmax (Multinomial) approach.\n",
        "\n",
        "\n",
        "## **Disadvantages of Logistic Regression**\n",
        "\n",
        "### 1. **Assumes Linear Relationship**\n",
        "- Assumes a linear relationship between the independent variables and the **log-odds** of the target.\n",
        "- Not suitable for complex or highly nonlinear data unless transformed appropriately.\n",
        "\n",
        "### 2. **Not Flexible**\n",
        "- Can't naturally capture interactions and complex patterns without feature engineering.\n",
        "- Decision boundaries are linear unless non-linear features are introduced.\n",
        "\n",
        "### 3. **Sensitive to Outliers**\n",
        "- Outliers can significantly influence the model's performance, especially without regularization.\n",
        "\n",
        "### 4. **Poor Performance with High-Dimensional Data (Without Regularization)**\n",
        "- Can overfit when there are many features and relatively few observations.\n",
        "- Needs regularization to remain effective in high dimensions.\n",
        "\n",
        "### 5. **Requires Feature Scaling**\n",
        "- While not always mandatory, solvers like `sag`, `saga`, or `lbfgs` perform better when features are scaled.\n",
        "\n",
        "### 6. **Assumes No Multicollinearity**\n",
        "- Assumes that independent variables are not highly correlated with each other.\n",
        "- Violating this assumption can distort coefficient estimates.\n"
      ],
      "metadata": {
        "id": "Tt6JSd4dkEpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  What are some use cases of Logistic Regression?\n",
        "ans. **Logistic Regression** is widely used for various classification tasks across different domains due to its simplicity, interpretability, and efficiency. Here are some common **use cases**:\n",
        "\n",
        "\n",
        "### 1. **Binary Classification**\n",
        "   - **Medical Diagnosis**:\n",
        "     - Predicting whether a patient has a certain disease (e.g., diabetes, cancer) based on medical features.\n",
        "     - Example: Predicting whether a tumor is malignant or benign (1 = malignant, 0 = benign).\n",
        "   \n",
        "   - **Credit Scoring**:\n",
        "     - Assessing whether a customer will default on a loan based on financial history.\n",
        "     - Example: Predicting if a loan applicant will default (1 = default, 0 = no default).\n",
        "   \n",
        "   - **Spam Detection**:\n",
        "     - Classifying emails as spam or not spam.\n",
        "     - Example: Predicting if an email is spam (1 = spam, 0 = not spam).\n",
        "   \n",
        "   - **Churn Prediction**:\n",
        "     - Predicting whether a customer will leave (churn) a service or product based on their usage pattern and demographics.\n",
        "     - Example: Predicting whether a subscriber will cancel their subscription (1 = churn, 0 = no churn).\n",
        "\n",
        "\n",
        "\n",
        "### 2. **Multiclass Classification**\n",
        "   - **Image Recognition**:\n",
        "     - Classifying images into multiple categories (e.g., types of objects, animal species).\n",
        "     - Example: Classifying an image as one of several categories (dog, cat, bird).\n",
        "   \n",
        "   - **Document Classification**:\n",
        "     - Categorizing text documents into predefined categories (e.g., news articles into different topics).\n",
        "     - Example: Classifying news articles into topics such as politics, sports, technology, etc.\n",
        "   \n",
        "   - **Customer Segmentation**:\n",
        "     - Classifying customers into different segments based on their behavior or characteristics.\n",
        "     - Example: Classifying customers into segments such as high value, moderate value, low value based on purchase behavior.\n",
        "\n",
        "\n",
        "\n",
        "### 3. **Fraud Detection**\n",
        "   - **Banking and E-commerce Fraud**:\n",
        "     - Identifying fraudulent transactions in banking or e-commerce.\n",
        "     - Example: Predicting if a transaction is fraudulent based on factors like transaction amount, location, and user behavior.\n",
        "\n",
        "\n",
        "\n",
        "### 4. **Health and Life Sciences**\n",
        "   - **Predicting Disease Outcomes**:\n",
        "     - Predicting the likelihood of a patient developing a condition given certain risk factors.\n",
        "     - Example: Predicting whether a patient will develop heart disease based on factors like age, cholesterol, and blood pressure.\n",
        "   \n",
        "   - **Drug Efficacy Prediction**:\n",
        "     - Predicting whether a drug will be effective for a specific patient based on genetic data and treatment history.\n",
        "     - Example: Predicting whether a cancer treatment will be effective (1 = effective, 0 = ineffective).\n",
        "\n",
        "\n",
        "### 5. **Marketing and Sales**\n",
        "   - **Targeted Marketing Campaigns**:\n",
        "     - Predicting the likelihood of a customer responding to a marketing campaign.\n",
        "     - Example: Predicting whether a customer will purchase a product after receiving an advertisement (1 = purchase, 0 = no purchase).\n",
        "   \n",
        "   - **Customer Lifetime Value (CLV) Prediction**:\n",
        "     - Estimating whether a customer will become high-value based on early behavior.\n",
        "     - Example: Predicting whether a customer will remain loyal for several years.\n",
        "\n",
        "\n",
        "\n",
        "### 6. **Politics and Social Science**\n",
        "   - **Election Prediction**:\n",
        "     - Predicting voting behavior or the outcome of an election.\n",
        "     - Example: Predicting whether a person will vote for a particular party based on demographic factors.\n",
        "   \n",
        "   - **Survey Data Analysis**:\n",
        "     - Analyzing survey responses to predict a certain outcome (e.g., public opinion).\n",
        "     - Example: Predicting whether respondents will support a new policy (1 = support, 0 = oppose).\n",
        "\n",
        "\n",
        "\n",
        "### 7. **Risk Assessment and Insurance**\n",
        "   - **Insurance Claim Prediction**:\n",
        "     - Predicting whether an insurance claim is likely to occur.\n",
        "     - Example: Predicting whether a person will file a health insurance claim based on their lifestyle and medical history.\n",
        "   \n",
        "   - **Insurance Fraud Detection**:\n",
        "     - Detecting fraudulent insurance claims.\n",
        "     - Example: Predicting whether an insurance claim is fraudulent based on the history and features of the claim.\n",
        "\n",
        "\n",
        "\n",
        "### 8. **Social Media and Text Classification**\n",
        "   - **Sentiment Analysis**:\n",
        "     - Classifying social media posts or product reviews into categories like positive, neutral, or negative.\n",
        "     - Example: Predicting whether a tweet is positive or negative about a product or event.\n",
        "   \n",
        "   - **Fake News Detection**:\n",
        "     - Classifying news articles as fake or legitimate.\n",
        "     - Example: Predicting whether a news story is true or false based on textual features.\n",
        "\n",
        "\n",
        "### 9. **Sports Analytics**\n",
        "   - **Player Performance Prediction**:\n",
        "     - Predicting whether a player will perform well in a game based on past performance data.\n",
        "     - Example: Predicting whether a football player will score a goal (1 = score, 0 = no goal).\n",
        "\n",
        "### 10. **Human Resources and Recruitment**\n",
        "   - **Employee Retention Prediction**:\n",
        "     - Predicting whether an employee is likely to leave the company based on their job satisfaction and performance.\n",
        "     - Example: Predicting whether an employee will quit their job (1 = quit, 0 = stay).\n"
      ],
      "metadata": {
        "id": "RCx2LC7Ak3zD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "ans. **Softmax Regression** and **Logistic Regression** are both classification algorithms, but they differ mainly in terms of the **number of classes** they can handle and the **approach** they use for making predictions. Here's a detailed comparison:\n",
        "\n",
        "\n",
        "\n",
        "### **1. Number of Classes**\n",
        "   - **Logistic Regression**:\n",
        "     - **Binary Classification**: Logistic regression is typically used for binary classification problems, i.e., problems where the target variable has two possible classes (e.g., 0 or 1).\n",
        "     - Example: Predicting if an email is spam (1) or not spam (0).\n",
        "   \n",
        "   - **Softmax Regression (Multinomial Logistic Regression)**:\n",
        "     - **Multiclass Classification**: Softmax regression is an extension of logistic regression that can handle **multiple classes** (more than two).\n",
        "     - It assigns an instance to one of several possible classes (e.g., 0, 1, 2 for 3 classes).\n",
        "     - Example: Classifying a photo as a dog (0), cat (1), or bird (2).\n",
        "\n",
        "\n",
        "### **2. Output**\n",
        "   - **Logistic Regression**:\n",
        "     - Produces a **single probability** for the positive class (1).\n",
        "     - The output is a value between 0 and 1, representing the probability that the instance belongs to the positive class.\n",
        "     - Decision: If the output probability is above 0.5, the prediction is **class 1**, otherwise, it's **class 0**.\n",
        "   \n",
        "   - **Softmax Regression**:\n",
        "     - Outputs a **vector of probabilities** for each class in the multiclass problem.\n",
        "     - The probabilities sum up to 1, and each probability corresponds to the likelihood of an instance belonging to a specific class.\n",
        "     - Example: For 3 classes, the output might be [0.2, 0.7, 0.1], meaning the model predicts the instance most likely belongs to class 1 (since 0.7 > 0.2 and 0.1).\n",
        "\n",
        "\n",
        "\n",
        "### **3. Decision Rule**\n",
        "   - **Logistic Regression**:\n",
        "     - Uses a **threshold** (typically 0.5) to decide between two classes.\n",
        "     - If the probability of class 1 is greater than 0.5, predict class 1; otherwise, predict class 0.\n",
        "   \n",
        "   - **Softmax Regression**:\n",
        "     - Uses the **highest probability** among all classes to make the prediction.\n",
        "     - The class with the highest predicted probability is selected as the model’s output.\n",
        "\n",
        "\n",
        "### **4. Loss Function**\n",
        "   - **Logistic Regression**:\n",
        "     - The loss function used in binary logistic regression is the **logistic loss function** or **binary cross-entropy**. It measures the difference between predicted probabilities and the actual binary labels.\n",
        "   \n",
        "   - **Softmax Regression**:\n",
        "     - The loss function used in softmax regression is **categorical cross-entropy** (also known as multinomial cross-entropy). It compares the predicted probabilities for each class with the true class label (which is one-hot encoded in multiclass problems).\n",
        "\n",
        "\n",
        "\n",
        "### **5. Model Structure**\n",
        "   - **Logistic Regression**:\n",
        "     - In binary logistic regression, there is a single sigmoid function, which calculates the probability for one class, and the decision boundary is linear in nature.\n",
        "   \n",
        "   - **Softmax Regression**:\n",
        "     - Softmax regression generalizes logistic regression for multiclass problems. It uses the **softmax function** to compute the probabilities for each class based on the logits (raw scores) produced by a linear combination of features.\n",
        "\n",
        "\n",
        "\n",
        "### **6. Algorithm**\n",
        "   - **Logistic Regression**:\n",
        "     - For binary classification, logistic regression is often implemented with a **sigmoid function**, which maps a real-valued number to a probability.\n",
        "   \n",
        "   - **Softmax Regression**:\n",
        "     - Softmax regression generalizes logistic regression by applying the **softmax function** to a vector of raw scores, which gives the probability distribution over multiple classes.\n",
        "\n",
        "\n",
        "\n",
        "### **7. Computational Complexity**\n",
        "   - **Logistic Regression**:\n",
        "     - Easier and faster to compute for binary classification because there is only one logistic function.\n",
        "   \n",
        "   - **Softmax Regression**:\n",
        "     - More computationally intensive for multiclass problems because it requires calculating probabilities for all possible classes using the softmax function.\n",
        "\n",
        "\n",
        "### **Which to Use?**\n",
        "\n",
        "- **Use Logistic Regression** when you have a binary classification problem, and the decision boundary can be modeled using a linear function.\n",
        "  \n",
        "- **Use Softmax Regression** when you are dealing with a **multiclass classification problem** and need to classify instances into more than two categories.\n",
        "\n"
      ],
      "metadata": {
        "id": "WhRCv3qrlSdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "ans.\n",
        "\n",
        "- **One-vs-Rest (OvR)**:\n",
        "  - Suitable for binary classifiers, trains one classifier per class.\n",
        "  - Works well when you need **independent class models** and can afford more computational cost.\n",
        "  - May struggle with **class imbalance**.\n",
        "\n",
        "- **Softmax Regression**:\n",
        "  - Directly handles multiclass problems, outputs a probability distribution over all classes.\n",
        "  - More **efficient** with a **single model** and better for modeling class relationships.\n",
        "  - Can be sensitive to **class imbalance** but offers **calibrated probabilities**.\n",
        "\n",
        "### use of OvR  when:\n",
        "- Using binary classifiers for each class or handling class imbalance independently.\n",
        "\n",
        "### **use of  Softmax** when:\n",
        "- You want a unified approach with a **single model** and direct **probability outputs** for multiclass classification.\n"
      ],
      "metadata": {
        "id": "8JnG5ABAmWpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "ans. In Logistic Regression, the coefficients represent the relationship between each predictor variable (feature) and the log-odds of the dependent variable (outcome). Here's how to interpret them:\n",
        "\n",
        "1. Log-Odds Interpretation\n",
        "The logistic regression model predicts the log-odds (the logarithm of the odds) of the dependent variable being in class 1.\n",
        "\n",
        "The coefficient (β) for a given feature indicates how much the log-odds change with a one-unit increase in that feature, holding all other variables constant.\n",
        "\n",
        "For example:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "log(\n",
        "1−p\n",
        "p\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "Where\n",
        "𝑝\n",
        "p is the probability of class 1, and\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "1−p\n",
        "p\n",
        "​\n",
        "  is the odds of class 1.\n",
        "\n",
        "2. Exponentiated Coefficients (Odds Ratios)\n",
        "The exponentiation of the coefficient (\n",
        "𝑒\n",
        "𝛽\n",
        "e\n",
        "β\n",
        " ) gives us the odds ratio.\n",
        "\n",
        "The odds ratio describes how the odds of the outcome change with a one-unit increase in the predictor variable.\n",
        "\n",
        "Interpretation of the odds ratio:\n",
        "\n",
        "If\n",
        "𝑒\n",
        "𝛽\n",
        ">\n",
        "1\n",
        "e\n",
        "β\n",
        " >1, the predictor increases the odds of the outcome being class 1 (positive relationship).\n",
        "\n",
        "If\n",
        "𝑒\n",
        "𝛽\n",
        "<\n",
        "1\n",
        "e\n",
        "β\n",
        " <1, the predictor decreases the odds of the outcome being class 1 (negative relationship).\n",
        "\n",
        "If\n",
        "𝑒\n",
        "𝛽\n",
        "=\n",
        "1\n",
        "e\n",
        "β\n",
        " =1, the predictor has no effect on the odds.\n",
        "\n",
        "Example:\n",
        "\n",
        "If\n",
        "𝛽\n",
        "1\n",
        "=\n",
        "0.5\n",
        "β\n",
        "1\n",
        "​\n",
        " =0.5 for feature\n",
        "𝑋\n",
        "1\n",
        "X\n",
        "1\n",
        "​\n",
        " , the odds ratio is:\n",
        "\n",
        "𝑒\n",
        "𝛽\n",
        "1\n",
        "=\n",
        "𝑒\n",
        "0.5\n",
        "≈\n",
        "1.65\n",
        "e\n",
        "β\n",
        "1\n",
        "​\n",
        "\n",
        " =e\n",
        "0.5\n",
        " ≈1.65\n",
        "This means that a one-unit increase in\n",
        "𝑋\n",
        "1\n",
        "X\n",
        "1\n",
        "​\n",
        "  increases the odds of being in class 1 by 65%.\n",
        "\n",
        "3. Interpretation of Intercept (\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        " )\n",
        "The intercept\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        "  represents the log-odds of the outcome when all predictor variables are zero.\n",
        "\n",
        "It is the baseline log-odds for the outcome when the predictor values are at their reference levels (zero or baseline category).\n",
        "\n",
        "The odds ratio interpretation for the intercept is less common but still useful to understand the starting point for the prediction.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dda5A1czm0Oa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###practical"
      ],
      "metadata": {
        "id": "c8oBazYiosIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xkXHroFiou83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target variable (labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k63usacCpPXh",
        "outputId": "d0ca2439-e5e7-485e-a6a8-c1c5ecf984ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy?\n"
      ],
      "metadata": {
        "id": "gQgtpjUUpjs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target variable (labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 Regularization (Lasso): {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "n_SQVoNwp28M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients?\n",
        "\n"
      ],
      "metadata": {
        "id": "00o0q1sfp_uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target variable (labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L2 Regularization (Ridge): {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the coefficients of the model\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "id": "HlGybw8xqJpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n"
      ],
      "metadata": {
        "id": "YnMIi_tbqUsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target variable (labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with Elastic Net regularization\n",
        "# We need to specify l1_ratio, which defines the mix of L1 and L2 regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the coefficients of the model\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "id": "t2AfJN54qcJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'\n",
        "\n"
      ],
      "metadata": {
        "id": "7xJqHQeEqkxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target variable (labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with One-vs-Rest (OvR) strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with One-vs-Rest (OvR) Strategy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the coefficients of the model\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "id": "tK2CfAJ4qunu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. \"C Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy?\n",
        "\n"
      ],
      "metadata": {
        "id": "58y0ZA6Hq2xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target variable (labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],         # Regularization strength\n",
        "    'penalty': ['l1', 'l2']         # Type of regularization\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV to tune the hyperparameters\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')  # 5-fold cross-validation\n",
        "\n",
        "# Train the model using the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict the labels on the testing set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best hyperparameters and accuracy\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV6_mXLgrho1",
        "outputId": "f1705847-478d-431d-d4c4-59c3c63fb4d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 1, 'penalty': 'l2'}\n",
            "Model Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "20 fits failed out of a total of 40.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.93333333        nan 0.96666667        nan 0.94166667\n",
            "        nan 0.95      ]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy?\n",
        "\n"
      ],
      "metadata": {
        "id": "CEMrz8-IrsdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Initialize Stratified K-Fold cross-validator\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate the model using cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "print(\"Accuracy for each fold:\", scores)\n",
        "\n",
        "# Print the average accuracy across all folds\n",
        "print(f\"Average Accuracy: {np.mean(scores) * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "cgPBBnS0svkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy?\n"
      ],
      "metadata": {
        "id": "gd_0JXfqs7KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Initialize Stratified K-Fold cross-validator\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate the model using cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "print(\"Accuracy for each fold:\", scores)\n",
        "\n",
        "# Print the average accuracy across all folds\n",
        "print(f\"Average Accuracy: {np.mean(scores) * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "qbHm1gpptiIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy?\n",
        "\n"
      ],
      "metadata": {
        "id": "ifSoyLwBtjJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the parameter distribution\n",
        "param_dist = {\n",
        "    'C': uniform(loc=0.01, scale=10),  # Random values from 0.01 to 10\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']  # solvers that support both l1 and l2\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    model, param_distributions=param_dist, n_iter=10, scoring='accuracy',\n",
        "    cv=5, random_state=42, n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and parameters\n",
        "best_model = random_search.best_estimator_\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "dzW2XthtwkOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. EHM Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy?\n"
      ],
      "metadata": {
        "id": "sJLEXOibwwnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a base logistic regression model\n",
        "base_model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Wrap it in One-vs-One classifier\n",
        "ovo_model = OneVsOneClassifier(base_model)\n",
        "\n",
        "# Train the OvO model\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"One-vs-One Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "sMs6-_Abw0WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification?\n"
      ],
      "metadata": {
        "id": "MSf6-J49w7i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title('Confusion Matrix - Logistic Regression (Binary)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "mweW-WPOxCsD",
        "outputId": "0c017caa-964e-4942-e8c7-dde78ece4135"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 95.61%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score?\n",
        "\n"
      ],
      "metadata": {
        "id": "5VGIWMiexdnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy:  {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall:    {recall * 100:.2f}%\")\n",
        "print(f\"F1 Score:  {f1 * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "i9HZSkbbxlWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance?\n",
        "\n"
      ],
      "metadata": {
        "id": "GbaLez-Dxw68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=2000, n_features=20, n_informative=2,\n",
        "                           n_redundant=10, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Show class distribution\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "print(\"Class Distribution:\", dict(zip(unique, counts)))\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with class_weight='balanced'\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "v1Qc9smayEa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance?\n",
        "\n"
      ],
      "metadata": {
        "id": "1o_uxHLQyblR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load Titanic dataset from seaborn for simplicity\n",
        "import seaborn as sns\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Select relevant features\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "X = titanic[features]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Handle categorical features\n",
        "X['sex'] = LabelEncoder().fit_transform(X['sex'].astype(str))\n",
        "X['embarked'] = LabelEncoder().fit_transform(X['embarked'].astype(str))\n",
        "\n",
        "# Handle missing values using SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X[['age', 'fare']] = imputer.fit_transform(X[['age', 'fare']])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "v3o5KOdyysyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling?\n",
        "\n"
      ],
      "metadata": {
        "id": "nw05B_9ny3m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---- Without Feature Scaling ----\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# ---- With Feature Scaling ----\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression(max_iter=200)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_with_scaling.predict(X_test_scaled)\n",
        "acc_with_scaling = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# ---- Results ----\n",
        "print(f\"Accuracy without Scaling: {acc_no_scaling * 100:.2f}%\")\n",
        "print(f\"Accuracy with Scaling:    {acc_with_scaling * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "y8ESjkfizCrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score?\n"
      ],
      "metadata": {
        "id": "mVH1MbkZzyjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\", color='darkorange')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZwS_uj7n0JBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy?\n"
      ],
      "metadata": {
        "id": "KJIrF7DL0Sj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with C = 0.5\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print result\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPnO8yfT0ivE",
        "outputId": "d7d0adb6-7dec-4aa2-912d-72f7baedae1b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train Logistic Regression and identify important features based on model coefficients?\n"
      ],
      "metadata": {
        "id": "IX5hZgs00trK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get the coefficients of the model\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame to view features and their corresponding coefficients\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "# Sort the features based on the absolute value of coefficients\n",
        "feature_importance['Absolute Coefficient'] = feature_importance['Coefficient'].abs()\n",
        "feature_importance = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "# Display the most important features\n",
        "print(\"Most Important Features Based on Coefficients:\")\n",
        "print(feature_importance[['Feature', 'Coefficient']])\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Fz6MviTq1l1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score?\n"
      ],
      "metadata": {
        "id": "KPlBo4aT1oIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict values\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "0NDZKopg1y9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification?\n"
      ],
      "metadata": {
        "id": "A5ueTQ4z1-4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict values\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "N7B-ZKUR2NU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy?\n",
        "\n"
      ],
      "metadata": {
        "id": "OBzVUlm_2pLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracies = {}\n",
        "\n",
        "# Train and evaluate Logistic Regression with different solvers\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=200)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies[solver] = accuracy\n",
        "\n",
        "# Print accuracy for each solver\n",
        "for solver, accuracy in accuracies.items():\n",
        "    print(f\"Accuracy with solver '{solver}': {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "m-enapGX28GX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)?\n",
        "\n"
      ],
      "metadata": {
        "id": "VJUWSBfT2_mA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict values\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient (MCC)\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "NSNZLSXo3GOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling?\n",
        "\n"
      ],
      "metadata": {
        "id": "yA_XHxY_3N-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# 2. Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. Train Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Compare accuracy\n",
        "print(f\"Accuracy with raw data: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy with standardized data: {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "id": "yt_mW6s83XOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation?\n",
        "\n"
      ],
      "metadata": {
        "id": "aOacSAxq3p4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the range of values for C to search over\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Use GridSearchCV for cross-validation to find the best C\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the optimal value of C\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimal C value: {best_C}\")\n",
        "print(f\"Best cross-validation accuracy: {best_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "DCxSsA9C30ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n"
      ],
      "metadata": {
        "id": "pvt2LB_63_z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the range of values for C to search over\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Use GridSearchCV for cross-validation to find the best C\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the optimal value of C\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimal C value: {best_C}\")\n",
        "print(f\"Best cross-validation accuracy: {best_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "pOIQJ9hP4MkE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}