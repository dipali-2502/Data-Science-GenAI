{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#theory"
      ],
      "metadata": {
        "id": "AnBTFcYWQpTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is a Support Vector Machine (SVM)?\n",
        "\n",
        "ans. A Support Vector Machine (SVM) is a supervised machine learning algorithm that can be used for classification and regression tasks, but it is mainly used for classification.\n",
        "\n",
        "SVM works by finding the best possible boundary (called a hyperplane) that separates data points belonging to different classes. The goal is to create a boundary that separates the classes with the largest possible margin from the nearest data points of each class.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "18wBhqibQxeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "ans. ## Difference between Hard Margin and Soft Margin SVM\n",
        "\n",
        "| Aspect               | Hard Margin SVM                     | Soft Margin SVM                     |\n",
        "|---------------------|-----------------------------------|------------------------------------|\n",
        "| Definition           | Finds a hyperplane that perfectly separates the classes with no misclassifications. | Allows some misclassifications to achieve a better overall separation when data is not perfectly separable. |\n",
        "| Assumption           | Assumes data is linearly separable (no overlap between classes). | Works when data is not perfectly separable (some overlap or noise). |\n",
        "| Flexibility          | Very strict; doesn‚Äôt tolerate any misclassified points. | More flexible; balances margin maximization and classification error. |\n",
        "| Effect of Outliers   | Very sensitive to outliers. A single outlier can drastically affect the hyperplane. | Less sensitive to outliers because it allows them within a controlled limit. |\n",
        "| Real-world use       | Rarely used in real-world problems because perfect separation is uncommon. | Commonly used in practice since real-world data often has noise and overlaps. |\n",
        "| Slack Variable (Œæ)   | Does not use slack variables. | Uses slack variables to allow some points to be inside the margin or misclassified. |\n"
      ],
      "metadata": {
        "id": "Cf6fG1IvRI2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical intuition behind SVM?\n",
        "\n",
        "ans.   Mathematical intuition of SVM\n",
        "\n",
        " SVM tries to find a hyperplane that separates two classes with the maximum margin (largest distance from the nearest points of each class, called support vectors).\n",
        "\n",
        "The hyperplane equation:\n",
        "\n",
        "ùë§\n",
        "‚ãÖ\n",
        "ùë•\n",
        "+\n",
        "ùëè\n",
        "=\n",
        "0\n",
        "w‚ãÖx+b=0\n",
        "We want to:\n",
        "\n",
        "min\n",
        "‚Å°\n",
        "1\n",
        "2\n",
        "‚à£\n",
        "‚à£\n",
        "ùë§\n",
        "‚à£\n",
        "‚à£\n",
        "2\n",
        "min\n",
        "2\n",
        "1\n",
        "‚Äã\n",
        " ‚à£‚à£w‚à£‚à£\n",
        "2\n",
        "\n",
        "subject to:\n",
        "\n",
        "ùë¶\n",
        "ùëñ\n",
        "(\n",
        "ùë§\n",
        "‚ãÖ\n",
        "ùë•\n",
        "ùëñ\n",
        "+\n",
        "ùëè\n",
        ")\n",
        "‚â•\n",
        "1\n",
        "y\n",
        "i\n",
        "‚Äã\n",
        " (w‚ãÖx\n",
        "i\n",
        "‚Äã\n",
        " +b)‚â•1\n",
        "ùë§\n",
        "w: weight vector\n",
        "\n",
        "ùëè\n",
        "b: bias\n",
        "\n",
        "ùë¶\n",
        "ùëñ\n",
        "y\n",
        "i\n",
        "‚Äã\n",
        " : class label (+1 or -1)\n",
        "\n",
        "If data is not perfectly separable, we add slack variables\n",
        "ùúâ\n",
        "ùëñ\n",
        "Œæ\n",
        "i\n",
        "‚Äã\n",
        "  to allow some misclassification:\n",
        "\n",
        "min\n",
        "‚Å°\n",
        "1\n",
        "2\n",
        "‚à£\n",
        "‚à£\n",
        "ùë§\n",
        "‚à£\n",
        "‚à£\n",
        "2\n",
        "+\n",
        "ùê∂\n",
        "‚àë\n",
        "ùúâ\n",
        "ùëñ\n",
        "min\n",
        "2\n",
        "1\n",
        "‚Äã\n",
        " ‚à£‚à£w‚à£‚à£\n",
        "2\n",
        " +C‚àëŒæ\n",
        "i\n",
        "‚Äã\n",
        "\n",
        "where\n",
        "ùê∂\n",
        "C balances margin and errors.\n",
        "\n",
        " For non-linear data, we use kernel functions to map data into higher dimensions where it becomes separable.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y0sXpqD_SD3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the role of Lagrange Multipliers in SVM?\n",
        "\n",
        "ans.\n",
        "\n",
        "## **Role of Lagrange Multipliers in SVM**\n",
        "\n",
        "In Support Vector Machine, we aim to find the optimal hyperplane by solving a **constrained optimization problem**. The constraint is that all data points must be correctly classified with a margin.\n",
        "\n",
        "To solve this, we use **Lagrange multipliers ($\\alpha_i$)** to convert the problem into a **Lagrangian function**:\n",
        "\n",
        "$$\n",
        "L(w,b,\\alpha) = \\frac{1}{2} ||w||^2 - \\sum_{i=1}^{n} \\alpha_i [y_i (w \\cdot x_i + b) - 1]\n",
        "$$\n",
        "\n",
        "The Lagrange multipliers allow us to combine the objective and constraints into one equation, making it possible to apply optimization techniques.\n",
        "\n",
        "By solving the **dual form** (maximizing $L$ with respect to $\\alpha$, minimizing with respect to $w$ and $b$), we:\n",
        "\n",
        "1. Represent the solution in terms of **support vectors** (only data points with $\\alpha_i > 0$ affect the model).\n",
        "2. Enable the **kernel trick** for non-linear separation because the solution depends only on dot products between data points.\n",
        "\n",
        "In summary, Lagrange multipliers help solve the constrained optimization, focus the solution on support vectors, and allow extending SVM to non-linear problems through kernels.\n",
        "\n"
      ],
      "metadata": {
        "id": "TU2DRn3MS4Yw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What are Support Vectors in SVM?\n",
        "\n",
        "ans.\n",
        "\n",
        "---\n",
        "\n",
        "## **What are Support Vectors in SVM**\n",
        "\n",
        "Support Vectors are the **data points that lie closest to the decision boundary (hyperplane)** in a Support Vector Machine. These points are the most critical because they **‚Äúsupport‚Äù or define the position and orientation of the hyperplane**.\n",
        "\n",
        "In mathematical terms, they are the points for which the constraint:\n",
        "\n",
        "$$\n",
        "y_i (w \\cdot x_i + b) = 1\n",
        "$$\n",
        "\n",
        "holds exactly.\n",
        "\n",
        "Key characteristics:\n",
        "\n",
        "* They are the points that lie **on or inside the margin boundary**.\n",
        "* Only support vectors have **non-zero Lagrange multipliers ($\\alpha_i > 0$)**.\n",
        "* Removing other non-support vector points does **not affect** the hyperplane.\n",
        "* The SVM model is fully determined by these support vectors.\n",
        "\n",
        "In summary, support vectors are the most influential data points that directly determine the optimal separating hyperplane in SVM.\n",
        "\n"
      ],
      "metadata": {
        "id": "1RoVn0onTr1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  What is a Support Vector Classifier (SVC)?\n",
        "\n",
        "ans.\n",
        "\n",
        "## **Support Vector Classifier (SVC)**\n",
        "\n",
        "A **Support Vector Classifier (SVC)** is a type of Support Vector Machine (SVM) used for **classification problems**. It finds the **optimal hyperplane** that separates data points of different classes with the **maximum margin**, allowing some misclassifications (soft margin) if the data is not perfectly separable.\n",
        "\n",
        "Key points:\n",
        "\n",
        "* SVC aims to balance **maximizing the margin** and **minimizing classification error**.\n",
        "* It uses **slack variables** to tolerate some points being inside the margin or misclassified.\n",
        "* It can be extended to **non-linear classification** by applying **kernel functions** (e.g., linear, polynomial, RBF).\n",
        "\n",
        "In summary, SVC is an implementation of SVM for classifying data by finding the best possible decision boundary, even when classes overlap.\n",
        "\n"
      ],
      "metadata": {
        "id": "rvF-_cucUQZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.   What is a Support Vector Regressor (SVR)?\n",
        "\n",
        "ans. **Support Vector Regressor (SVR)**\n",
        "\n",
        "A **Support Vector Regressor (SVR)** is a variant of Support Vector Machines (SVM) used for **regression tasks**. Unlike classification, where the goal is to separate data into classes, SVR aims to find a **function** that approximates the relationship between input features and continuous output values.\n",
        "\n",
        "Key points:\n",
        "\n",
        "* SVR tries to find a hyperplane that **fits the data** while allowing errors within a certain **tolerance margin** (defined by a parameter $\\epsilon$).\n",
        "* It uses a **soft margin**, where data points within the margin do not contribute to the error.\n",
        "* It can be extended to **non-linear regression** by using **kernel functions** (e.g., RBF, polynomial).\n",
        "\n",
        "In summary, SVR is a regression model that seeks to approximate continuous output values by fitting a hyperplane with the least error, while allowing for some flexibility in the form of margin tolerance.\n",
        "\n"
      ],
      "metadata": {
        "id": "FhTNCXyUUo1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Kernel Trick in SVM?\n",
        "\n",
        "ans.\n",
        "\n",
        "##  Kernel Trick in SVM\n",
        "\n",
        "The **Kernel Trick** in SVM allows for the **extension of SVM to non-linear problems** without explicitly mapping the data to a higher-dimensional space. Instead of computing the transformation manually, the kernel trick computes the **inner product** in the transformed space directly using a **kernel function**.\n",
        "\n",
        "Key points:\n",
        "\n",
        "* The kernel function calculates the dot product of data points in a higher-dimensional feature space without needing to compute the transformation explicitly.\n",
        "* This enables SVM to find a **non-linear decision boundary** (hyperplane) even in original feature space.\n",
        "* Common kernel functions include:\n",
        "\n",
        "  * **Linear kernel**: $K(x, x') = x \\cdot x'$\n",
        "  * **Polynomial kernel**: $K(x, x') = (x \\cdot x' + c)^d$\n",
        "  * **Radial Basis Function (RBF) kernel**: $K(x, x') = \\exp(-\\gamma ||x - x'||^2)$\n",
        "\n",
        "In summary, the Kernel Trick allows SVM to classify data that is not linearly separable by using kernel functions to implicitly map data to higher dimensions without the computational cost of transforming every data point.\n",
        "\n"
      ],
      "metadata": {
        "id": "mygTT7taVJ47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Compare Linear Kernel, Polynomial Kernel, and RBF Kernel?\n",
        "\n",
        "ans.  Kernel Comparison:\n",
        "\n",
        "1. Linear Kernel\n",
        "- Captures Non-linearity: No\n",
        "- Pros: Fast, simple, interpretable\n",
        "- Cons: Poor performance on non-linear data\n",
        "- When to use: When data is linearly separable\n",
        "\n",
        "2. Polynomial Kernel\n",
        "- Captures Non-linearity: Yes\n",
        "- Pros: Can model moderate complexity\n",
        "- Cons: Risk of overfitting, slower, sensitive to hyperparameters\n",
        "- When to use: When data shows polynomial relationships\n",
        "\n",
        "3. RBF (Radial Basis Function) Kernel\n",
        "- Captures Non-linearity: Yes\n",
        "- Pros: Captures complex patterns, works well in most cases\n",
        "- Cons: Computationally expensive, needs tuning (gamma)\n",
        "- When to use: When data has unknown or complex boundaries\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iENXkEO9YS6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  What is the effect of the C parameter in SVM?\n",
        "\n",
        "ans. Effect of C parameter in SVM:\n",
        "\n",
        "The C parameter controls the trade-off between achieving a low training error and a low testing error (generalization).\n",
        "\n",
        "It acts as a regularization parameter that controls the penalty for misclassification.\n",
        "\n",
        "When C is large:\n",
        "\n",
        "The SVM will try to classify all training examples correctly.\n",
        "\n",
        "Results in a smaller margin.\n",
        "\n",
        "Leads to low bias but high variance.\n",
        "\n",
        "Higher risk of overfitting to the training data.\n",
        "\n",
        "When C is small:\n",
        "\n",
        "The SVM allows more misclassifications on the training data.\n",
        "\n",
        "Results in a larger margin.\n",
        "\n",
        "Leads to higher bias but lower variance.\n",
        "\n",
        "Better generalization but possibly higher training error.\n",
        "\n",
        "Summary table:\n",
        "\n",
        "C Value\tMargin Size\tTolerance to Errors\tRisk\n",
        "Large\tSmall\tLow\tOverfitting\n",
        "Small\tLarge\tHigh\tUnderfitting\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZXm1Yqb-ZF3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "\n",
        "ans. Role of Gamma in RBF Kernel SVM:\n",
        "\n",
        "Gamma defines how far the influence of a single training example reaches.\n",
        "\n",
        "It controls the curvature of the decision boundary.\n",
        "\n",
        "Technically, gamma determines the weight of each support vector's contribution based on the distance.\n",
        "\n",
        "When gamma is large:\n",
        "\n",
        "Each point‚Äôs influence is very close and localized.\n",
        "\n",
        "The model creates complex, tight decision boundaries around data points.\n",
        "\n",
        "Risk of overfitting the training data.\n",
        "\n",
        "When gamma is small:\n",
        "\n",
        "Each point‚Äôs influence reaches farther.\n",
        "\n",
        "The model creates smoother, more generalized decision boundaries.\n",
        "\n",
        "Risk of underfitting the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "SjMIeF_9Z9Pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "\n",
        "ans.\n",
        "\n",
        "**Na√Øve Bayes Classifier:**\n",
        "\n",
        "* It is a **probabilistic machine learning algorithm** based on **Bayes‚Äô Theorem**.\n",
        "* It is used for **classification tasks**.\n",
        "* The classifier predicts the probability that a data point belongs to a particular class.\n",
        "\n",
        "**Bayes‚Äô Theorem formula:**\n",
        "\n",
        "$$\n",
        "P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $P(C|X)$: probability of class $C$ given features $X$\n",
        "* $P(X|C)$: probability of features $X$ given class $C$\n",
        "* $P(C)$: prior probability of class $C$\n",
        "* $P(X)$: prior probability of features $X$\n",
        "\n",
        "\n",
        "\n",
        "**Why is it called \"Na√Øve\"?**\n",
        "\n",
        "* It assumes that **all features are independent of each other given the class label**.\n",
        "* This is a **strong and often unrealistic assumption** in real-world data.\n",
        "* In reality, features are usually **correlated**, but the algorithm ignores this dependence, treating them as **conditionally independent**.\n",
        "\n",
        "Despite this \"na√Øve\" assumption, the classifier works **surprisingly well** in many applications, especially in **text classification, spam detection, and sentiment analysis**.\n",
        "\n"
      ],
      "metadata": {
        "id": "dPnoIaOqadge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Bayes‚Äô Theorem?\n",
        "\n",
        "ans.\n",
        "\n",
        "**Bayes‚Äô Theorem** is a mathematical formula used to determine the probability of a hypothesis based on prior knowledge and new evidence.\n",
        "\n",
        "The formula is:\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $P(A|B)$: probability of event A happening given that event B has occurred (posterior probability)\n",
        "* $P(B|A)$: probability of event B happening given that event A has occurred (likelihood)\n",
        "* $P(A)$: probability of event A (prior probability)\n",
        "* $P(B)$: probability of event B (marginal probability)\n",
        "\n",
        "\n",
        "\n",
        "**Interpretation:**\n",
        "Bayes‚Äô Theorem updates the probability of a hypothesis (A) when new evidence (B) is observed.\n",
        "\n",
        "**Example:**\n",
        "If you want to know the probability that a person has a disease (A) given that they tested positive (B), Bayes' theorem helps calculate it by combining the test accuracy and the general prevalence of the disease.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NfS9dmJhd6yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes?\n",
        "\n",
        "ans.\n",
        "\n",
        "### **1. Gaussian Na√Øve Bayes**\n",
        "\n",
        "* **Assumes that features follow a normal (Gaussian) distribution**.\n",
        "* Used for **continuous input features**.\n",
        "* Calculates the likelihood of features using the Gaussian probability density function.\n",
        "\n",
        "**Example applications:**\n",
        "\n",
        "* Predicting using continuous features like age, height, weight.\n",
        "\n",
        "\n",
        "### **2. Multinomial Na√Øve Bayes**\n",
        "\n",
        "* **Assumes that features represent counts or frequencies**.\n",
        "* Used for **discrete features** (typically count data).\n",
        "* Suitable for **text classification** (e.g., word counts in documents).\n",
        "\n",
        "**Example applications:**\n",
        "\n",
        "* Document classification, spam detection (with word count vectors).\n",
        "\n",
        "\n",
        "### **3. Bernoulli Na√Øve Bayes**\n",
        "\n",
        "* **Assumes binary-valued features (0 or 1)**.\n",
        "* Models the presence or absence of a feature.\n",
        "* Focuses on **whether a word or feature occurs or not**, not how many times.\n",
        "\n",
        "**Example applications:**\n",
        "\n",
        "* Document classification where features are binary (word present or not).\n",
        "\n",
        "\n",
        "### **Key Differences Table:**\n",
        "\n",
        "| Classifier              | Feature Type      | Assumption                          | Example Use Case                         |\n",
        "| ----------------------- | ----------------- | ----------------------------------- | ---------------------------------------- |\n",
        "| Gaussian Na√Øve Bayes    | Continuous        | Features follow normal distribution | Classification with real-valued features |\n",
        "| Multinomial Na√Øve Bayes | Discrete (counts) | Features are counts or frequencies  | Text classification (word counts)        |\n",
        "| Bernoulli Na√Øve Bayes   | Binary            | Features are binary (0/1)           | Text classification (word presence)      |\n"
      ],
      "metadata": {
        "id": "4-QGh4efekd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  When should you use Gaussian Na√Øve Bayes over other variants?\n",
        "\n",
        "ans. use **Gaussian Na√Øve Bayes** over other variants when:\n",
        "\n",
        "### **1. Your Features Are Continuous and Assumed to Follow a Normal Distribution:**\n",
        "\n",
        "* **Gaussian Na√Øve Bayes** is specifically designed for continuous features that are assumed to follow a **Gaussian (normal) distribution**.\n",
        "* It works well when the data is roughly bell-shaped, i.e., the distribution of feature values follows a normal distribution.\n",
        "\n",
        "### **2. The Data is Not Count-Based or Binary:**\n",
        "\n",
        "* If your features represent **real-valued continuous data** (such as height, weight, temperature, etc.), then **Gaussian Na√Øve Bayes** is a suitable choice.\n",
        "* It‚Äôs not ideal for categorical (binary or count-based) data, for which **Bernoulli Na√Øve Bayes** or **Multinomial Na√Øve Bayes** would be more appropriate.\n",
        "\n",
        "### **3. You Have No Strong Prior Knowledge of the Data's Distribution:**\n",
        "\n",
        "* If you don't know the distribution of your data but suspect it's Gaussian or roughly normal, **Gaussian Na√Øve Bayes** is a good starting point.\n",
        "* It‚Äôs more straightforward than estimating a custom distribution for continuous features.\n",
        "\n"
      ],
      "metadata": {
        "id": "XSfo4tPFf8Vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  What are the key assumptions made by Na√Øve Bayes?\n",
        "\n",
        "ans.\n",
        "\n",
        "### **Key Assumptions of Na√Øve Bayes:**\n",
        "\n",
        "1. **Feature Independence (Conditional Independence):**\n",
        "\n",
        "   * All features are **independent of each other given the class label**.\n",
        "   * The presence (or value) of one feature **does not affect** the presence (or value) of another feature within the same class.\n",
        "\n",
        "2. **Equal Contribution of Features:**\n",
        "\n",
        "   * Each feature contributes **equally and independently** to the outcome or prediction.\n",
        "\n",
        "3. **Correct Model for Feature Distribution:**\n",
        "\n",
        "   * Assumes a specific distribution for features:\n",
        "\n",
        "     * **Gaussian distribution** for Gaussian Na√Øve Bayes (continuous features).\n",
        "     * **Multinomial distribution** for Multinomial Na√Øve Bayes (count features).\n",
        "     * **Bernoulli distribution** for Bernoulli Na√Øve Bayes (binary features).\n",
        "\n",
        "4. **No Missing Values in Data:**\n",
        "\n",
        "   * Assumes that **all features are observed** (no missing values) or missing values are properly handled.\n",
        "\n",
        "\n",
        "\n",
        "### **Why it‚Äôs called ‚ÄúNa√Øve‚Äù:**\n",
        "\n",
        "Because the assumption of **feature independence is often unrealistic in real-world data**, but the model still performs well in many scenarios.\n",
        "\n",
        "Let me know if you'd like an example illustrating these assumptions!\n",
        "\n"
      ],
      "metadata": {
        "id": "UPf_6bcrstIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  What are the advantages and disadvantages of Na√Øve Bayes?\n",
        "\n",
        "ans.\n",
        "\n",
        "### **Advantages of Na√Øve Bayes:**\n",
        "\n",
        "1. **Simple and fast to train:** Works well even with small datasets.\n",
        "2. **Efficient for large datasets:** Scales well with high-dimensional data.\n",
        "3. **Performs well with categorical and text data:** Especially effective for tasks like spam filtering, sentiment analysis, and document classification.\n",
        "4. **Requires less training data:** Handles irrelevant features well due to independence assumption.\n",
        "5. **Handles multiclass classification easily.**\n",
        "\n",
        "\n",
        "\n",
        "### **Disadvantages of Na√Øve Bayes:**\n",
        "\n",
        "1. **Strong independence assumption:** Assumes features are independent, which is rarely true in real-world data.\n",
        "2. **Not suitable for highly correlated features:** Performance degrades if features are dependent.\n",
        "3. **Zero probability problem:** If a category in test data wasn‚Äôt seen in training data, it assigns zero probability (can be mitigated using smoothing techniques).\n",
        "4. **May not give accurate probability estimates:** Good for classification, but predicted probabilities may not be reliable.\n",
        "\n",
        "\n",
        "\n",
        "### **When to use:**\n",
        "\n",
        "* Works well as a baseline model or for text-based classification tasks.\n",
        "* Less suitable when feature interactions or dependencies are important.\n",
        "\n",
        "Let me know if you need application examples or comparisons with other models!\n",
        "\n"
      ],
      "metadata": {
        "id": "9C1DqFsHtJCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  Why is Na√Øve Bayes a good choice for text classification?\n",
        "\n",
        "ans.\n",
        "### **Why Na√Øve Bayes is effective for text classification:**\n",
        "\n",
        "1. **High dimensionality handling:**\n",
        "\n",
        "   * Text data has thousands of features (words), but Na√Øve Bayes handles **high-dimensional sparse data efficiently**.\n",
        "\n",
        "2. **Feature independence assumption fits well:**\n",
        "\n",
        "   * In text, words are often treated as **independent features** (bag-of-words model), aligning with Na√Øve Bayes' independence assumption.\n",
        "\n",
        "3. **Simple and fast:**\n",
        "\n",
        "   * Training and prediction are **computationally efficient**, even for large datasets.\n",
        "\n",
        "4. **Works well with small amounts of data:**\n",
        "\n",
        "   * Performs well **even with limited labeled data**.\n",
        "\n",
        "5. **Performs well in practice:**\n",
        "\n",
        "   * Despite its simplicity, Na√Øve Bayes often achieves **high accuracy in spam detection, sentiment analysis, document classification, etc.**\n",
        "\n",
        "6. **Handles irrelevant features:**\n",
        "\n",
        "   * Irrelevant words (features) have minimal impact due to the probabilistic approach.\n",
        "\n"
      ],
      "metadata": {
        "id": "-aTmG0J0tbo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  Compare SVM and Na√Øve Bayes for classification tasks?\n",
        "\n",
        "ans.\n",
        "\n",
        "### **Comparison of SVM vs. Na√Øve Bayes**\n",
        "\n",
        "| Aspect                          | **SVM**                                         | **Na√Øve Bayes**                               |\n",
        "| ------------------------------- | ----------------------------------------------- | --------------------------------------------- |\n",
        "| **Type of Model**               | Discriminative                                  | Generative                                    |\n",
        "| **Approach**                    | Finds optimal decision boundary (margin)        | Uses Bayes theorem with feature probabilities |\n",
        "| **Feature Dependency**          | Can handle correlated features                  | Assumes feature independence                  |\n",
        "| **Performance**                 | High accuracy, especially with non-linear data  | Works well for text & categorical data        |\n",
        "| **Training Time**               | Slower (esp. on large datasets)                 | Very fast                                     |\n",
        "| **Interpretability**            | Less interpretable                              | Easy to interpret probabilities               |\n",
        "| **Handles High Dimensions**     | Good                                            | Very good                                     |\n",
        "| **Output**                      | Class labels (can be extended to probabilities) | Probabilities & class labels                  |\n",
        "| **Sensitive to Noise/Outliers** | Sensitive                                       | Less sensitive                                |\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use:**\n",
        "\n",
        " **Use Na√Øve Bayes** when:\n",
        "\n",
        "* Features are **independent or loosely correlated**\n",
        "* Working with **text data, spam filtering, document classification**\n",
        "* You need a **simple, fast baseline model**\n",
        "\n",
        " **Use SVM** when:\n",
        "\n",
        "* Data is **complex, high-dimensional, and not linearly separable**\n",
        "* You need **higher accuracy and better boundary fitting**\n",
        "* Feature relationships are **important**\n",
        "\n"
      ],
      "metadata": {
        "id": "WiDtG9YKtq2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How does Laplace Smoothing help in Na√Øve Bayes?\n",
        "\n",
        "ans.\n",
        "\n",
        "### **How does Laplace Smoothing help in Na√Øve Bayes?**\n",
        "\n",
        "**1. Problem:**\n",
        "In Na√Øve Bayes, if a feature (like a word) never appears with a certain class in the training data, its probability becomes **zero**.\n",
        "Since probabilities are multiplied in Na√Øve Bayes, this **zero makes the entire probability zero**, even if other features suggest the class.\n",
        "\n",
        "\n",
        "\n",
        "**2. Solution ‚Äì Laplace Smoothing (Add-One Smoothing):**\n",
        "Laplace smoothing adds a small positive number (usually 1) to **every count** to avoid zero probabilities.\n",
        "The formula changes from:\n",
        "\n",
        "Without smoothing:\n",
        "P(feature | class) = count(feature, class) / count(class)\n",
        "\n",
        "With smoothing:\n",
        "P(feature | class) = (count(feature, class) + 1) / (count(class) + V)\n",
        "\n",
        "where:\n",
        "\n",
        "* V = total number of unique features (e.g., vocabulary size)\n",
        "\n",
        "\n",
        "\n",
        "**3. Benefits of Laplace Smoothing:**\n",
        "\n",
        "* Avoids zero probabilities\n",
        "* Makes the classifier **robust to unseen features** in new data\n",
        "* Prevents a single missing feature from nullifying the whole prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "44375Tp7t7RC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practical"
      ],
      "metadata": {
        "id": "2gM2a2ywuUSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.  Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy?\n"
      ],
      "metadata": {
        "id": "7E6dHejJuY01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM classifier with a linear kernel\n",
        "clf = SVC(kernel='linear')\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy of SVM classifier on Iris dataset:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJzQhu3lufvP",
        "outputId": "311ae346-cdc1-4c33-f6e3-b2f8f11f8192"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM classifier on Iris dataset: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracy?\n"
      ],
      "metadata": {
        "id": "La04eruHun7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracy with Linear Kernel:\", accuracy_linear)\n",
        "print(\"Accuracy with RBF Kernel:\", accuracy_rbf)\n",
        "\n"
      ],
      "metadata": {
        "id": "jiIOmpnPuwRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.  Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean squared error (MSE)?\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPrk8Jlku09x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the SVR model (with RBF kernel)\n",
        "svr_model = SVR(kernel='rbf')\n",
        "\n",
        "# Train the model\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = svr_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error (MSE) of SVR on Housing Dataset:\", mse)\n"
      ],
      "metadata": {
        "id": "e1p9pZAlu7W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary?\n",
        "\n"
      ],
      "metadata": {
        "id": "0B0xjdc_vRLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate a 2D toy classification dataset\n",
        "X, y = datasets.make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create SVM classifier with Polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1)\n",
        "\n",
        "# Train the classifier\n",
        "svm_poly.fit(X_train, y_train)\n",
        "\n",
        "# Plot decision boundary\n",
        "xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 500),\n",
        "                     np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 500))\n",
        "\n",
        "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the data points\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.coolwarm)  # Decision boundary\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=100, cmap=plt.cm.coolwarm)  # Data points\n",
        "plt.title('SVM Classifier with Polynomial Kernel (Degree 3)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WBu52TXXvZe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.  Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate accuracy?\n",
        "\n"
      ],
      "metadata": {
        "id": "D7lPqp9IvgZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer_data = load_breast_cancer()\n",
        "X = cancer_data.data\n",
        "y = cancer_data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Gaussian Na√Øve Bayes model\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy of Gaussian Na√Øve Bayes classifier on Breast Cancer dataset:\", accuracy)\n"
      ],
      "metadata": {
        "id": "og8NR1rgvoSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.  Write a Python program to train a Multinomial Na√Øve Bayes classifier for text classification using the 20 Newsgroups dataset?\n",
        "\n"
      ],
      "metadata": {
        "id": "-5MWjCt2vw-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all')\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the text data into a bag-of-words model using CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Create a Multinomial Na√Øve Bayes model\n",
        "mnb = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "mnb.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = mnb.predict(X_test_vec)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Multinomial Na√Øve Bayes classifier on the 20 Newsgroups dataset: {accuracy:.4f}\")\n",
        "\n",
        "# Optional: Print classification report for detailed evaluation\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=newsgroups.target_names))\n"
      ],
      "metadata": {
        "id": "M5EJMH1sv8QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.  Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually?\n",
        "\n"
      ],
      "metadata": {
        "id": "wC0HTeqnv9Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate a 2D toy classification dataset\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the C values to compare\n",
        "C_values = [0.1, 1, 10]\n",
        "\n",
        "# Create a figure to plot the decision boundaries\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Loop through the different C values\n",
        "for i, C in enumerate(C_values, 1):\n",
        "    # Create an SVM classifier with a linear kernel and the current C value\n",
        "    svm = SVC(kernel='linear', C=C)\n",
        "\n",
        "    # Train the SVM classifier\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    plt.subplot(1, len(C_values), i)\n",
        "\n",
        "    # Create a mesh grid for plotting decision boundary\n",
        "    xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 500),\n",
        "                         np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 500))\n",
        "\n",
        "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.coolwarm)\n",
        "\n",
        "    # Plot the data points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=100, cmap=plt.cm.coolwarm)\n",
        "\n",
        "    # Set plot title and labels\n",
        "    plt.title(f'SVM Classifier with C={C}')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wxCkLtUFwLK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.  Write a Python program to train a Bernoulli Na√Øve Bayes classifier for binary classification on a dataset with binary features?\n",
        "\n"
      ],
      "metadata": {
        "id": "2iV07XxnwMZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Generate a binary classification dataset with binary features\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_classes=2,\n",
        "                           random_state=42, flip_y=0, n_clusters_per_class=1,\n",
        "                           weights=[0.5, 0.5], class_sep=2)\n",
        "\n",
        "# Convert features to binary (0 or 1)\n",
        "X = (X > 0).astype(int)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bernoulli Na√Øve Bayes model\n",
        "bnb = BernoulliNB()\n",
        "\n",
        "# Train the model\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = bnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Bernoulli Na√Øve Bayes classifier: {accuracy:.4f}\")\n",
        "\n",
        "# Optional: Print classification report for detailed evaluation\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "qbCkK9qAwccA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.  Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data ?\n",
        "\n"
      ],
      "metadata": {
        "id": "i9dZTxF_wdbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Train the SVM model without scaling\n",
        "svm_no_scaling = SVC(kernel='linear', random_state=42)\n",
        "svm_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_no_scaling = svm_no_scaling.predict(X_test)\n",
        "\n",
        "# Accuracy without scaling\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# 2. Apply feature scaling using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM model with scaled features\n",
        "svm_with_scaling = SVC(kernel='linear', random_state=42)\n",
        "svm_with_scaling.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_with_scaling = svm_with_scaling.predict(X_test_scaled)\n",
        "\n",
        "# Accuracy with scaling\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scaling:.4f}\")\n",
        "\n",
        "# Optional: Visualize the comparison using a bar plot\n",
        "labels = ['Without Scaling', 'With Scaling']\n",
        "accuracy_scores = [accuracy_no_scaling, accuracy_with_scaling]\n",
        "\n",
        "plt.bar(labels, accuracy_scores, color=['red', 'green'])\n",
        "plt.title('Comparison of SVM Accuracy with and without Feature Scaling')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UnXXq4t7wmP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Write a Python program to train a Gaussian Na√Øve Bayes model and compare the predictions before and after Laplace Smoothing?\n",
        "\n"
      ],
      "metadata": {
        "id": "8c9fxnalww12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Train the Gaussian Na√Øve Bayes model without Laplace smoothing\n",
        "gnb_no_smoothing = GaussianNB(var_smoothing=0)  # No smoothing\n",
        "gnb_no_smoothing.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_no_smoothing = gnb_no_smoothing.predict(X_test)\n",
        "\n",
        "# Accuracy without smoothing\n",
        "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
        "\n",
        "# 2. Train the Gaussian Na√Øve Bayes model with Laplace smoothing\n",
        "# GaussianNB by default has Laplace smoothing with var_smoothing > 0\n",
        "gnb_with_smoothing = GaussianNB(var_smoothing=1e-9)  # Apply Laplace smoothing\n",
        "gnb_with_smoothing.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_with_smoothing = gnb_with_smoothing.predict(X_test)\n",
        "\n",
        "# Accuracy with smoothing\n",
        "accuracy_with_smoothing = accuracy_score(y_test, y_pred_with_smoothing)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy without Laplace smoothing: {accuracy_no_smoothing:.4f}\")\n",
        "print(f\"Accuracy with Laplace smoothing: {accuracy_with_smoothing:.4f}\")\n",
        "\n",
        "# Optional: Compare the predictions directly\n",
        "print(\"\\nPredictions without smoothing:\", y_pred_no_smoothing[:10])\n",
        "print(\"Predictions with smoothing:\", y_pred_with_smoothing[:10])\n"
      ],
      "metadata": {
        "id": "wgZ5ctNbw5bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel)?\n",
        "\n"
      ],
      "metadata": {
        "id": "OIy9Yos2xCuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],  # Kernel coefficient for 'rbf', 'poly', etc.\n",
        "    'kernel': ['linear', 'rbf', 'poly']  # Kernel types\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the model on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and the best score from GridSearchCV\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate the model with the best found parameters on the test set\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred = best_svm.predict(X_test)\n",
        "\n",
        "# Accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "tn8s_RkgxREn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32.  Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy?\n",
        "\n"
      ],
      "metadata": {
        "id": "Ujofefgax3UF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check class distribution in the target variable (imbalanced dataset)\n",
        "print(\"Class distribution in target (y):\")\n",
        "print(f\"0: {sum(y == 0)}  (Malignant)\")\n",
        "print(f\"1: {sum(y == 1)}  (Benign)\")\n",
        "\n",
        "# Compute class weights based on the class distribution\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=[0, 1], y=y_train)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "# 1. Train the SVM model without class weighting\n",
        "svm_no_weight = SVC(kernel='linear')\n",
        "svm_no_weight.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_no_weight = svm_no_weight.predict(X_test)\n",
        "\n",
        "# Accuracy without class weighting\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "print(f\"\\nAccuracy without class weighting: {accuracy_no_weight:.4f}\")\n",
        "\n",
        "# 2. Train the SVM model with class weighting\n",
        "svm_with_weight = SVC(kernel='linear', class_weight=class_weight_dict)\n",
        "svm_with_weight.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_with_weight = svm_with_weight.predict(X_test)\n",
        "\n",
        "# Accuracy with class weighting\n",
        "accuracy_with_weight = accuracy_score(y_test, y_pred_with_weight)\n",
        "print(f\"Accuracy with class weighting: {accuracy_with_weight:.4f}\")\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check class distribution in the target variable (imbalanced dataset)\n",
        "print(\"Class distribution in target (y):\")\n",
        "print(f\"0: {sum(y == 0)}  (Malignant)\")\n",
        "print(f\"1: {sum(y == 1)}  (Benign)\")\n",
        "\n",
        "# Compute class weights based on the class distribution\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=[0, 1], y=y_train)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "# 1. Train the SVM model without class weighting\n",
        "svm_no_weight = SVC(kernel='linear')\n",
        "svm_no_weight.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_no_weight = svm_no_weight.predict(X_test)\n",
        "\n",
        "# Accuracy without class weighting\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "print(f\"\\nAccuracy without class weighting: {accuracy_no_weight:.4f}\")\n",
        "\n",
        "# 2. Train the SVM model with class weighting\n",
        "svm_with_weight = SVC(kernel='linear', class_weight=class_weight_dict)\n",
        "svm_with_weight.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_with_weight = svm_with_weight.predict(X_test)\n",
        "\n",
        "# Accuracy with class weighting\n",
        "accuracy_with_weight = accuracy_score(y_test, y_pred_with_weight)\n",
        "print(f\"Accuracy with class weighting: {accuracy_with_weight:.4f}\")\n"
      ],
      "metadata": {
        "id": "Xm2ioyANySOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33.  Write a Python program to implement a Na√Øve Bayes classifier for spam detection using email data?\n"
      ],
      "metadata": {
        "id": "MO93D6riyakI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the SMS Spam Collection dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
        "dataset = pd.read_csv(url, sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"Sample data:\")\n",
        "print(dataset.head())\n",
        "\n",
        "# Preprocess the text (convert labels to numeric, split into training and test sets)\n",
        "dataset['label'] = dataset['label'].map({'ham': 0, 'spam': 1})  # Convert labels to 0 (ham) and 1 (spam)\n",
        "X = dataset['message']\n",
        "y = dataset['label']\n",
        "\n",
        "# Split dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data into numeric form using CountVectorizer (Bag of Words model)\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize and train the Multinomial Na√Øve Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_vect, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = nb_model.predict(X_test_vect)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy of Na√Øve Bayes classifier: {accuracy:.4f}\")\n",
        "\n",
        "# Print a classification report (precision, recall, F1-score)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n"
      ],
      "metadata": {
        "id": "mRwnZrkSy0Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34.  Write a Python program to train an SVM Classifier and a Na√Øve Bayes Classifier on the same dataset and compare their accuracy?\n",
        "\n"
      ],
      "metadata": {
        "id": "UG7bMRUsy1kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Train SVM Classifier\n",
        "svm_classifier = SVC(kernel='linear')  # Using Linear Kernel for SVM\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set using SVM\n",
        "y_pred_svm = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy of SVM\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"SVM Classifier Accuracy: {accuracy_svm:.4f}\")\n",
        "\n",
        "# 2. Train Na√Øve Bayes Classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set using Na√Øve Bayes\n",
        "y_pred_nb = nb_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy of Na√Øve Bayes\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "print(f\"Na√Øve Bayes Classifier Accuracy: {accuracy_nb:.4f}\")\n",
        "\n",
        "# Compare the accuracy of both classifiers\n",
        "if accuracy_svm > accuracy_nb:\n",
        "    print(\"\\nSVM Classifier performs better than Na√Øve Bayes.\")\n",
        "elif accuracy_svm < accuracy_nb:\n",
        "    print(\"\\nNa√Øve Bayes Classifier performs better than SVM.\")\n",
        "else:\n",
        "    print(\"\\nBoth classifiers have the same accuracy.\")\n"
      ],
      "metadata": {
        "id": "Mc39hQ4qy_tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Write a Python program to perform feature selection before training a Na√Øve Bayes classifier and compare results?\n",
        "\n"
      ],
      "metadata": {
        "id": "5Fd9wqJjzJpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Train Na√Øve Bayes classifier without feature selection\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_no_fs = nb_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy without feature selection\n",
        "accuracy_no_fs = accuracy_score(y_test, y_pred_no_fs)\n",
        "print(f\"Na√Øve Bayes Accuracy without Feature Selection: {accuracy_no_fs:.4f}\")\n",
        "\n",
        "# 2. Apply feature selection (Select top 2 features using SelectKBest)\n",
        "selector = SelectKBest(score_func=f_classif, k=2)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# 3. Train Na√Øve Bayes classifier with feature selection\n",
        "nb_classifier_fs = GaussianNB()\n",
        "nb_classifier_fs.fit(X_train_selected, y_train)\n",
        "\n",
        "# Predict on the test set with feature selection\n",
        "y_pred_fs = nb_classifier_fs.predict(X_test_selected)\n",
        "\n",
        "# Calculate accuracy with feature selection\n",
        "accuracy_fs = accuracy_score(y_test, y_pred_fs)\n",
        "print(f\"Na√Øve Bayes Accuracy with Feature Selection: {accuracy_fs:.4f}\")\n",
        "\n",
        "# Compare the accuracy of both models\n",
        "if accuracy_no_fs > accuracy_fs:\n",
        "    print(\"\\nNa√Øve Bayes performs better without feature selection.\")\n",
        "elif accuracy_no_fs < accuracy_fs:\n",
        "    print(\"\\nNa√Øve Bayes performs better with feature selection.\")\n",
        "else:\n",
        "    print(\"\\nNa√Øve Bayes performs the same with and without feature selection.\")\n"
      ],
      "metadata": {
        "id": "Foq-_IDPzdT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36.  Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy?\n",
        "\n"
      ],
      "metadata": {
        "id": "CBEaTrUHzle-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Train SVM Classifier with One-vs-Rest (OvR) strategy\n",
        "svm_ovr = OneVsRestClassifier(SVC(kernel='linear'))  # Using linear kernel for SVM\n",
        "svm_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set using OvR\n",
        "y_pred_ovr = svm_ovr.predict(X_test)\n",
        "\n",
        "# Calculate accuracy of OvR\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(f\"One-vs-Rest (OvR) Classifier Accuracy: {accuracy_ovr:.4f}\")\n",
        "\n",
        "# 2. Train SVM Classifier with One-vs-One (OvO) strategy\n",
        "svm_ovo = OneVsOneClassifier(SVC(kernel='linear'))  # Using linear kernel for SVM\n",
        "svm_ovo.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set using OvO\n",
        "y_pred_ovo = svm_ovo.predict(X_test)\n",
        "\n",
        "# Calculate accuracy of OvO\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "print(f\"One-vs-One (OvO) Classifier Accuracy: {accuracy_ovo:.4f}\")\n",
        "\n",
        "# Compare the accuracy of both models\n",
        "if accuracy_ovr > accuracy_ovo:\n",
        "    print(\"\\nOne-vs-Rest (OvR) strategy performs better than One-vs-One (OvO).\")\n",
        "elif accuracy_ovr < accuracy_ovo:\n",
        "    print(\"\\nOne-vs-One (OvO) strategy performs better than One-vs-Rest (OvR).\")\n",
        "else:\n",
        "    print(\"\\nBoth One-vs-Rest (OvR) and One-vs-One (OvO) strategies have the same accuracy.\")\n"
      ],
      "metadata": {
        "id": "mGssiE_FzvXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37.  Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy?\n",
        "\n"
      ],
      "metadata": {
        "id": "KktwT0bFz9h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the different kernels\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "\n",
        "# Initialize a dictionary to store accuracies for each kernel\n",
        "accuracies = {}\n",
        "\n",
        "# Train an SVM Classifier with each kernel and calculate accuracy\n",
        "for kernel in kernels:\n",
        "    # Initialize the SVM Classifier with the specific kernel\n",
        "    svm_classifier = SVC(kernel=kernel, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Store the accuracy in the dictionary\n",
        "    accuracies[kernel] = accuracy\n",
        "\n",
        "# Print the accuracies for each kernel\n",
        "for kernel, accuracy in accuracies.items():\n",
        "    print(f\"Accuracy with {kernel} kernel: {accuracy:.4f}\")\n",
        "\n",
        "# Compare the kernel performances\n",
        "best_kernel = max(accuracies, key=accuracies.get)\n",
        "print(f\"\\nBest performing kernel: {best_kernel} kernel\")\n"
      ],
      "metadata": {
        "id": "IAW5m6n70IqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy?\n"
      ],
      "metadata": {
        "id": "KDS7ZUiS0O8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Initialize the Stratified K-Fold Cross-Validation\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize the SVM Classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# List to store accuracies for each fold\n",
        "accuracies = []\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy for this fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Compute the average accuracy across all folds\n",
        "average_accuracy = np.mean(accuracies)\n",
        "\n",
        "# Print the accuracy for each fold and the average accuracy\n",
        "for i, accuracy in enumerate(accuracies, 1):\n",
        "    print(f\"Accuracy for fold {i}: {accuracy:.4f}\")\n",
        "\n",
        "print(f\"\\nAverage accuracy across all folds: {average_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "otLxteet0cSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39.  Write a Python program to train a Na√Øve Bayes classifier using different prior probabilities and compare performance?\n"
      ],
      "metadata": {
        "id": "Nm89GuV40oNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different sets of priors\n",
        "priors_list = [\n",
        "    [1/3, 1/3, 1/3],  # Equal priors (default)\n",
        "    [0.5, 0.25, 0.25],  # Heavier prior on class 0\n",
        "    [0.2, 0.5, 0.3],  # Heavier prior on class 1\n",
        "    [0.3, 0.3, 0.4]   # Heavier prior on class 2\n",
        "]\n",
        "\n",
        "# Initialize a dictionary to store the accuracies for each prior\n",
        "accuracies = {}\n",
        "\n",
        "# Train the Na√Øve Bayes model with different priors\n",
        "for priors in priors_list:\n",
        "    # Initialize the Na√Øve Bayes classifier with the specified priors\n",
        "    nb_classifier = GaussianNB(priors=priors)\n",
        "\n",
        "    # Train the model\n",
        "    nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Store the accuracy for this set of priors\n",
        "    accuracies[str(priors)] = accuracy\n",
        "\n",
        "# Print the accuracies for each prior\n",
        "for priors, accuracy in accuracies.items():\n",
        "    print(f\"Accuracy with priors {priors}: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "SGHlg1fo0tlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40.  Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy?\n",
        "\n"
      ],
      "metadata": {
        "id": "5T3HBE3o07GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM Classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Initialize Recursive Feature Elimination (RFE)\n",
        "rfe = RFE(svm_classifier, n_features_to_select=10)  # Select 10 most important features\n",
        "\n",
        "# Fit the RFE model to the training data\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "\n",
        "# Train the SVM Classifier on the reduced features\n",
        "svm_classifier.fit(X_train_rfe, y_train)\n",
        "\n",
        "# Predict on the test set using the selected features\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "y_pred = svm_classifier.predict(X_test_rfe)\n",
        "\n",
        "# Calculate accuracy of the model with RFE\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Train the SVM Classifier without feature selection (using all features)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "y_pred_full = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy without feature selection\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the accuracies for both models\n",
        "print(f\"Accuracy with RFE (10 selected features): {accuracy_rfe:.4f}\")\n",
        "print(f\"Accuracy without feature selection (using all features): {accuracy_full:.4f}\")\n"
      ],
      "metadata": {
        "id": "xDu1k7l11Xpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5NDR8FpH1jMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM Classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Initialize Recursive Feature Elimination (RFE)\n",
        "rfe = RFE(svm_classifier, n_features_to_select=10)  # Select 10 most important features\n",
        "\n",
        "# Fit the RFE model to the training data\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "\n",
        "# Train the SVM Classifier on the reduced features\n",
        "svm_classifier.fit(X_train_rfe, y_train)\n",
        "\n",
        "# Predict on the test set using the selected features\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "y_pred = svm_classifier.predict(X_test_rfe)\n",
        "\n",
        "# Calculate accuracy of the model with RFE\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Train the SVM Classifier without feature selection (using all features)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "y_pred_full = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy without feature selection\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the accuracies for both models\n",
        "print(f\"Accuracy with RFE (10 selected features): {accuracy_rfe:.4f}\")\n",
        "print(f\"Accuracy without feature selection (using all features): {accuracy_full:.4f}\")\n"
      ],
      "metadata": {
        "id": "-1Ugt0_q1qyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42.  Write a Python program to train a Na√Øve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss)?\n",
        "\n"
      ],
      "metadata": {
        "id": "U8RnZUy-10-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Na√Øve Bayes classifier (Gaussian Na√Øve Bayes)\n",
        "nb_classifier = GaussianNB()\n",
        "\n",
        "# Train the Na√Øve Bayes classifier on the training data\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set (not just class labels)\n",
        "y_prob = nb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Calculate Log Loss (Cross-Entropy Loss)\n",
        "logloss = log_loss(y_test, y_prob)\n",
        "\n",
        "# Print the Log Loss\n",
        "print(f\"Log Loss (Cross-Entropy Loss) on the test set: {logloss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tp539MY2BZi",
        "outputId": "59bae672-fc06-45ed-d96b-022164701942"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with RFE (10 selected features): 0.9737\n",
            "Accuracy without feature selection (using all features): 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "43.  Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn?\n"
      ],
      "metadata": {
        "id": "yDB2LSQ72S7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM Classifier with a linear kernel\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the SVM Classifier on the training data\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=cancer.target_names, yticklabels=cancer.target_names)\n",
        "plt.title('Confusion Matrix for SVM Classifier')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a1p7Cdev2aln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. \u001e\u001e= Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE?\n",
        "\n"
      ],
      "metadata": {
        "id": "Ra93175525Or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM Regressor (SVR) with a Radial Basis Function (RBF) kernel\n",
        "svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
        "\n",
        "# Train the SVR model\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "# Compute the Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Print the MAE value\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "# Visualize the results (actual vs predicted)\n",
        "plt.scatter(X_test, y_test, color='red', label='True values')\n",
        "plt.scatter(X_test, y_pred, color='blue', label='Predicted values')\n",
        "plt.plot(X_test, svr.predict(X_test), color='black', lw=2, label='SVR fit')\n",
        "plt.title('SVR Regression - Actual vs Predicted')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TUyJlCr_3B1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Write a Python program to train a Na√Øve Bayes classifier and evaluate its performance using the ROC-AUC score?\n"
      ],
      "metadata": {
        "id": "Tjmmrrdf3LqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Na√Øve Bayes classifier (Gaussian Naive Bayes)\n",
        "nb_classifier = GaussianNB()\n",
        "\n",
        "# Train the Na√Øve Bayes model\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "y_prob = nb_classifier.predict_proba(X_test)[:, 1]  # Probability estimates for the positive class\n",
        "\n",
        "# Compute the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot the ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.title('ROC Curve for Na√Øve Bayes Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vwQVdgSJ3Vjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "46.  Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve."
      ],
      "metadata": {
        "id": "dNU6KEDQ395g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM classifier with a linear kernel\n",
        "svm_classifier = SVC(kernel='linear', probability=True)\n",
        "\n",
        "# Train the SVM model\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and get probabilities on the test set\n",
        "y_prob = svm_classifier.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Compute Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Compute Average Precision Score\n",
        "average_precision = average_precision_score(y_test, y_prob)\n",
        "print(f'Average Precision: {average_precision:.4f}')\n",
        "\n",
        "# Plot the Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AP = {average_precision:.4f})')\n",
        "plt.fill_between(recall, precision, color='lightblue', alpha=0.5)\n",
        "plt.title('Precision-Recall Curve for SVM Classifier')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "e4ru0Er44BBl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}