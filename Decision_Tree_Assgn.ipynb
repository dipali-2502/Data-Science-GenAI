{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###Theoretical"
      ],
      "metadata": {
        "id": "4UFtzkw5sNLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work?\n",
        "\n",
        "ans. **Decision Tree--**  \n",
        "A **Decision Tree** is a type of machine learning model that looks like a flowchart. It helps make decisions by asking a series of **yes/no** or **true/false** questions.  \n",
        "At each step, it splits the data based on the answers, moving down different \"branches\" until it reaches a final decision or prediction (which we call a \"leaf node\").\n",
        "\n",
        "\n",
        "**How does a Decision Tree work?**\n",
        "1. **Start at the root** (the top of the tree).\n",
        "2. **Choose the best feature** (or question) to split the data.  \n",
        "   - This is done using metrics like **Gini Impurity**, **Entropy**, or **Information Gain** (for classification) or **variance reduction** (for regression).\n",
        "3. **Split the data** into groups based on the answer.\n",
        "4. **Repeat** the process for each group (sub-tree) until:\n",
        "   - You meet a stopping condition (like tree depth limit or minimum samples).\n",
        "   - Or all the data in a branch belongs to one class.\n",
        "\n"
      ],
      "metadata": {
        "id": "zqsMyHNcsc8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are impurity measures in Decision Trees\t?\n",
        "\n",
        "ans. What are impurity measures in Decision Trees?\n",
        "An impurity measure tells us how mixed up the data is at a certain point in the tree.\n",
        "\n",
        "If a group (node) has only one class (say, all \"yes\" or all \"no\"), the impurity is low (or zero — meaning it's pure).\n",
        "\n",
        "If the group has a mix of classes (like 50% \"yes\" and 50% \"no\"), the impurity is high.\n",
        "\n",
        "When building a Decision Tree, we always try to reduce impurity as much as possible at each split — so each branch becomes as \"pure\" as it can.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d7TCyYA2tBe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What is the mathematical formula for Gini Impurity\t?\n",
        "\n",
        "ans. **mathematical formula** for **Gini Impurity**:\n",
        "\n",
        "\\[\n",
        "Gini = 1 - \\sum_{i=1}^{n} p_i^2\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( p_i \\) = the probability of picking an item of class \\( i \\) from the node\n",
        "- \\( n \\) = total number of class"
      ],
      "metadata": {
        "id": "nEiKD0MOtR0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the mathematical formula for Entropy\t?\n",
        "\n",
        "ans.  **mathematical formula** for **Entropy**:\n",
        "\n",
        "\\[\n",
        "Entropy = -\\sum_{i=1}^{n} p_i \\log_2(p_i)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( p_i \\) = probability of class \\( i \\) at the node\n",
        "- \\( n \\) = total number of classes\n",
        "\n"
      ],
      "metadata": {
        "id": "nEsdIAU7thar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Information Gain, and how is it used in Decision Trees\t?\n",
        "\n",
        "ans.  Information Gain (IG) measures how much \"uncertainty\" or \"disorder\" decreases after splitting the data based on a feature.\n",
        "\n",
        "In short:\n",
        "\n",
        "Before split: You have some disorder (measured by Entropy).\n",
        "\n",
        "After split: You check how much the disorder has reduced.\n",
        "\n",
        "Information Gain = (Entropy before split) – (Weighted entropy after split)\n",
        "\n",
        "The feature with the highest Information Gain is chosen to split the data at that step!\n",
        "\n",
        "Mathematical formula for Information Gain:\n",
        "\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "parent\n",
        ")\n",
        "−\n",
        "∑\n",
        "𝑘\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "(\n",
        "𝑁\n",
        "𝑘\n",
        "𝑁\n",
        "×\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "child\n",
        "𝑘\n",
        ")\n",
        ")\n",
        "IG=Entropy(parent)−\n",
        "k=1\n",
        "∑\n",
        "K\n",
        "​\n",
        " (\n",
        "N\n",
        "N\n",
        "k\n",
        "​\n",
        "\n",
        "​\n",
        " ×Entropy(child\n",
        "k\n",
        "​\n",
        " ))\n",
        "Where:\n",
        "\n",
        "𝑁\n",
        "N = total number of samples in parent\n",
        "\n",
        "𝑁\n",
        "𝑘\n",
        "N\n",
        "k\n",
        "​\n",
        "  = number of samples in child\n",
        "𝑘\n",
        "k\n",
        "\n",
        "𝐾\n",
        "K = number of child nodes\n",
        "\n",
        "How is it used in Decision Trees?\n",
        "At each node:\n",
        "\n",
        "Calculate the Entropy of the current data (parent node).\n",
        "\n",
        "For each feature:\n",
        "\n",
        "Imagine splitting the data based on that feature.\n",
        "\n",
        "Calculate the weighted Entropy of resulting groups (children).\n",
        "\n",
        "Find the Information Gain.\n",
        "\n",
        "Choose the feature with the highest Information Gain to split.\n",
        "\n",
        "Goal: Split the data to make it as pure as possible, as fast as possible!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iO8TVvG0t0O2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the difference between Gini Impurity and Entropy\t?\n",
        "\n",
        "ans. Gini Impurity vs. Entropy\n",
        "\n",
        "Aspect\tGini Impurity\tEntropy\n",
        "Meaning\tMeasures how often a randomly chosen element would be incorrectly labeled if it was randomly labeled according to the distribution of labels.\tMeasures the amount of \"disorder\" or \"uncertainty\" in the dataset.\n",
        "Formula\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−∑\n",
        "i=1\n",
        "n\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy=−∑\n",
        "i=1\n",
        "n\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "Value Range\tBetween 0 and 0.5 (for binary classification).\tBetween 0 and 1 (for binary classification).\n",
        "Interpretation\tLower Gini means purer groups.\tLower Entropy means purer groups.\n",
        "Calculation Speed\tFaster to compute (no logarithms).\tSlightly slower (uses logarithms).\n",
        "Usage\tOften used in CART (Classification and Regression Trees).\tOften used in ID3, C4.5 decision tree algorithms.\n",
        "Preference\tGood for when we want quicker, simpler trees.\tGood when you want more information-theoretic splitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "-zuoF5gIMFNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is the mathematical explanation behind Decision Trees\t?\n",
        "\n",
        "ans. Decision Trees split the data into subsets based on the value of features to make the groups purer.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "At each node, calculate an impurity measure (like Gini or Entropy).\n",
        "\n",
        "For each feature, compute how much splitting on it reduces impurity (Information Gain or Gini Gain).\n",
        "\n",
        "Choose the feature with the highest gain to split the node.\n",
        "\n",
        "Repeat the process for each child node until a stopping condition is met (like pure nodes or maximum depth).\n",
        "\n",
        "The tree grows by recursively minimizing impurity\n"
      ],
      "metadata": {
        "id": "-p9znSqvMkDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Pre-Pruning in Decision Trees\t?\n",
        "\n",
        "ans. Pre-Pruning (also called early stopping) means stopping the tree growth early, before it becomes too complex, to prevent overfitting.\n",
        "\n",
        "Instead of growing the full tree and then trimming it, in pre-pruning we apply conditions during tree building to decide when to stop splitting.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HVJwd9t0M9wS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is Post-Pruning in Decision Trees\t?\n",
        "\n",
        "ans. Post-Pruning means first growing the full Decision Tree and then trimming it back to remove unnecessary branches, after it has been built.\n",
        "\n",
        "It simplifies the tree by removing splits that do not improve the model's performance significantly, usually based on validation set results.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3mCvMJmXNS9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the difference between Pre-Pruning and Post-Pruning\t?\n",
        "\n",
        "ans.   \n",
        "###Pre-Pruning (Early Stopping)\n",
        "\n",
        "In Pre-Pruning, the Decision Tree is restricted while it is being built.\n",
        "The tree stops growing when a certain condition is met, such as:\n",
        "\n",
        "Maximum depth reached.\n",
        "\n",
        "Minimum number of samples in a node.\n",
        "\n",
        "Gain in impurity reduction is below a threshold.\n",
        "\n",
        "Purpose:\n",
        "\n",
        "To prevent the tree from becoming too large and complex.\n",
        "\n",
        "To avoid overfitting the training data early.\n",
        "\n",
        "Drawback:\n",
        "\n",
        "If the tree is stopped too soon, it may underfit the data (miss important patterns).\n",
        "\n",
        "###post-Pruning (Pruning After Full Growth)\n",
        "\n",
        "In Post-Pruning, the Decision Tree is allowed to grow fully without restrictions.\n",
        "After the tree is built:\n",
        "\n",
        "It is analyzed to find parts (branches) that do not contribute much to prediction accuracy.\n",
        "\n",
        "These branches are removed (pruned) based on performance on a validation set or cost-complexity criteria.\n",
        "\n",
        "Purpose:\n",
        "\n",
        "To reduce overfitting by simplifying the tree after it captures all possible patterns.\n",
        "\n",
        "Advantage:\n",
        "\n",
        "Post-pruning usually produces a tree that balances high accuracy and simplicity.\n",
        "\n",
        "Final Point:\n",
        "Pre-Pruning tries to control complexity during training.\n",
        "\n",
        "Post-Pruning tries to correct complexity after training.\n",
        "\n",
        "Both methods aim to create a tree that generalizes better to unseen data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lBKpSnncNg47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is a Decision Tree Regressor\t?\n",
        "\n",
        "ans. ​A Decision Tree Regressor is a supervised machine learning algorithm used for predicting continuous numerical values. It operates by partitioning the data into subsets based on feature values, constructing a tree-like model where each internal node represents a decision on a feature, and each leaf node represents a predicted numerical value.​\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BhJi6jTbPjq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  What are the advantages and disadvantages of Decision Trees\t?\n",
        "\n",
        "ans.\n",
        "\n",
        "**Advantages of Decision Trees:**\n",
        "\n",
        "- Easy to understand and interpret  \n",
        "- Little data preparation needed  \n",
        "- Handles classification and regression  \n",
        "- Captures nonlinear patterns  \n",
        "- Provides feature importance  \n",
        "- Can handle missing values\n",
        "\n",
        "**Disadvantages of Decision Trees:**\n",
        "\n",
        "- Prone to overfitting  \n",
        "- Sensitive to data changes  \n",
        "- Biased with imbalanced data  \n",
        "- Finds local, not global, optimum  \n",
        "- Poor at modeling smooth trends  \n",
        "- Weaker than ensemble methods\n"
      ],
      "metadata": {
        "id": "cDOKvHQ9HKhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  How does a Decision Tree handle missing values\t?\n",
        "\n",
        "ans.\n",
        "A **Decision Tree** can handle missing values in two main ways, depending on the implementation:\n",
        "\n",
        "1. **Ignoring missing values during splits**  \n",
        "   The tree splits only on available (non-missing) data when choosing the best split at each node.\n",
        "\n",
        "2. **Surrogate splits (used in some algorithms, like CART)**  \n",
        "   When a value is missing for the main splitting feature, the tree uses another correlated feature (surrogate) to decide which branch to follow.\n",
        "\n",
        "Some simpler implementations may just **exclude rows with missing data** or require preprocessing before training, so it depends on the software or library used.\n",
        "\n"
      ],
      "metadata": {
        "id": "KII7ZyU-sfFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  How does a Decision Tree handle categorical features\t?\n",
        "\n",
        " ans.\n",
        "A **Decision Tree** handles categorical features by splitting the data based on the category values.  \n",
        "\n",
        "Specifically:\n",
        "\n",
        "1. **For binary splits (e.g., CART):**  \n",
        "   It groups category values into two subsets that best separate the target (for example, {A, B} vs. {C, D}) and splits the node accordingly.\n",
        "\n",
        "2. **For multiway splits (e.g., ID3, C4.5):**  \n",
        "   It creates a separate branch for each category (for example, one branch each for A, B, C, D).\n",
        "\n",
        "3. **Using one-hot or ordinal encoding (sometimes required before training):**  \n",
        "   In some implementations, you first convert categories into numbers or dummy variables.\n",
        "\n",
        "So, decision trees can **natively handle categorical features** without needing you to preconvert them in most modern libraries.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R0pwhS01tFto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  What are some real-world applications of Decision Trees?\n",
        "\n",
        "ans.  1. **Medical diagnosis** — predicting diseases based on symptoms and test results.  \n",
        "2. **Credit scoring** — evaluating if a person qualifies for a loan or credit card.  \n",
        "3. **Customer churn prediction** — identifying customers likely to leave a service.  \n",
        "4. **Fraud detection** — spotting suspicious transactions in banking or e-commerce.  \n",
        "5. **Marketing** — segmenting customers and predicting responses to campaigns.  \n",
        "6. **Manufacturing** — detecting equipment faults or predicting maintenance needs.  \n",
        "7. **Human resources** — screening job candidates or predicting employee turnover.  \n",
        "8. **Healthcare** — selecting treatment plans based on patient profiles.\n",
        "\n",
        "If you want, I can also give you two or three short example case studies! Let me know.\n"
      ],
      "metadata": {
        "id": "EAGvYqHvtcTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Practical"
      ],
      "metadata": {
        "id": "nrWl4Ub-ttbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy?\n",
        "\n"
      ],
      "metadata": {
        "id": "TdqpeFzItxAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8gb7eCNuIgb",
        "outputId": "fd375cbf-f33b-4908-fa3c-d01b98f161dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances?\n",
        "\n"
      ],
      "metadata": {
        "id": "AskD-XUFuSqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Classifier using Gini impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NtJ48CQuaE-",
        "outputId": "cc8f1dd5-5cc2-49d4-f35c-b5d9fe27107f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy?\n",
        "\n"
      ],
      "metadata": {
        "id": "LrF0Bvd9ulkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Classifier using Entropy\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy using Entropy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIKqknIJut4o",
        "outputId": "9fc32d99-1ba2-4e7a-9dbb-dbdaf660f561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy using Entropy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mA9r0LiIu13G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "pivnXBRtvC51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz?\n"
      ],
      "metadata": {
        "id": "UgPZp-djvPBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "import graphviz\n",
        "import pydotplus\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Visualize the Decision Tree using Graphviz\n",
        "dot_data = export_graphviz(clf, out_file=None,\n",
        "                           feature_names=iris.feature_names,\n",
        "                           class_names=iris.target_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "\n",
        "# Create the Graphviz source and display the tree\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"decision_tree\")  # Save the tree as a PDF file\n",
        "graph.view()  # View the tree in the default PDF viewer\n"
      ],
      "metadata": {
        "id": "o43ml-HJvUz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.  Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree?\n",
        "\n"
      ],
      "metadata": {
        "id": "x8xZKBIdvdSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier with no max depth (fully grown tree)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on both the training set and the test set\n",
        "y_train_pred_full = clf_full.predict(X_train)\n",
        "y_test_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy on both sets\n",
        "train_accuracy_full = accuracy_score(y_train, y_train_pred_full)\n",
        "test_accuracy_full = accuracy_score(y_test, y_test_pred_full)\n",
        "\n",
        "print(f\"Training Accuracy (Fully Grown Tree): {train_accuracy_full:.2f}\")\n",
        "print(f\"Test Accuracy (Fully Grown Tree): {test_accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "id": "AlqyHAjMvwjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree?\n",
        "\n"
      ],
      "metadata": {
        "id": "wQIgqNyKwPCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the default Decision Tree Classifier\n",
        "clf_default = DecisionTreeClassifier(random_state=42)\n",
        "clf_default.fit(X_train, y_train)\n",
        "\n",
        "# Create and train the Decision Tree Classifier with min_samples_split=5\n",
        "clf_min_samples_split = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_min_samples_split.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on both the training set and the test set for both classifiers\n",
        "y_train_pred_default = clf_default.predict(X_train)\n",
        "y_test_pred_default = clf_default.predict(X_test)\n",
        "\n",
        "y_train_pred_split = clf_min_samples_split.predict(X_train)\n",
        "y_test_pred_split = clf_min_samples_split.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy on both sets for both models\n",
        "train_accuracy_default = accuracy_score(y_train, y_train_pred_default)\n",
        "test_accuracy_default = accuracy_score(y_test, y_test_pred_default)\n",
        "\n",
        "train_accuracy_split = accuracy_score(y_train, y_train_pred_split)\n",
        "test_accuracy_split = accuracy_score(y_test, y_test_pred_split)\n",
        "\n",
        "print(f\"Default Tree Training Accuracy: {train_accuracy_default:.2f}\")\n",
        "print(f\"Default Tree Test Accuracy: {test_accuracy_default:.2f}\")\n",
        "\n",
        "print(f\"Tree with min_samples_split=5 Training Accuracy: {train_accuracy_split:.2f}\")\n",
        "print(f\"Tree with min_samples_split=5 Test Accuracy: {test_accuracy_split:.2f}\")\n"
      ],
      "metadata": {
        "id": "zqdbe8IswWvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.  Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data?\n",
        "\n"
      ],
      "metadata": {
        "id": "QfmoU5GfwgWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Without Feature Scaling ---\n",
        "# Create and train the Decision Tree Classifier without scaling\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_train_pred_unscaled = clf_unscaled.predict(X_train)\n",
        "y_test_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy for unscaled data\n",
        "train_accuracy_unscaled = accuracy_score(y_train, y_train_pred_unscaled)\n",
        "test_accuracy_unscaled = accuracy_score(y_test, y_test_pred_unscaled)\n",
        "\n",
        "# --- With Feature Scaling ---\n",
        "# Apply StandardScaler to the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train the Decision Tree Classifier with scaled data\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled test set\n",
        "y_train_pred_scaled = clf_scaled.predict(X_train_scaled)\n",
        "y_test_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Calculate and print the accuracy for scaled data\n",
        "train_accuracy_scaled = accuracy_score(y_train, y_train_pred_scaled)\n",
        "test_accuracy_scaled = accuracy_score(y_test, y_test_pred_scaled)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Accuracy without Feature Scaling:\")\n",
        "print(f\"  Training Accuracy: {train_accuracy_unscaled:.2f}\")\n",
        "print(f\"  Test Accuracy: {test_accuracy_unscaled:.2f}\")\n",
        "\n",
        "print(f\"\\nAccuracy with Feature Scaling:\")\n",
        "print(f\"  Training Accuracy: {train_accuracy_scaled:.2f}\")\n",
        "print(f\"  Test Accuracy: {test_accuracy_scaled:.2f}\")\n"
      ],
      "metadata": {
        "id": "PbVKJIqjwp8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.  Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification?\n",
        "\n"
      ],
      "metadata": {
        "id": "WMWnrAbBwwco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Wrap the classifier with OneVsRest strategy\n",
        "ovr_clf = OneVsRestClassifier(clf)\n",
        "\n",
        "# Train the model\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovr_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with One-vs-Rest strategy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "aDhbFsrHw3-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.  Write a Python program to train a Decision Tree Classifier and display the feature importance scores?\n"
      ],
      "metadata": {
        "id": "WaG4CtrCw_qQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Create and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Get the feature importance scores\n",
        "importance_scores = clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display the feature names and their importance scores\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance Score': importance_scores\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance score in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance Score', ascending=False)\n",
        "\n",
        "# Display the feature importance scores\n",
        "print(\"Feature Importance Scores:\")\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "id": "a3_tXDozxYn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.  Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree?\n",
        "\n"
      ],
      "metadata": {
        "id": "tvVfxMISy4zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston housing dataset\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Unrestricted Decision Tree Regressor ---\n",
        "# Create and train the unrestricted Decision Tree Regressor\n",
        "clf_unrestricted = DecisionTreeRegressor(random_state=42)\n",
        "clf_unrestricted.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_unrestricted = clf_unrestricted.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE) for the unrestricted model\n",
        "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
        "\n",
        "# --- Restricted Decision Tree Regressor (max_depth=5) ---\n",
        "# Create and train the Decision Tree Regressor with max_depth=5\n",
        "clf_restricted = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "clf_restricted.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_restricted = clf_restricted.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE) for the restricted model\n",
        "mse_restricted = mean_squared_error(y_test, y_pred_restricted)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Unrestricted Tree MSE: {mse_unrestricted:.2f}\")\n",
        "print(f\"Restricted Tree (max_depth=5) MSE: {mse_restricted:.2f}\")\n"
      ],
      "metadata": {
        "id": "-gvbg0JVzKhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.  Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy?\n",
        "\n"
      ],
      "metadata": {
        "id": "0n26LwoFzL0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 1: Train a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 2: Apply Cost Complexity Pruning (CCP)\n",
        "# Find the effective alphas (alpha values for pruning)\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# Step 3: Train pruned trees for each alpha value\n",
        "clfs = []\n",
        "for alpha in ccp_alphas:\n",
        "    clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
        "    clf_pruned.fit(X_train, y_train)\n",
        "    clfs.append(clf_pruned)\n",
        "\n",
        "# Step 4: Visualize accuracy vs alpha (effect of pruning)\n",
        "train_accuracies = [accuracy_score(y_train, clf.predict(X_train)) for clf in clfs]\n",
        "test_accuracies = [accuracy_score(y_test, clf.predict(X_test)) for clf in clfs]\n",
        "\n",
        "# Plot accuracy vs alpha\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(ccp_alphas, train_accuracies, label=\"Train Accuracy\", marker='o')\n",
        "plt.plot(ccp_alphas, test_accuracies, label=\"Test Accuracy\", marker='o')\n",
        "plt.xlabel(\"Alpha (ccp_alpha)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of Cost Complexity Pruning (CCP) on Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Display the pruned tree with the best alpha value\n",
        "# Find the best alpha based on test accuracy\n",
        "best_alpha = ccp_alphas[test_accuracies.index(max(test_accuracies))]\n",
        "clf_best_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha)\n",
        "clf_best_pruned.fit(X_train, y_train)\n",
        "\n",
        "# Visualize the pruned tree\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_tree(clf_best_pruned, filled=True, feature_names=iris.feature_names, class_names=iris.target_names, rounded=True)\n",
        "plt.title(f\"Pruned Decision Tree (Best Alpha: {best_alpha})\")\n",
        "plt.show()\n",
        "\n",
        "# Print the best alpha value and corresponding accuracy\n",
        "print(f\"Best alpha value: {best_alpha}\")\n",
        "print(f\"Best Test Accuracy: {max(test_accuracies):.2f}\")\n"
      ],
      "metadata": {
        "id": "IofkhhlUzUld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.  Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score?\n",
        "\n"
      ],
      "metadata": {
        "id": "A_tar7Fizicq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score for each class\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Weighted for multiclass\n",
        "recall = recall_score(y_test, y_pred, average='weighted')  # Weighted for multiclass\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Weighted for multiclass\n",
        "\n",
        "# Print Precision, Recall, and F1-Score\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Optionally, print a classification report for a more detailed analysis\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "id": "YT7fDVef0M3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn?\n"
      ],
      "metadata": {
        "id": "TjGcIuYT0bpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "Hx2ycFV70hzX",
        "outputId": "88e27c3d-0a3f-4deb-bb33-eb268f587d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.  Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split?\n",
        "\n"
      ],
      "metadata": {
        "id": "2SmjwYTk00xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, 10, None],  # Test different depths\n",
        "    'min_samples_split': [2, 5, 10, 15]  # Test different values for min_samples_split\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best Cross-validation Accuracy: {best_score:.4f}\")\n",
        "\n",
        "# Test the model with the best parameters on the test set\n",
        "best_clf = grid_search.best_estimator_\n",
        "y_pred = best_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "YZAa5zCp1LUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GYq_1zqV00La"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-954cF9HsUmb"
      }
    }
  ]
}