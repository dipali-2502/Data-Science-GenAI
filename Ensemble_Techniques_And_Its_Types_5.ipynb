{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. You are working on a machine learning project where you have a dataset containing numerical and\n",
        "categorical features. You have identified that some of the features are highly correlated and there are\n",
        "missing values in some of the columns. You want to build a pipeline that automates the feature\n",
        "engineering process and handles the missing values.\n",
        "\n",
        "\n",
        "ans.\n",
        "\n",
        "We are building a pipeline that:\n",
        "\n",
        "* Handles missing values\n",
        "* Encodes categorical variables\n",
        "* Scales numerical features\n",
        "* Removes multicollinearity\n",
        "* Selects important features automatically\n",
        "* Trains and evaluates a Random Forest classifier\n",
        "\n",
        "We'll use `scikit-learn`'s pipeline tools.\n",
        "\n",
        "\n",
        "###  **Step 1: Sample Dataset Creation**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample dataset\n",
        "df = pd.DataFrame({\n",
        "    'age': [25, 30, np.nan, 35, 45, 50, np.nan, 40],\n",
        "    'salary': [50000, 60000, 55000, np.nan, 70000, 65000, 62000, 72000],\n",
        "    'experience': [1, 3, 2, 4, 5, 6, 7, 8],\n",
        "    'gender': ['M', 'F', 'F', np.nan, 'M', 'M', 'F', 'F'],\n",
        "    'purchased': [1, 0, 1, 0, 1, 0, 1, 0]\n",
        "})\n",
        "\n",
        "X = df.drop(\"purchased\", axis=1)\n",
        "y = df[\"purchased\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "\n",
        "\n",
        "### ðŸ”¹ **Step 2: Preprocessing Pipelines**\n",
        "\n",
        "####  Numerical Pipeline:\n",
        "\n",
        "* Impute missing values using **mean**\n",
        "* Scale using **StandardScaler**\n",
        "\n",
        "####  Categorical Pipeline:\n",
        "\n",
        "* Impute missing using **most frequent**\n",
        "* Apply **OneHotEncoder**\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Identify column types\n",
        "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Numerical pipeline\n",
        "num_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical pipeline\n",
        "cat_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine pipelines\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_pipeline, num_cols),\n",
        "    ('cat', cat_pipeline, cat_cols)\n",
        "])\n",
        "```\n",
        "\n",
        "\n",
        "###  **Step 3: Add Feature Selection**\n",
        "\n",
        "We'll use `SelectFromModel` with a `RandomForestClassifier` to select important features.\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "```\n",
        "\n",
        "\n",
        "###  **Step 4: Full Pipeline with Model**\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Full pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('feature_selection', SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Train\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "###  **Explanation of Steps**\n",
        "\n",
        "| Step                     | Description                                                           |\n",
        "| ------------------------ | --------------------------------------------------------------------- |\n",
        "| **1. Imputation**        | Handles missing values using mean (numerical) and mode (categorical). |\n",
        "| **2. Scaling**           | Normalizes numerical features for better ML model performance.        |\n",
        "| **3. One-Hot Encoding**  | Converts categorical features into numeric form.                      |\n",
        "| **4. Feature Selection** | Keeps only important features based on Random Forest importance.      |\n",
        "| **5. Classification**    | Trains a `RandomForestClassifier` on the transformed data.            |\n",
        "| **6. Evaluation**        | Accuracy is printed as the performance metric.                        |\n",
        "\n",
        "\n",
        "###  **Interpretation of Results**\n",
        "\n",
        "If you run the code above, you'll get something like:\n",
        "\n",
        "```bash\n",
        "Model Accuracy: 1.00\n",
        "```\n",
        "\n",
        "(Results will vary due to small dataset.)\n",
        "\n",
        "\n",
        "###  **Possible Improvements**\n",
        "\n",
        "1. **Cross-validation**: Use `cross_val_score` for robust accuracy.\n",
        "2. **Hyperparameter tuning**: Use `GridSearchCV` for `RandomForestClassifier`.\n",
        "3. **Outlier handling**: Add a step to remove or cap outliers.\n",
        "4. **Feature interaction**: Try polynomial features or interactions.\n",
        "5. **Use pipelines on larger real-world data** for better generalization.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zx3c2GzChXC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then\n",
        "use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its\n",
        "accuracy.\n",
        "\n",
        "##  Ensemble Voting Classifier Pipeline on Iris Dataset\n",
        "\n",
        "###  **Goal**\n",
        "\n",
        "* Use both **Random Forest** and **Logistic Regression** in a pipeline.\n",
        "* Combine them using a **VotingClassifier**.\n",
        "* Train on the **Iris dataset**.\n",
        "* Evaluate the model accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "##  **Theory**\n",
        "\n",
        "| Component                    | Description                                                                                   |\n",
        "| ---------------------------- | --------------------------------------------------------------------------------------------- |\n",
        "| **Random Forest Classifier** | Ensemble of decision trees; good for capturing non-linear patterns.                           |\n",
        "| **Logistic Regression**      | A simple, fast, and interpretable linear model.                                               |\n",
        "| **Voting Classifier**        | Combines multiple models. Uses majority vote (for classification) to decide the final output. |\n",
        "| **Pipeline**                 | Ensures consistent preprocessing and model execution.                                         |\n",
        "\n",
        "---\n",
        "\n",
        "##  **Code Implementation**\n",
        "\n",
        "```python\n",
        "# Step 1: Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Step 3: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Create individual pipelines for models\n",
        "rf_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "lr_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', LogisticRegression(max_iter=200, random_state=42))\n",
        "])\n",
        "\n",
        "# Step 5: Create voting classifier\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('rf', rf_pipeline), ('lr', lr_pipeline)],\n",
        "    voting='hard'  # Use 'soft' for probabilities\n",
        ")\n",
        "\n",
        "# Step 6: Train and predict\n",
        "voting_clf.fit(X_train, y_train)\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Voting Classifier Accuracy on Iris Dataset: {accuracy:.2f}\")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "##  **Interpretation of Results**\n",
        "\n",
        "| Metric   | Value                                           |\n",
        "| -------- | ----------------------------------------------- |\n",
        "| Accuracy | Typically \\~0.93â€“1.00 depending on random split |\n",
        "\n",
        "* **Random Forest** captures complex patterns.\n",
        "* **Logistic Regression** adds robustness and interpretability.\n",
        "* **VotingClassifier** benefits from the strengths of both.\n",
        "\n",
        "\n",
        "\n",
        "##  **Possible Improvements**\n",
        "\n",
        "1. Use `voting='soft'` for probability-based voting.\n",
        "2. Tune hyperparameters with `GridSearchCV`.\n",
        "3. Try other classifiers like `KNeighbors`, `SVC`, `GradientBoosting`.\n",
        "\n"
      ],
      "metadata": {
        "id": "-u1Q_2A1xTgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4XPdN2vDwA3K"
      }
    }
  ]
}