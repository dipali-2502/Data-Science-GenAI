# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZwXs_jnQ1lJETcTHPZvjaYT76LKqmft-

###Theoretical

1. What is a Decision Tree, and how does it work?

ans. **Decision Tree--**  
A **Decision Tree** is a type of machine learning model that looks like a flowchart. It helps make decisions by asking a series of **yes/no** or **true/false** questions.  
At each step, it splits the data based on the answers, moving down different "branches" until it reaches a final decision or prediction (which we call a "leaf node").


**How does a Decision Tree work?**
1. **Start at the root** (the top of the tree).
2. **Choose the best feature** (or question) to split the data.  
   - This is done using metrics like **Gini Impurity**, **Entropy**, or **Information Gain** (for classification) or **variance reduction** (for regression).
3. **Split the data** into groups based on the answer.
4. **Repeat** the process for each group (sub-tree) until:
   - You meet a stopping condition (like tree depth limit or minimum samples).
   - Or all the data in a branch belongs to one class.

2. What are impurity measures in Decision Trees	?

ans. What are impurity measures in Decision Trees?
An impurity measure tells us how mixed up the data is at a certain point in the tree.

If a group (node) has only one class (say, all "yes" or all "no"), the impurity is low (or zero ‚Äî meaning it's pure).

If the group has a mix of classes (like 50% "yes" and 50% "no"), the impurity is high.

When building a Decision Tree, we always try to reduce impurity as much as possible at each split ‚Äî so each branch becomes as "pure" as it can.

3.  What is the mathematical formula for Gini Impurity	?

ans. **mathematical formula** for **Gini Impurity**:

\[
Gini = 1 - \sum_{i=1}^{n} p_i^2
\]

Where:
- \( p_i \) = the probability of picking an item of class \( i \) from the node
- \( n \) = total number of class

4. What is the mathematical formula for Entropy	?

ans.  **mathematical formula** for **Entropy**:

\[
Entropy = -\sum_{i=1}^{n} p_i \log_2(p_i)
\]

Where:
- \( p_i \) = probability of class \( i \) at the node
- \( n \) = total number of classes

5. What is Information Gain, and how is it used in Decision Trees	?

ans.  Information Gain (IG) measures how much "uncertainty" or "disorder" decreases after splitting the data based on a feature.

In short:

Before split: You have some disorder (measured by Entropy).

After split: You check how much the disorder has reduced.

Information Gain = (Entropy before split) ‚Äì (Weighted entropy after split)

The feature with the highest Information Gain is chosen to split the data at that step!

Mathematical formula for Information Gain:

ùêº
ùê∫
=
ùê∏
ùëõ
ùë°
ùëü
ùëú
ùëù
ùë¶
(
parent
)
‚àí
‚àë
ùëò
=
1
ùêæ
(
ùëÅ
ùëò
ùëÅ
√ó
ùê∏
ùëõ
ùë°
ùëü
ùëú
ùëù
ùë¶
(
child
ùëò
)
)
IG=Entropy(parent)‚àí
k=1
‚àë
K
‚Äã
 (
N
N
k
‚Äã

‚Äã
 √óEntropy(child
k
‚Äã
 ))
Where:

ùëÅ
N = total number of samples in parent

ùëÅ
ùëò
N
k
‚Äã
  = number of samples in child
ùëò
k

ùêæ
K = number of child nodes

How is it used in Decision Trees?
At each node:

Calculate the Entropy of the current data (parent node).

For each feature:

Imagine splitting the data based on that feature.

Calculate the weighted Entropy of resulting groups (children).

Find the Information Gain.

Choose the feature with the highest Information Gain to split.

Goal: Split the data to make it as pure as possible, as fast as possible!

6. What is the difference between Gini Impurity and Entropy	?

ans. Gini Impurity vs. Entropy

Aspect	Gini Impurity	Entropy
Meaning	Measures how often a randomly chosen element would be incorrectly labeled if it was randomly labeled according to the distribution of labels.	Measures the amount of "disorder" or "uncertainty" in the dataset.
Formula
ùê∫
ùëñ
ùëõ
ùëñ
=
1
‚àí
‚àë
ùëñ
=
1
ùëõ
ùëù
ùëñ
2
Gini=1‚àí‚àë
i=1
n
‚Äã
 p
i
2
‚Äã

ùê∏
ùëõ
ùë°
ùëü
ùëú
ùëù
ùë¶
=
‚àí
‚àë
ùëñ
=
1
ùëõ
ùëù
ùëñ
log
‚Å°
2
(
ùëù
ùëñ
)
Entropy=‚àí‚àë
i=1
n
‚Äã
 p
i
‚Äã
 log
2
‚Äã
 (p
i
‚Äã
 )
Value Range	Between 0 and 0.5 (for binary classification).	Between 0 and 1 (for binary classification).
Interpretation	Lower Gini means purer groups.	Lower Entropy means purer groups.
Calculation Speed	Faster to compute (no logarithms).	Slightly slower (uses logarithms).
Usage	Often used in CART (Classification and Regression Trees).	Often used in ID3, C4.5 decision tree algorithms.
Preference	Good for when we want quicker, simpler trees.	Good when you want more information-theoretic splitting.

7. What is the mathematical explanation behind Decision Trees	?

ans. Decision Trees split the data into subsets based on the value of features to make the groups purer.

Mathematically:

At each node, calculate an impurity measure (like Gini or Entropy).

For each feature, compute how much splitting on it reduces impurity (Information Gain or Gini Gain).

Choose the feature with the highest gain to split the node.

Repeat the process for each child node until a stopping condition is met (like pure nodes or maximum depth).

The tree grows by recursively minimizing impurity

8. What is Pre-Pruning in Decision Trees	?

ans. Pre-Pruning (also called early stopping) means stopping the tree growth early, before it becomes too complex, to prevent overfitting.

Instead of growing the full tree and then trimming it, in pre-pruning we apply conditions during tree building to decide when to stop splitting.

9. What is Post-Pruning in Decision Trees	?

ans. Post-Pruning means first growing the full Decision Tree and then trimming it back to remove unnecessary branches, after it has been built.

It simplifies the tree by removing splits that do not improve the model's performance significantly, usually based on validation set results.

10. What is the difference between Pre-Pruning and Post-Pruning	?

ans.   
###Pre-Pruning (Early Stopping)

In Pre-Pruning, the Decision Tree is restricted while it is being built.
The tree stops growing when a certain condition is met, such as:

Maximum depth reached.

Minimum number of samples in a node.

Gain in impurity reduction is below a threshold.

Purpose:

To prevent the tree from becoming too large and complex.

To avoid overfitting the training data early.

Drawback:

If the tree is stopped too soon, it may underfit the data (miss important patterns).

###post-Pruning (Pruning After Full Growth)

In Post-Pruning, the Decision Tree is allowed to grow fully without restrictions.
After the tree is built:

It is analyzed to find parts (branches) that do not contribute much to prediction accuracy.

These branches are removed (pruned) based on performance on a validation set or cost-complexity criteria.

Purpose:

To reduce overfitting by simplifying the tree after it captures all possible patterns.

Advantage:

Post-pruning usually produces a tree that balances high accuracy and simplicity.

Final Point:
Pre-Pruning tries to control complexity during training.

Post-Pruning tries to correct complexity after training.

Both methods aim to create a tree that generalizes better to unseen data.

11. What is a Decision Tree Regressor	?

ans. ‚ÄãA Decision Tree Regressor is a supervised machine learning algorithm used for predicting continuous numerical values. It operates by partitioning the data into subsets based on feature values, constructing a tree-like model where each internal node represents a decision on a feature, and each leaf node represents a predicted numerical value.‚Äã

12.  What are the advantages and disadvantages of Decision Trees	?

ans.

**Advantages of Decision Trees:**

- Easy to understand and interpret  
- Little data preparation needed  
- Handles classification and regression  
- Captures nonlinear patterns  
- Provides feature importance  
- Can handle missing values

**Disadvantages of Decision Trees:**

- Prone to overfitting  
- Sensitive to data changes  
- Biased with imbalanced data  
- Finds local, not global, optimum  
- Poor at modeling smooth trends  
- Weaker than ensemble methods

13.  How does a Decision Tree handle missing values	?

ans.
A **Decision Tree** can handle missing values in two main ways, depending on the implementation:

1. **Ignoring missing values during splits**  
   The tree splits only on available (non-missing) data when choosing the best split at each node.

2. **Surrogate splits (used in some algorithms, like CART)**  
   When a value is missing for the main splitting feature, the tree uses another correlated feature (surrogate) to decide which branch to follow.

Some simpler implementations may just **exclude rows with missing data** or require preprocessing before training, so it depends on the software or library used.

14.  How does a Decision Tree handle categorical features	?

 ans.
A **Decision Tree** handles categorical features by splitting the data based on the category values.  

Specifically:

1. **For binary splits (e.g., CART):**  
   It groups category values into two subsets that best separate the target (for example, {A, B} vs. {C, D}) and splits the node accordingly.

2. **For multiway splits (e.g., ID3, C4.5):**  
   It creates a separate branch for each category (for example, one branch each for A, B, C, D).

3. **Using one-hot or ordinal encoding (sometimes required before training):**  
   In some implementations, you first convert categories into numbers or dummy variables.

So, decision trees can **natively handle categorical features** without needing you to preconvert them in most modern libraries.

15.  What are some real-world applications of Decision Trees?

ans.  1. **Medical diagnosis** ‚Äî predicting diseases based on symptoms and test results.  
2. **Credit scoring** ‚Äî evaluating if a person qualifies for a loan or credit card.  
3. **Customer churn prediction** ‚Äî identifying customers likely to leave a service.  
4. **Fraud detection** ‚Äî spotting suspicious transactions in banking or e-commerce.  
5. **Marketing** ‚Äî segmenting customers and predicting responses to campaigns.  
6. **Manufacturing** ‚Äî detecting equipment faults or predicting maintenance needs.  
7. **Human resources** ‚Äî screening job candidates or predicting employee turnover.  
8. **Healthcare** ‚Äî selecting treatment plans based on patient profiles.

If you want, I can also give you two or three short example case studies! Let me know.

###Practical

16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy?
"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Calculate and print the accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model accuracy: {accuracy:.2f}")

"""17.  Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances?


"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the Decision Tree Classifier using Gini impurity
clf = DecisionTreeClassifier(criterion='gini', random_state=42)
clf.fit(X_train, y_train)

# Print feature importances
print("Feature importances:")
for feature, importance in zip(iris.feature_names, clf.feature_importances_):
    print(f"{feature}: {importance:.4f}")

"""18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy?


"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the Decision Tree Classifier using Entropy
clf = DecisionTreeClassifier(criterion='entropy', random_state=42)
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Calculate and print the accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model accuracy using Entropy: {accuracy:.2f}")

"""19.  Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)?



"""

from sklearn.datasets import fetch_california_housing
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Load the California housing dataset
data = fetch_california_housing()
X = data.data
y = data.target

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the Decision Tree Regressor
regressor = DecisionTreeRegressor(random_state=42)
regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred = regressor.predict(X_test)

# Calculate and print the Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.2f}")

"""20.  Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz?

"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.model_selection import train_test_split
from sklearn import tree
import graphviz
import pydotplus

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Visualize the Decision Tree using Graphviz
dot_data = export_graphviz(clf, out_file=None,
                           feature_names=iris.feature_names,
                           class_names=iris.target_names,
                           filled=True, rounded=True,
                           special_characters=True)

# Create the Graphviz source and display the tree
graph = graphviz.Source(dot_data)
graph.render("decision_tree")  # Save the tree as a PDF file
graph.view()  # View the tree in the default PDF viewer

"""21.  Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree?


"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Decision Tree Classifier with no max depth (fully grown tree)
clf_full = DecisionTreeClassifier(random_state=42)
clf_full.fit(X_train, y_train)

# Make predictions on both the training set and the test set
y_train_pred_full = clf_full.predict(X_train)
y_test_pred_full = clf_full.predict(X_test)

# Calculate and print the accuracy on both sets
train_accuracy_full = accuracy_score(y_train, y_train_pred_full)
test_accuracy_full = accuracy_score(y_test, y_test_pred_full)

print(f"Training Accuracy (Fully Grown Tree): {train_accuracy_full:.2f}")
print(f"Test Accuracy (Fully Grown Tree): {test_accuracy_full:.2f}")

"""22.  Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree?


"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the default Decision Tree Classifier
clf_default = DecisionTreeClassifier(random_state=42)
clf_default.fit(X_train, y_train)

# Create and train the Decision Tree Classifier with min_samples_split=5
clf_min_samples_split = DecisionTreeClassifier(min_samples_split=5, random_state=42)
clf_min_samples_split.fit(X_train, y_train)

# Make predictions on both the training set and the test set for both classifiers
y_train_pred_default = clf_default.predict(X_train)
y_test_pred_default = clf_default.predict(X_test)

y_train_pred_split = clf_min_samples_split.predict(X_train)
y_test_pred_split = clf_min_samples_split.predict(X_test)

# Calculate and print the accuracy on both sets for both models
train_accuracy_default = accuracy_score(y_train, y_train_pred_default)
test_accuracy_default = accuracy_score(y_test, y_test_pred_default)

train_accuracy_split = accuracy_score(y_train, y_train_pred_split)
test_accuracy_split = accuracy_score(y_test, y_test_pred_split)

print(f"Default Tree Training Accuracy: {train_accuracy_default:.2f}")
print(f"Default Tree Test Accuracy: {test_accuracy_default:.2f}")

print(f"Tree with min_samples_split=5 Training Accuracy: {train_accuracy_split:.2f}")
print(f"Tree with min_samples_split=5 Test Accuracy: {test_accuracy_split:.2f}")

"""23.  Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data?


"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Without Feature Scaling ---
# Create and train the Decision Tree Classifier without scaling
clf_unscaled = DecisionTreeClassifier(random_state=42)
clf_unscaled.fit(X_train, y_train)

# Make predictions on the test set
y_train_pred_unscaled = clf_unscaled.predict(X_train)
y_test_pred_unscaled = clf_unscaled.predict(X_test)

# Calculate and print the accuracy for unscaled data
train_accuracy_unscaled = accuracy_score(y_train, y_train_pred_unscaled)
test_accuracy_unscaled = accuracy_score(y_test, y_test_pred_unscaled)

# --- With Feature Scaling ---
# Apply StandardScaler to the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create and train the Decision Tree Classifier with scaled data
clf_scaled = DecisionTreeClassifier(random_state=42)
clf_scaled.fit(X_train_scaled, y_train)

# Make predictions on the scaled test set
y_train_pred_scaled = clf_scaled.predict(X_train_scaled)
y_test_pred_scaled = clf_scaled.predict(X_test_scaled)

# Calculate and print the accuracy for scaled data
train_accuracy_scaled = accuracy_score(y_train, y_train_pred_scaled)
test_accuracy_scaled = accuracy_score(y_test, y_test_pred_scaled)

# Output the results
print(f"Accuracy without Feature Scaling:")
print(f"  Training Accuracy: {train_accuracy_unscaled:.2f}")
print(f"  Test Accuracy: {test_accuracy_unscaled:.2f}")

print(f"\nAccuracy with Feature Scaling:")
print(f"  Training Accuracy: {train_accuracy_scaled:.2f}")
print(f"  Test Accuracy: {test_accuracy_scaled:.2f}")

"""24.  Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification?


"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Wrap the classifier with OneVsRest strategy
ovr_clf = OneVsRestClassifier(clf)

# Train the model
ovr_clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = ovr_clf.predict(X_test)

# Calculate and print the accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy with One-vs-Rest strategy: {accuracy:.2f}")

"""25.  Write a Python program to train a Decision Tree Classifier and display the feature importance scores?

"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
import pandas as pd

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

# Create and train the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X, y)

# Get the feature importance scores
importance_scores = clf.feature_importances_

# Create a DataFrame to display the feature names and their importance scores
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance Score': importance_scores
})

# Sort the DataFrame by importance score in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance Score', ascending=False)

# Display the feature importance scores
print("Feature Importance Scores:")
print(feature_importance_df)

"""26.  Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree?


"""

from sklearn.datasets import load_boston
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Load the Boston housing dataset
boston = load_boston()
X = boston.data
y = boston.target

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Unrestricted Decision Tree Regressor ---
# Create and train the unrestricted Decision Tree Regressor
clf_unrestricted = DecisionTreeRegressor(random_state=42)
clf_unrestricted.fit(X_train, y_train)

# Make predictions on the test set
y_pred_unrestricted = clf_unrestricted.predict(X_test)

# Calculate the Mean Squared Error (MSE) for the unrestricted model
mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)

# --- Restricted Decision Tree Regressor (max_depth=5) ---
# Create and train the Decision Tree Regressor with max_depth=5
clf_restricted = DecisionTreeRegressor(max_depth=5, random_state=42)
clf_restricted.fit(X_train, y_train)

# Make predictions on the test set
y_pred_restricted = clf_restricted.predict(X_test)

# Calculate the Mean Squared Error (MSE) for the restricted model
mse_restricted = mean_squared_error(y_test, y_pred_restricted)

# Output the results
print(f"Unrestricted Tree MSE: {mse_unrestricted:.2f}")
print(f"Restricted Tree (max_depth=5) MSE: {mse_restricted:.2f}")

"""27.  Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy?


"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 1: Train a Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Step 2: Apply Cost Complexity Pruning (CCP)
# Find the effective alphas (alpha values for pruning)
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities

# Step 3: Train pruned trees for each alpha value
clfs = []
for alpha in ccp_alphas:
    clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)
    clf_pruned.fit(X_train, y_train)
    clfs.append(clf_pruned)

# Step 4: Visualize accuracy vs alpha (effect of pruning)
train_accuracies = [accuracy_score(y_train, clf.predict(X_train)) for clf in clfs]
test_accuracies = [accuracy_score(y_test, clf.predict(X_test)) for clf in clfs]

# Plot accuracy vs alpha
plt.figure(figsize=(10, 6))
plt.plot(ccp_alphas, train_accuracies, label="Train Accuracy", marker='o')
plt.plot(ccp_alphas, test_accuracies, label="Test Accuracy", marker='o')
plt.xlabel("Alpha (ccp_alpha)")
plt.ylabel("Accuracy")
plt.title("Effect of Cost Complexity Pruning (CCP) on Accuracy")
plt.legend()
plt.grid(True)
plt.show()

# Step 5: Display the pruned tree with the best alpha value
# Find the best alpha based on test accuracy
best_alpha = ccp_alphas[test_accuracies.index(max(test_accuracies))]
clf_best_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha)
clf_best_pruned.fit(X_train, y_train)

# Visualize the pruned tree
plt.figure(figsize=(10, 8))
plot_tree(clf_best_pruned, filled=True, feature_names=iris.feature_names, class_names=iris.target_names, rounded=True)
plt.title(f"Pruned Decision Tree (Best Alpha: {best_alpha})")
plt.show()

# Print the best alpha value and corresponding accuracy
print(f"Best alpha value: {best_alpha}")
print(f"Best Test Accuracy: {max(test_accuracies):.2f}")

"""28.  Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score?


"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Calculate Precision, Recall, and F1-Score for each class
precision = precision_score(y_test, y_pred, average='weighted')  # Weighted for multiclass
recall = recall_score(y_test, y_pred, average='weighted')  # Weighted for multiclass
f1 = f1_score(y_test, y_pred, average='weighted')  # Weighted for multiclass

# Print Precision, Recall, and F1-Score
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

# Optionally, print a classification report for a more detailed analysis
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

"""29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn?

"""

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Visualize the confusion matrix using seaborn heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.title('Confusion Matrix for Decision Tree Classifier')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""30.  Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split?


"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Define the parameter grid to search
param_grid = {
    'max_depth': [3, 5, 7, 10, None],  # Test different depths
    'min_samples_split': [2, 5, 10, 15]  # Test different values for min_samples_split
}

# Apply GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Get the best parameters and the best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Best Hyperparameters: {best_params}")
print(f"Best Cross-validation Accuracy: {best_score:.4f}")

# Test the model with the best parameters on the test set
best_clf = grid_search.best_estimator_
y_pred = best_clf.predict(X_test)

# Calculate accuracy on the test set
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {test_accuracy:.4f}")