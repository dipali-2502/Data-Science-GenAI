{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
        "\n",
        "\n",
        "ans.   Missing Values in a Dataset?\n",
        "\n",
        "**Missing values** are entries in a dataset where no data value is stored for a variable in an observation. This can happen due to various reasons such as:\n",
        "\n",
        "* Errors in data collection\n",
        "* Data corruption\n",
        "* Incomplete data entry\n",
        "* Sensor or system failures\n",
        "\n",
        "\n",
        "\n",
        "###  Why is it Essential to Handle Missing Values?\n",
        "\n",
        "Handling missing values is crucial because:\n",
        "\n",
        "1. **Bias Prevention**: Missing data can skew or bias the results.\n",
        "2. **Model Accuracy**: Many machine learning algorithms can't handle missing values and may crash or give incorrect results.\n",
        "3. **Data Integrity**: Ensures that conclusions drawn from the data are valid.\n",
        "4. **Performance**: Unhandled missing values can negatively impact model training and prediction.\n",
        "\n",
        "\n",
        "\n",
        "###  Algorithms That Are Not Affected by Missing Values:\n",
        "\n",
        "Some algorithms can **handle missing data internally** or are **less sensitive** to it. Examples include:\n",
        "\n",
        "1. **XGBoost (Extreme Gradient Boosting)**\n",
        "2. **LightGBM (Light Gradient Boosting Machine)**\n",
        "3. **CatBoost**\n",
        "4. **k-Nearest Neighbors (k-NN)** (if imputation is built in)\n",
        "5. **Some decision tree-based models** (e.g., CART can handle missing values by surrogate splits)\n",
        "\n",
        ">  Note: While some models tolerate missing values, **it's still good practice** to analyze and impute or address them appropriately.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wKs7DWktwG60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: List down techniques used to handle missing data.  Give an example of each with python code."
      ],
      "metadata": {
        "id": "5X4UVeYHwqAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample DataFrame with missing values\n",
        "df = pd.DataFrame({\n",
        "    'Name': ['Alice', 'Bob', np.nan, 'David'],\n",
        "    'Age': [25, np.nan, 30, 40]\n",
        "})\n",
        "\n",
        "# 1. Remove rows with missing data\n",
        "df_dropped = df.dropna()\n",
        "\n",
        "# 2. Mean/Median/Mode Imputation\n",
        "df['Age_mean'] = df['Age'].fillna(df['Age'].mean())  # Mean\n",
        "df['Age_median'] = df['Age'].fillna(df['Age'].median())  # Median\n",
        "\n",
        "# 3. Forward Fill / Backward Fill\n",
        "df['Age_ffill'] = df['Age'].fillna(method='ffill')  # Forward fill\n",
        "df['Age_bfill'] = df['Age'].fillna(method='bfill')  # Backward fill\n",
        "\n",
        "# 4. Interpolation\n",
        "df['Age_interp'] = df['Age'].interpolate()  # Linear interpolation\n",
        "\n",
        "# 5. Constant Imputation\n",
        "df['Name_const'] = df['Name'].fillna('Unknown')\n",
        "\n",
        "# 6. Predictive Imputation (Linear Regression)\n",
        "# Encode Name column\n",
        "df['Name_enc'] = df['Name'].astype('category').cat.codes\n",
        "train = df[df['Age'].notnull()]\n",
        "test = df[df['Age'].isnull()]\n",
        "model = LinearRegression()\n",
        "model.fit(train[['Name_enc']], train['Age'])\n",
        "df.loc[df['Age'].isnull(), 'Age_predicted'] = model.predict(test[['Name_enc']])\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QMlClh6xhUo",
        "outputId": "fcdb7899-8db1-424d-8f54-7e7273a2c544"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Name   Age   Age_mean  Age_median  Age_ffill  Age_bfill  Age_interp  \\\n",
            "0  Alice  25.0  25.000000        25.0       25.0       25.0        25.0   \n",
            "1    Bob   NaN  31.666667        30.0       25.0       30.0        27.5   \n",
            "2    NaN  30.0  30.000000        30.0       30.0       30.0        30.0   \n",
            "3  David  40.0  40.000000        40.0       40.0       40.0        40.0   \n",
            "\n",
            "  Name_const  Name_enc  Age_predicted  \n",
            "0      Alice         0            NaN  \n",
            "1        Bob         1      34.285714  \n",
            "2    Unknown        -1            NaN  \n",
            "3      David         2            NaN  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-708e8ba2318d>:19: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df['Age_ffill'] = df['Age'].fillna(method='ffill')  # Forward fill\n",
            "<ipython-input-1-708e8ba2318d>:20: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df['Age_bfill'] = df['Age'].fillna(method='bfill')  # Backward fill\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
      ],
      "metadata": {
        "id": "xSzfzsaBxkRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalanced data refers to a situation in classification problems where the number of observations in each class is significantly different. For example, in a binary classification problem for fraud detection:\n",
        "\n",
        "\n",
        "ans.\n",
        "\n",
        "Class 0 (Non-fraud): 98%\n",
        "\n",
        "Class 1 (Fraud): 2%\n",
        "\n",
        "This imbalance can bias the model toward the majority class.\n",
        "\n",
        " What Happens if Imbalanced Data Is Not Handled?\n",
        "If you don’t handle imbalanced data:\n",
        "\n",
        "Misleading Accuracy\n",
        "The model may show high accuracy just by predicting the majority class.\n",
        "Example: Predicting \"Non-fraud\" for every case gives 98% accuracy but is useless for finding fraud.\n",
        "\n",
        "Poor Recall for Minority Class\n",
        "The model fails to correctly identify important minority class events like fraud, disease, defects, etc.\n",
        "\n",
        "Biased Predictions\n",
        "The model becomes biased toward the majority class, ignoring the minority class patterns.\n",
        "\n",
        " Why Is It Critical?\n",
        "In many applications like:\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "Medical diagnosis\n",
        "\n",
        "Spam detection\n",
        "the minority class is more important, and missing it can lead to serious consequences.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-TuViszVwGwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and downsampling are required."
      ],
      "metadata": {
        "id": "_Lw6hhStyWTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Example dataframe\n",
        "data = {\n",
        "    'feature': [1,2,3,4,5,6,7,8,9,10],\n",
        "    'class':   [0,0,0,0,0,1,1,1,1,1]  # 0 = majority, 1 = minority\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "majority = df[df['class'] == 0]\n",
        "minority = df[df['class'] == 1]\n",
        "\n",
        "# Down-sample majority class to match minority class size\n",
        "majority_downsampled = resample(majority, replace=False, n_samples=len(minority), random_state=42)\n",
        "\n",
        "# Combine downsampled majority and minority class\n",
        "df_downsampled = pd.concat([majority_downsampled, minority])\n",
        "\n",
        "print(\"Downsampled dataset:\")\n",
        "print(df_downsampled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzCHLw-Hy63B",
        "outputId": "facd3e67-8f8c-4fcc-8a14-c5d42729b301"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downsampled dataset:\n",
            "   feature  class\n",
            "1        2      0\n",
            "4        5      0\n",
            "2        3      0\n",
            "0        1      0\n",
            "3        4      0\n",
            "5        6      1\n",
            "6        7      1\n",
            "7        8      1\n",
            "8        9      1\n",
            "9       10      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: What is data Augmentation? Explain SMOTE."
      ],
      "metadata": {
        "id": "d-HvKpNsy8uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6],  # Majority class\n",
        "              [6, 7], [7, 8]])                          # Minority class\n",
        "y = np.array([0, 0, 0, 0, 0, 1, 1])  # 0 = majority, 1 = minority\n",
        "\n",
        "print('Original dataset shape:', Counter(y))\n",
        "\n",
        "# Set k_neighbors less than number of minority samples (which is 2 here)\n",
        "smote = SMOTE(random_state=42, k_neighbors=1)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "print('Resampled dataset shape:', Counter(y_resampled))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBlyBfJh4GxA",
        "outputId": "85956dbd-7819-4960-ca6d-9b1a2c017af7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape: Counter({np.int64(0): 5, np.int64(1): 2})\n",
            "Resampled dataset shape: Counter({np.int64(0): 5, np.int64(1): 5})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
        "\n",
        "ans.\n",
        "\n",
        "**Outliers:**  \n",
        "Outliers are data points that differ significantly from other observations in the dataset. They lie far away from the majority of the data, either much higher or much lower than most values.\n",
        "\n",
        "**Why handle outliers?**\n",
        "\n",
        "* **Distort statistical analysis:** Outliers can skew mean, variance, and correlation, leading to misleading conclusions.\n",
        "* **Affect model performance:** Many machine learning algorithms assume data is normally distributed and can perform poorly if outliers exist.\n",
        "* **Impact visualization:** Outliers can stretch scales in plots, hiding the overall data pattern.\n",
        "* **Data quality issues:** Outliers may indicate data entry errors, measurement errors, or rare events that need special attention.\n",
        "\n",
        "Handling outliers ensures more reliable, robust, and interpretable data analysis and modeling.\n"
      ],
      "metadata": {
        "id": "3mkDCJHS4I5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
        "\n",
        "\n",
        "Ans.\n",
        "1. **Remove missing data:**\n",
        "\n",
        "   * Remove rows or columns with missing values if the missingness is small or not important.\n",
        "   * Example: `df.dropna()` removes rows with any missing values.\n",
        "\n",
        "2. **Imputation:**\n",
        "\n",
        "   * Fill missing values with a specific value, such as mean, median, or mode.\n",
        "   * Example: `df['age'].fillna(df['age'].mean(), inplace=True)`\n",
        "\n",
        "3. **Predictive imputation:**\n",
        "\n",
        "   * Use models (like regression, k-NN) to predict missing values based on other features.\n",
        "\n",
        "4. **Using indicators:**\n",
        "\n",
        "   * Create a new boolean feature indicating where data was missing.\n",
        "\n",
        "5. **Forward or backward fill:**\n",
        "\n",
        "   * Fill missing values using previous or next valid value (useful in time series).\n",
        "   * Example: `df.fillna(method='ffill')`\n",
        "\n",
        "6. **Model-based handling:**\n",
        "\n",
        "   * Use algorithms that handle missing data internally, e.g., XGBoost, Random Forest.\n",
        "\n",
        "7. **Treat missing data as a separate category:**\n",
        "\n",
        "   * For categorical variables, consider missing as its own category.\n",
        "\n",
        "\n",
        "\n",
        "Selecting the right technique depends on the data nature, amount of missingness, and analysis goals.\n"
      ],
      "metadata": {
        "id": "CBTi6uNu4WKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
        "\n"
      ],
      "metadata": {
        "id": "GurQSGb35FUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
        "\n",
        "To determine whether data is Missing Completely at Random (MCAR), Missing at Random (MAR), or Not Missing at Random (NMAR), you can use the following strategies:\n",
        "\n",
        "1. Visual Inspection:\n",
        "   - Use heatmaps or visualization tools to check for missing data patterns.\n",
        "   Example:\n",
        "   import seaborn as sns\n",
        "   sns.heatmap(df.isnull(), cbar=False)\n",
        "\n",
        "   import missingno as msno\n",
        "   msno.matrix(df)\n",
        "   msno.heatmap(df)\n",
        "\n",
        "2. Summary Statistics Comparison:\n",
        "   - Compare distributions (mean, median, etc.) of other columns grouped by missingness.\n",
        "   Example:\n",
        "   df.groupby(df['age'].isnull())['income'].mean()\n",
        "\n",
        "3. Correlation with Missingness:\n",
        "   - Create an indicator variable for missing values and find correlation with other columns.\n",
        "   Example:\n",
        "   df['age_missing'] = df['age'].isnull().astype(int)\n",
        "   print(df.corr()['age_missing'])\n",
        "\n",
        "4. Little’s MCAR Test:\n",
        "   - A statistical test (available in R and in some Python packages) to test if data is MCAR.\n",
        "\n",
        "5. Domain Knowledge:\n",
        "   - Use contextual understanding to identify reasons for missingness. For example, income data might be missing for unemployed individuals.\n",
        "\n",
        "Summary:\n",
        "- MCAR: Missingness is independent of any data → safe to drop.\n",
        "- MAR: Missingness depends on observed data → use imputation.\n",
        "- NMAR: Missingness depends on unobserved data → difficult to handle; may require advanced modeling.\n",
        "\n",
        "Understanding the type of missing data helps in choosing the right imputation or data-cleaning strategy.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "alZMu3eh5hWm",
        "outputId": "6b0ce011-376a-4d2a-b82c-05e07d7c34fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nQ8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\\n\\nTo determine whether data is Missing Completely at Random (MCAR), Missing at Random (MAR), or Not Missing at Random (NMAR), you can use the following strategies:\\n\\n1. Visual Inspection:\\n   - Use heatmaps or visualization tools to check for missing data patterns.\\n   Example:\\n   import seaborn as sns\\n   sns.heatmap(df.isnull(), cbar=False)\\n\\n   import missingno as msno\\n   msno.matrix(df)\\n   msno.heatmap(df)\\n\\n2. Summary Statistics Comparison:\\n   - Compare distributions (mean, median, etc.) of other columns grouped by missingness.\\n   Example:\\n   df.groupby(df['age'].isnull())['income'].mean()\\n\\n3. Correlation with Missingness:\\n   - Create an indicator variable for missing values and find correlation with other columns.\\n   Example:\\n   df['age_missing'] = df['age'].isnull().astype(int)\\n   print(df.corr()['age_missing'])\\n\\n4. Little’s MCAR Test:\\n   - A statistical test (available in R and in some Python packages) to test if data is MCAR.\\n\\n5. Domain Knowledge:\\n   - Use contextual understanding to identify reasons for missingness. For example, income data might be missing for unemployed individuals.\\n\\nSummary:\\n- MCAR: Missingness is independent of any data → safe to drop.\\n- MAR: Missingness depends on observed data → use imputation.\\n- NMAR: Missingness depends on unobserved data → difficult to handle; may require advanced modeling.\\n\\nUnderstanding the type of missing data helps in choosing the right imputation or data-cleaning strategy.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
        "\n"
      ],
      "metadata": {
        "id": "bklUTpVb5tMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
        "\n",
        "When dealing with imbalanced datasets in medical diagnosis (where false negatives can be costly), traditional accuracy may be misleading. Use the following evaluation strategies:\n",
        "\n",
        "1. Confusion Matrix:\n",
        "   - Gives a detailed breakdown of TP, TN, FP, and FN.\n",
        "   - Helps understand how many positive cases are missed.\n",
        "\n",
        "2. Precision, Recall, and F1-Score:\n",
        "   - Precision = TP / (TP + FP): Focuses on minimizing false positives.\n",
        "   - Recall = TP / (TP + FN): Important when false negatives are costly (e.g., undiagnosed disease).\n",
        "   - F1-Score = Harmonic mean of Precision and Recall.\n",
        "\n",
        "3. ROC Curve and AUC (Area Under Curve):\n",
        "   - AUC-ROC shows the trade-off between TPR and FPR.\n",
        "   - Better for visualizing model performance on imbalanced data.\n",
        "\n",
        "4. Precision-Recall Curve:\n",
        "   - More informative than ROC when the dataset is highly imbalanced.\n",
        "   - Focuses on performance for the minority class.\n",
        "\n",
        "5. Stratified K-Fold Cross-Validation:\n",
        "   - Ensures each fold maintains the same class distribution as the overall dataset.\n",
        "\n",
        "6. Use Class Weights:\n",
        "   - Pass `class_weight='balanced'` to classifiers like Logistic Regression or RandomForestClassifier.\n",
        "\n",
        "7. Threshold Tuning:\n",
        "   - Adjust the decision threshold (default is 0.5) to increase sensitivity towards minority class.\n",
        "\n",
        "Example (in sklearn):\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))\n",
        "\n",
        "Conclusion:\n",
        "Choose evaluation metrics that reflect the cost of misclassification, especially focusing on recall and F1-score in medical applications.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "wC6cOTd55-R9",
        "outputId": "d8fd4fc4-ad81-444c-f1c3-5153d8e9ecf6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nQ9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\\n\\nWhen dealing with imbalanced datasets in medical diagnosis (where false negatives can be costly), traditional accuracy may be misleading. Use the following evaluation strategies:\\n\\n1. Confusion Matrix:\\n   - Gives a detailed breakdown of TP, TN, FP, and FN.\\n   - Helps understand how many positive cases are missed.\\n\\n2. Precision, Recall, and F1-Score:\\n   - Precision = TP / (TP + FP): Focuses on minimizing false positives.\\n   - Recall = TP / (TP + FN): Important when false negatives are costly (e.g., undiagnosed disease).\\n   - F1-Score = Harmonic mean of Precision and Recall.\\n\\n3. ROC Curve and AUC (Area Under Curve):\\n   - AUC-ROC shows the trade-off between TPR and FPR.\\n   - Better for visualizing model performance on imbalanced data.\\n\\n4. Precision-Recall Curve:\\n   - More informative than ROC when the dataset is highly imbalanced.\\n   - Focuses on performance for the minority class.\\n\\n5. Stratified K-Fold Cross-Validation:\\n   - Ensures each fold maintains the same class distribution as the overall dataset.\\n\\n6. Use Class Weights:\\n   - Pass `class_weight=\\'balanced\\'` to classifiers like Logistic Regression or RandomForestClassifier.\\n\\n7. Threshold Tuning:\\n   - Adjust the decision threshold (default is 0.5) to increase sensitivity towards minority class.\\n\\nExample (in sklearn):\\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\\n\\ny_pred = model.predict(X_test)\\nprint(confusion_matrix(y_test, y_pred))\\nprint(classification_report(y_test, y_pred))\\nprint(\"ROC-AUC:\", roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))\\n\\nConclusion:\\nChoose evaluation metrics that reflect the cost of misclassification, especially focusing on recall and F1-score in medical applications.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
        "\n"
      ],
      "metadata": {
        "id": "pQ6Kjoh46AGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
        "\n",
        "To balance the dataset and ensure fair learning, especially when most customers report being satisfied, you can apply the following methods:\n",
        "\n",
        "1. **Down-Sampling the Majority Class:**\n",
        "   - Randomly remove samples from the majority class (e.g., 'satisfied') to match the number of minority class instances.\n",
        "\n",
        "   Example using Pandas and sklearn:\n",
        "   ----------------------------------\n",
        "   from sklearn.utils import resample\n",
        "   import pandas as pd\n",
        "\n",
        "   # Example DataFrame\n",
        "   df = pd.DataFrame({\n",
        "       'satisfaction': ['satisfied'] * 90 + ['unsatisfied'] * 10,\n",
        "       'score': list(range(100))\n",
        "   })\n",
        "\n",
        "   # Separate majority and minority\n",
        "   df_majority = df[df.satisfaction == 'satisfied']\n",
        "   df_minority = df[df.satisfaction == 'unsatisfied']\n",
        "\n",
        "   # Downsample majority class\n",
        "   df_majority_downsampled = resample(df_majority,\n",
        "                                      replace=False,     # without replacement\n",
        "                                      n_samples=len(df_minority),  # to match minority\n",
        "                                      random_state=42)\n",
        "\n",
        "   # Combine minority class with downsampled majority class\n",
        "   df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
        "\n",
        "   print(\"Balanced dataset:\\n\", df_balanced['satisfaction'].value_counts())\n",
        "\n",
        "2. **Other Techniques (if down-sampling isn’t ideal):**\n",
        "   - Up-Sampling the Minority Class: Duplicate or synthetically generate new samples (e.g., with SMOTE).\n",
        "   - Use of Class Weights: Adjust algorithm to penalize misclassification of minority class more heavily.\n",
        "   - Ensemble Techniques: Use ensemble models like BalancedRandomForest or EasyEnsembleClassifier.\n",
        "\n",
        "Conclusion:\n",
        "Down-sampling helps prevent model bias towards the majority class but may lead to loss of information. Always compare model performance using proper metrics (F1-score, recall) after balancing.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "uIgFu7AB6T6T",
        "outputId": "c7bd0ba6-c10a-4090-b519-e2197cd72be6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nQ10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\\n\\nTo balance the dataset and ensure fair learning, especially when most customers report being satisfied, you can apply the following methods:\\n\\n1. **Down-Sampling the Majority Class:**\\n   - Randomly remove samples from the majority class (e.g., \\'satisfied\\') to match the number of minority class instances.\\n\\n   Example using Pandas and sklearn:\\n   ----------------------------------\\n   from sklearn.utils import resample\\n   import pandas as pd\\n\\n   # Example DataFrame\\n   df = pd.DataFrame({\\n       \\'satisfaction\\': [\\'satisfied\\'] * 90 + [\\'unsatisfied\\'] * 10,\\n       \\'score\\': list(range(100))\\n   })\\n\\n   # Separate majority and minority\\n   df_majority = df[df.satisfaction == \\'satisfied\\']\\n   df_minority = df[df.satisfaction == \\'unsatisfied\\']\\n\\n   # Downsample majority class\\n   df_majority_downsampled = resample(df_majority, \\n                                      replace=False,     # without replacement\\n                                      n_samples=len(df_minority),  # to match minority\\n                                      random_state=42)\\n\\n   # Combine minority class with downsampled majority class\\n   df_balanced = pd.concat([df_majority_downsampled, df_minority])\\n\\n   print(\"Balanced dataset:\\n\", df_balanced[\\'satisfaction\\'].value_counts())\\n\\n2. **Other Techniques (if down-sampling isn’t ideal):**\\n   - Up-Sampling the Minority Class: Duplicate or synthetically generate new samples (e.g., with SMOTE).\\n   - Use of Class Weights: Adjust algorithm to penalize misclassification of minority class more heavily.\\n   - Ensemble Techniques: Use ensemble models like BalancedRandomForest or EasyEnsembleClassifier.\\n\\nConclusion:\\nDown-sampling helps prevent model bias towards the majority class but may lead to loss of information. Always compare model performance using proper metrics (F1-score, recall) after balancing.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "edwgDY296VWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\n",
        "\n",
        "To handle class imbalance when the rare event (minority class) is underrepresented, **up-sampling** the minority class can help balance the dataset. Below are effective techniques:\n",
        "\n",
        "1. **Random Over-Sampling:**\n",
        "   - Replicate samples from the minority class until it matches the size of the majority class.\n",
        "\n",
        "   Example:\n",
        "   --------\n",
        "   from sklearn.utils import resample\n",
        "   import pandas as pd\n",
        "\n",
        "   # Create an imbalanced dataset\n",
        "   df = pd.DataFrame({\n",
        "       'event': ['no'] * 90 + ['yes'] * 10,\n",
        "       'value': list(range(100))\n",
        "   })\n",
        "\n",
        "   # Separate minority and majority classes\n",
        "   df_majority = df[df.event == 'no']\n",
        "   df_minority = df[df.event == 'yes']\n",
        "\n",
        "   # Upsample minority class\n",
        "   df_minority_upsampled = resample(df_minority,\n",
        "                                    replace=True,              # Sample with replacement\n",
        "                                    n_samples=len(df_majority),# Match majority class\n",
        "                                    random_state=42)\n",
        "\n",
        "   # Combine majority and upsampled minority\n",
        "   df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "   print(\"Balanced dataset:\\n\", df_balanced['event'].value_counts())\n",
        "\n",
        "2. **SMOTE (Synthetic Minority Over-sampling Technique):**\n",
        "   - Generates synthetic samples for the minority class using k-nearest neighbors.\n",
        "\n",
        "   Example:\n",
        "   --------\n",
        "   from imblearn.over_sampling import SMOTE\n",
        "   from collections import Counter\n",
        "   from sklearn.datasets import make_classification\n",
        "\n",
        "   # Sample imbalanced data\n",
        "   X, y = make_classification(n_samples=100, n_features=2, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "   print(\"Original class distribution:\", Counter(y))\n",
        "\n",
        "   sm = SMOTE(random_state=42)\n",
        "   X_resampled, y_resampled = sm.fit_resample(X, y)\n",
        "\n",
        "   print(\"Resampled class distribution:\", Counter(y_resampled))\n",
        "\n",
        "3. **Use Class Weighting in Algorithms:**\n",
        "   - Specify `class_weight='balanced'` in classifiers like `LogisticRegression`, `RandomForestClassifier`, etc.\n",
        "\n",
        "Conclusion:\n",
        "Up-sampling prevents the model from being biased toward the majority class. Choose the method based on your dataset size and whether synthetic generation (like SMOTE) makes sense contextually.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "fE6ecV1W65bc",
        "outputId": "ed13d755-2acc-4c66-ee03-24cae485d39b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nQ11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\\n\\nTo handle class imbalance when the rare event (minority class) is underrepresented, **up-sampling** the minority class can help balance the dataset. Below are effective techniques:\\n\\n1. **Random Over-Sampling:**\\n   - Replicate samples from the minority class until it matches the size of the majority class.\\n\\n   Example:\\n   --------\\n   from sklearn.utils import resample\\n   import pandas as pd\\n\\n   # Create an imbalanced dataset\\n   df = pd.DataFrame({\\n       \\'event\\': [\\'no\\'] * 90 + [\\'yes\\'] * 10,\\n       \\'value\\': list(range(100))\\n   })\\n\\n   # Separate minority and majority classes\\n   df_majority = df[df.event == \\'no\\']\\n   df_minority = df[df.event == \\'yes\\']\\n\\n   # Upsample minority class\\n   df_minority_upsampled = resample(df_minority,\\n                                    replace=True,              # Sample with replacement\\n                                    n_samples=len(df_majority),# Match majority class\\n                                    random_state=42)\\n\\n   # Combine majority and upsampled minority\\n   df_balanced = pd.concat([df_majority, df_minority_upsampled])\\n\\n   print(\"Balanced dataset:\\n\", df_balanced[\\'event\\'].value_counts())\\n\\n2. **SMOTE (Synthetic Minority Over-sampling Technique):**\\n   - Generates synthetic samples for the minority class using k-nearest neighbors.\\n\\n   Example:\\n   --------\\n   from imblearn.over_sampling import SMOTE\\n   from collections import Counter\\n   from sklearn.datasets import make_classification\\n\\n   # Sample imbalanced data\\n   X, y = make_classification(n_samples=100, n_features=2, weights=[0.9, 0.1], random_state=42)\\n\\n   print(\"Original class distribution:\", Counter(y))\\n\\n   sm = SMOTE(random_state=42)\\n   X_resampled, y_resampled = sm.fit_resample(X, y)\\n\\n   print(\"Resampled class distribution:\", Counter(y_resampled))\\n\\n3. **Use Class Weighting in Algorithms:**\\n   - Specify `class_weight=\\'balanced\\'` in classifiers like `LogisticRegression`, `RandomForestClassifier`, etc.\\n\\nConclusion:\\nUp-sampling prevents the model from being biased toward the majority class. Choose the method based on your dataset size and whether synthetic generation (like SMOTE) makes sense contextually.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V8i6YDyl67KH"
      }
    }
  ]
}