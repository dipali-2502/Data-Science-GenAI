{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Ensemble Learning"
      ],
      "metadata": {
        "id": "BNlsPiEYRVY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Can we use Bagging for regression problems?\n",
        "\n",
        "ans. Yes, Bagging (Bootstrap Aggregating) can be used for regression problems.\n",
        "\n",
        "How it works in regression:\n",
        "Bagging creates multiple training datasets by randomly sampling (with replacement) from the original training data.\n",
        "\n",
        "A regression model, such as a Decision Tree Regressor, is trained on each of these datasets.\n",
        "\n",
        "The final prediction is made by averaging the predictions from all the individual models.\n"
      ],
      "metadata": {
        "id": "Os7cCww8Rhpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What is the difference between multiple model training and single model training?\n",
        "\n",
        "ans.\n",
        "\n",
        "**Single Model Training** uses one model for the entire task.\n",
        "\n",
        "* Simple, fast, easy to manage\n",
        "* May miss complex patterns\n",
        "\n",
        "**Multiple Model Training** uses several models (e.g., per segment or combined in ensembles).\n",
        "\n",
        "* More accurate, handles complex data\n",
        "* More complex and resource-heavy\n",
        "\n"
      ],
      "metadata": {
        "id": "PVnfqOA5SCBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the concept of feature randomness in Random Forest?\n",
        "\n",
        "ans.\n",
        "\n",
        " **Feature randomness in Random Forest** refers to the process of selecting a **random subset of features** at each split in the decision trees.\n",
        "\n",
        "### Why it's used:\n",
        "\n",
        "To introduce **diversity** among the trees, which reduces correlation between them and improves overall model accuracy and generalization.\n",
        "\n",
        "### How it works:\n",
        "\n",
        "* At each node split, instead of considering **all features**, Random Forest picks a **random subset** (usually √(total features) for classification).\n",
        "* The best split is chosen from this subset.\n",
        "\n",
        "### Benefit:\n",
        "\n",
        "* Reduces overfitting\n",
        "* Makes the ensemble more robust and less biased\n"
      ],
      "metadata": {
        "id": "wubTaFAaSjsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "ans. **OOB (Out-of-Bag) Score** is a way to **evaluate the performance** of a Random Forest model **without using a separate validation set**.\n",
        "\n",
        "### How it works:\n",
        "\n",
        "* In Random Forest, each tree is trained on a **bootstrap sample** (random sample with replacement) of the data.\n",
        "* About **one-third** of the data is **not included** in that sample — these are called **Out-of-Bag samples**.\n",
        "* The tree is tested on its OOB samples, and the error is recorded.\n",
        "\n",
        "### OOB Score:\n",
        "\n",
        "* It's the **average accuracy (or error)** of all trees on their respective OOB samples.\n",
        "* Acts like a built-in cross-validation score.\n",
        "\n",
        "### Benefit:\n",
        "\n",
        "* Saves time and data — no need for separate validation.\n",
        "* Gives a reliable estimate of model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "2D-hror1Syvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "ans. we can measure feature importance in a Random Forest model using two main methods:\n",
        "\n",
        "\n",
        "\n",
        "### 1. **Mean Decrease in Impurity (MDI)**\n",
        "\n",
        "* Measures how much each feature reduces impurity (e.g., Gini index) when used in tree splits.\n",
        "* Higher impurity reduction = more important feature.\n",
        "* Automatically computed by most libraries (e.g., `feature_importances_` in scikit-learn).\n",
        "\n",
        "\n",
        "\n",
        "### 2. **Permutation Importance (Mean Decrease in Accuracy)**\n",
        "\n",
        "* Randomly shuffle values of one feature and measure how much the model’s accuracy drops.\n",
        "* A larger drop means the feature is more important.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eKFVy_6IXjS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Explain the working principle of a Bagging Classifier?\n",
        "\n",
        "ans.\n",
        "\n",
        "### Bagging Classifier – Working Principle\n",
        "\n",
        "**Bagging** (Bootstrap Aggregating) is an ensemble method that improves model accuracy and reduces overfitting by training multiple models on random subsets of data.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. **Bootstrap Sampling**: Create multiple datasets by sampling the original data with replacement.\n",
        "2. **Train Models**: Fit a base learner (e.g., decision tree) on each dataset.\n",
        "3. **Aggregate Predictions**:\n",
        "\n",
        "   * **Classification**: Use majority voting.\n",
        "   * **Regression**: Take the average.\n",
        "\n"
      ],
      "metadata": {
        "id": "eb04Z2CLW5OE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  How do you evaluate a Bagging Classifier’s performance?\n",
        "\n",
        "ans.\n",
        "1. **Train-Test Split or Cross-Validation**\n",
        "\n",
        "   * Split the data into training and testing sets, or use k-fold cross-validation to assess performance reliably.\n",
        "\n",
        "2. **Performance Metrics** (for classification):\n",
        "\n",
        "   * **Accuracy**: Overall correctness.\n",
        "   * **Precision & Recall**: Useful for imbalanced data.\n",
        "   * **F1-Score**: Harmonic mean of precision and recall.\n",
        "   * **Confusion Matrix**: Shows true vs. predicted classes.\n",
        "   * **ROC-AUC Score**: Measures classification quality across thresholds.\n",
        "\n",
        "3. **Compare with Baseline**\n",
        "\n",
        "   * Compare against individual base learners or simpler models to see the benefit of bagging.\n",
        "\n",
        "4. **Check Variance Reduction**\n",
        "\n",
        "   * Bagging usually reduces model variance. Compare standard deviations of predictions if needed.\n",
        "\n",
        "5. **Out-of-Bag (OOB) Score**\n",
        "\n",
        "   * Built-in feature for Bagging: uses unused samples during bootstrapping to evaluate performance without separate test data.\n",
        "\n"
      ],
      "metadata": {
        "id": "N6mwN2U9Xdyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  How does a Bagging Regressor work?\n",
        "\n",
        "ans.\n",
        "\n",
        "### 1. **Bagging Classifier – Working Principle**\n",
        "\n",
        "* Trains multiple models on different random subsets (with replacement) of data.\n",
        "* Combines predictions by **majority vote** (for classification).\n",
        "* Reduces variance and overfitting.\n",
        "* Example: Random Forest.\n",
        "\n",
        "\n",
        "### 2. **Evaluating Bagging Classifier**\n",
        "\n",
        "* Use **train-test split** or **cross-validation**.\n",
        "* Metrics: **Accuracy**, **Precision**, **Recall**, **F1-score**, **ROC-AUC**.\n",
        "* Use **OOB score** for built-in evaluation.\n",
        "* Compare with single model to see improvement.\n",
        "\n",
        "\n",
        "### 3. **Bagging Regressor – Working**\n",
        "\n",
        "* Trains multiple regressors on bootstrapped data.\n",
        "* Final prediction = **average** of all outputs.\n",
        "* Reduces overfitting, improves stability.\n",
        "* Works well with high-variance models.\n"
      ],
      "metadata": {
        "id": "i8H_2yzaX0a5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  What is the main advantage of ensemble techniques?\n",
        "\n",
        "ans. ### Main Advantage of Ensemble Techniques (Brief Explanation)\n",
        "\n",
        "The main advantage is that they **combine multiple models** to make more **accurate and reliable predictions** than any single model alone.\n",
        "\n",
        "* **Reduce Variance**: Methods like **Bagging** (e.g., Random Forest) help avoid overfitting by averaging predictions.\n",
        "* **Reduce Bias**: Methods like **Boosting** (e.g., AdaBoost) focus on errors and improve weak models.\n",
        "* **More Robust**: Ensemble models are less sensitive to noise and changes in data.\n",
        "\n",
        "**Result**: Higher accuracy, better generalization, and more stable predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "GuRHrLnFYOrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  What is the main challenge of ensemble methods?\n",
        "\n",
        "ans.  Main Challenge of Ensemble Methods\n",
        "\n",
        "The main challenge is **increased complexity**.\n",
        "\n",
        "* **Hard to Interpret**: Combining many models makes the final output less transparent.\n",
        "* **High Computational Cost**: Training multiple models takes more time and resources.\n",
        "* **Risk of Overfitting** (in some ensembles like boosting, if not tuned properly).\n",
        "* **Difficult to Tune**: More hyperparameters and interactions to manage.\n",
        "\n",
        "In short: better performance, but at the cost of **complexity and interpretability**.\n",
        "\n"
      ],
      "metadata": {
        "id": "Tsu1UzTRYpf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Explain the key idea behind ensemble techniques?\n",
        "\n",
        "ans.  Key Idea Behind Ensemble Techniques\n",
        "\n",
        "The key idea of ensemble techniques is to **combine multiple weak models** to create a stronger overall model. This approach improves the model’s performance by leveraging the strengths of each individual model and reducing their individual weaknesses.\n",
        "\n",
        "* **Diversification**: Different models make different errors, so combining them reduces overall error.\n",
        "* **Aggregation**: Use methods like **voting** (for classification) or **averaging** (for regression) to combine predictions from multiple models.\n",
        "* **Improved Accuracy**: The ensemble is typically more accurate, stable, and generalizable than a single model.\n",
        "\n"
      ],
      "metadata": {
        "id": "LjdfwzuaY2EP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is a Random Forest Classifier?\n",
        "\n",
        "ans. Random Forest Classifier\n",
        "\n",
        "A **Random Forest Classifier** is an ensemble learning method that combines multiple decision trees to improve classification accuracy.\n",
        "\n",
        "#### Key Features:\n",
        "\n",
        "* **Bootstrap Sampling**: Multiple datasets are created by sampling with replacement from the original data.\n",
        "* **Random Feature Selection**: During each split in a decision tree, a random subset of features is considered.\n",
        "* **Majority Voting**: The final classification is determined by majority voting from all trees in the forest.\n",
        "\n",
        "#### Advantages:\n",
        "\n",
        "* **Reduces Overfitting**: Aggregates the predictions of many trees, making it less prone to overfitting.\n",
        "* **Handles High Variance**: Works well for complex datasets with high variance.\n",
        "* **Robust and Accurate**: Performs well in various types of data without heavy tuning.\n",
        "\n",
        "In short, a **Random Forest Classifier** is a powerful model that builds multiple decision trees and combines their results to make a more accurate and robust prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "OJto-NuBeSrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are the main types of ensemble techniques?\n",
        "\n",
        "ans.  Main Types of Ensemble Techniques\n",
        "\n",
        "1. **Bagging (Bootstrap Aggregating)**\n",
        "\n",
        "   * **Idea**: Trains multiple models (typically the same type) on different random subsets of the data (with replacement).\n",
        "   * **Example**: Random Forest.\n",
        "   * **Goal**: Reduces variance and overfitting.\n",
        "\n",
        "2. **Boosting**\n",
        "\n",
        "   * **Idea**: Trains models sequentially, each model corrects the errors of the previous one by giving more weight to misclassified instances.\n",
        "   * **Example**: AdaBoost, Gradient Boosting.\n",
        "   * **Goal**: Reduces bias and improves prediction accuracy.\n",
        "\n",
        "3. **Stacking**\n",
        "\n",
        "   * **Idea**: Combines different types of models (base models) and uses a meta-model to combine their predictions.\n",
        "   * **Example**: Using a logistic regression model to combine outputs from decision trees, SVMs, etc.\n",
        "   * **Goal**: Leverages the strengths of multiple diverse models.\n",
        "\n",
        "4. **Voting**\n",
        "\n",
        "   * **Idea**: Combines the predictions of multiple models by taking a majority vote (for classification) or average (for regression).\n",
        "   * **Example**: Hard Voting (majority class wins) and Soft Voting (probabilities averaged).\n",
        "   * **Goal**: Combines the diverse outputs from multiple models.\n",
        "\n",
        "These methods are used to improve model performance by combining multiple weak learners into a stronger, more accurate model.\n",
        "\n"
      ],
      "metadata": {
        "id": "yyudGglSew7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is ensemble learning in machine learning?\n",
        "\n",
        "ans.\n",
        "### Ensemble Learning in Machine Learning\n",
        "\n",
        "**Ensemble learning** is a technique where multiple models (often called \"learners\") are combined to solve a problem and improve performance compared to a single model.\n",
        "\n",
        "#### Key Ideas:\n",
        "\n",
        "* **Combining Predictions**: Multiple models are trained independently, and their predictions are aggregated (e.g., by voting for classification or averaging for regression).\n",
        "* **Improves Accuracy**: The combined model tends to perform better by reducing errors and improving stability.\n",
        "* **Reduces Overfitting**: Ensemble methods like Bagging (e.g., Random Forest) reduce variance, while Boosting reduces bias.\n",
        "\n",
        "#### Common Ensemble Methods:\n",
        "\n",
        "* **Bagging**: Uses multiple models trained on different subsets of the data (e.g., Random Forest).\n",
        "* **Boosting**: Models are trained sequentially, each focusing on the mistakes made by previous ones (e.g., AdaBoost, Gradient Boosting).\n",
        "* **Stacking**: Combines different types of models, using a meta-model to make the final prediction.\n",
        "\n",
        "In short, **ensemble learning** leverages the power of multiple models to make more robust, accurate predictions.\n"
      ],
      "metadata": {
        "id": "7de2eHdJfFqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should we avoid using ensemble methods?\n",
        "\n",
        "ans.  When to Avoid Using Ensemble Methods:\n",
        "\n",
        "1. **High Computational Cost**:\n",
        "\n",
        "   * If you have limited resources or need real-time predictions, the computational overhead of training and maintaining multiple models can be prohibitive.\n",
        "\n",
        "2. **Simple Problems**:\n",
        "\n",
        "   * For relatively simple problems where a single model (like a decision tree or linear regression) already performs well, using an ensemble method may not provide significant improvements and can unnecessarily complicate the solution.\n",
        "\n",
        "3. **Interpretability Concerns**:\n",
        "\n",
        "   * If interpretability is a key requirement (e.g., in some regulatory or medical applications), ensemble methods like Random Forests or Boosting may be difficult to explain due to their complex structure.\n",
        "\n",
        "4. **Small Datasets**:\n",
        "\n",
        "   * Ensemble methods can lead to overfitting when used with very small datasets, especially if the individual models are already highly complex.\n",
        "\n",
        "5. **Imbalanced Datasets**:\n",
        "\n",
        "   * If the dataset is highly imbalanced, ensemble methods may still favor the majority class. Proper techniques like sampling or adjusting class weights should be considered.\n",
        "\n",
        "In these cases, simpler models might be more appropriate.\n",
        "\n"
      ],
      "metadata": {
        "id": "7WbGvIZWfiyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How does Bagging help in reducing overfitting?\n",
        "\n",
        "\n",
        "ans. How Bagging Helps in Reducing Overfitting\n",
        "\n",
        "**Bagging** (Bootstrap Aggregating) reduces overfitting by combining predictions from multiple models trained on different random subsets of the data. Here’s how it helps:\n",
        "\n",
        "1. **Averaging/Minority Voting**:\n",
        "\n",
        "   * By aggregating the predictions (averaging for regression, majority voting for classification), Bagging smooths out the individual errors of each model, leading to more stable and generalizable predictions.\n",
        "\n",
        "2. **Reduces Variance**:\n",
        "\n",
        "   * Overfitting occurs when a model becomes too sensitive to noise in the training data. Bagging reduces the variance of individual models by training them on different random subsets of data, so the model as a whole is less likely to overfit.\n",
        "\n",
        "3. **Training on Random Subsets**:\n",
        "\n",
        "   * Since each model is trained on a different subset of the data, each one has a slightly different view, reducing the likelihood that any one model will fit noise or outliers in the data.\n",
        "\n",
        "4. **Parallel Training**:\n",
        "\n",
        "   * Training multiple models in parallel means that the final ensemble is less likely to overfit the data compared to a single, overtrained model.\n",
        "\n"
      ],
      "metadata": {
        "id": "Iv00Gs3cf9VW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "ans.  Why Random Forest is Better than a Single Decision Tree\n",
        "\n",
        "1. **Reduces Overfitting**:\n",
        "\n",
        "   * A single decision tree is prone to overfitting, especially with complex data. Random Forest reduces this by averaging multiple trees, which reduces the model's variance.\n",
        "\n",
        "2. **Improved Accuracy**:\n",
        "\n",
        "   * Random Forest combines the predictions of many trees, leading to better overall accuracy. Each tree might make different errors, but the aggregation of results tends to give a more accurate prediction.\n",
        "\n",
        "3. **Handles Complex Data**:\n",
        "\n",
        "   * Random Forest is more robust and can handle a wide range of data, including noisy and missing data, better than a single decision tree.\n",
        "\n",
        "4. **More Robust to Outliers**:\n",
        "\n",
        "   * A single decision tree can be overly influenced by outliers, but Random Forest averages over many trees, making it less sensitive to outliers.\n",
        "\n",
        "5. **Feature Importance**:\n",
        "\n",
        "   * Random Forest can provide insights into feature importance, whereas a single decision tree might not give as detailed or reliable feature ranking.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SbujQJJmgP9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        "ans. Role of Bootstrap Sampling in Bagging\n",
        "\n",
        "**Bootstrap sampling** is a key technique in Bagging (Bootstrap Aggregating) that plays a crucial role in improving model performance. Here's how it works:\n",
        "\n",
        "1. **Create Multiple Training Sets**:\n",
        "\n",
        "   * Randomly select subsets of the original data with **replacement** (i.e., some instances may appear multiple times, while others may be missing). This is known as **bootstrap sampling**.\n",
        "\n",
        "2. **Independence in Models**:\n",
        "\n",
        "   * Each subset is used to train a different model (typically the same type, like decision trees). These models are trained independently of each other.\n",
        "\n",
        "3. **Reduces Overfitting**:\n",
        "\n",
        "   * By training on different data subsets, each model is less likely to overfit on noise or outliers in the data. The diversity among models helps reduce overfitting in the final ensemble.\n",
        "\n",
        "4. **Enhances Accuracy**:\n",
        "\n",
        "   * The aggregated predictions (via voting or averaging) from these different models lead to more stable and accurate predictions than any individual model, especially when the data is noisy.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MqmvDx1Lgqju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What are some real-world applications of ensemble techniques?\n",
        "\n",
        "ans.  Real-World Applications of Ensemble Techniques\n",
        "\n",
        "1. **Financial Sector**:\n",
        "\n",
        "   * **Credit Scoring**: Predicting loan defaults using models like Random Forest or Gradient Boosting to improve accuracy and reduce risk.\n",
        "   * **Stock Price Prediction**: Combining multiple models to forecast market trends and stock prices.\n",
        "\n",
        "2. **Healthcare**:\n",
        "\n",
        "   * **Disease Prediction**: Ensemble methods like Random Forest are used to predict diseases based on patient data, improving accuracy and robustness in diagnosis.\n",
        "   * **Medical Imaging**: Combining various image classification models to detect anomalies like tumors in radiology scans.\n",
        "\n",
        "3. **E-commerce**:\n",
        "\n",
        "   * **Recommendation Systems**: Using ensemble models to provide better product recommendations by combining predictions from different algorithms (e.g., collaborative filtering + content-based filtering).\n",
        "   * **Customer Churn Prediction**: Predicting customer churn with higher precision by using ensemble methods.\n",
        "\n",
        "4. **Natural Language Processing (NLP)**:\n",
        "\n",
        "   * **Sentiment Analysis**: Combining predictions from multiple classifiers (e.g., SVM, Naive Bayes) for better sentiment analysis of reviews or social media posts.\n",
        "   * **Spam Detection**: Using ensemble techniques to classify emails as spam or not spam with higher accuracy.\n",
        "\n",
        "5. **Image and Speech Recognition**:\n",
        "\n",
        "   * **Face Recognition**: Improving face recognition accuracy by combining multiple models, such as Convolutional Neural Networks (CNNs) with other classifiers.\n",
        "   * **Speech-to-Text**: Combining different machine learning models for more accurate speech recognition systems.\n",
        "\n",
        "6. **Fraud Detection**:\n",
        "\n",
        "   * **Credit Card Fraud**: Ensemble models, like Random Forest or XGBoost, help identify fraudulent transactions by combining results from various classifiers.\n",
        "\n",
        "7. **Weather Forecasting**:\n",
        "\n",
        "   * Combining predictions from different weather models (ensemble forecasting) improves prediction accuracy in areas like temperature and precipitation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xLZyp04ihELt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  What is the difference between Bagging and Boosting?\n",
        "\n",
        "\n",
        "ans.\n",
        "\n",
        "### **Difference Between Bagging and Boosting**\n",
        "\n",
        "1. **Approach**:\n",
        "\n",
        "   * **Bagging**: Bagging stands for **Bootstrap Aggregating**. It uses parallel training of multiple models independently. Each model is trained on a random subset of the training data (with replacement). The final prediction is made by aggregating the predictions from all individual models.\n",
        "   * **Boosting**: Boosting is a sequential method where each model is trained one after the other. Each new model focuses on the errors made by the previous models, thus correcting them. The models in boosting are combined by weighted voting or summing.\n",
        "\n",
        "2. **Model Training**:\n",
        "\n",
        "   * **Bagging**: In bagging, each model is trained independently, typically using the same algorithm (e.g., decision trees), but on different bootstrapped subsets of the data. The idea is to reduce variance by combining many models.\n",
        "   * **Boosting**: In boosting, models are trained sequentially. Each new model tries to correct the errors made by the previous model, putting more weight on misclassified instances.\n",
        "\n",
        "3. **Goal**:\n",
        "\n",
        "   * **Bagging**: The primary goal of bagging is to reduce **variance** (i.e., overfitting). By averaging or voting over multiple models, the randomness in individual models is reduced, resulting in a more stable and generalized model.\n",
        "   * **Boosting**: Boosting's goal is to reduce **bias** (i.e., underfitting). It improves the model's accuracy by focusing on the mistakes made by previous models, resulting in stronger models that improve over time.\n",
        "\n",
        "4. **Data Subsets**:\n",
        "\n",
        "   * **Bagging**: Uses **bootstrap sampling**, meaning it generates multiple subsets of data by randomly sampling from the original dataset with replacement.\n",
        "   * **Boosting**: Uses the **entire dataset** for each model, but assigns more weight to the misclassified instances from previous models, ensuring that errors are corrected in the next iteration.\n",
        "\n",
        "5. **Aggregation**:\n",
        "\n",
        "   * **Bagging**: The predictions from multiple models are aggregated by **majority voting** (for classification) or **averaging** (for regression) to make the final prediction.\n",
        "   * **Boosting**: The predictions are combined by **weighted voting** or summing, where each model's contribution is weighted based on its accuracy.\n",
        "\n",
        "6. **Example Algorithms**:\n",
        "\n",
        "   * **Bagging**: Common algorithms include **Random Forest** and **Bagged Decision Trees**.\n",
        "   * **Boosting**: Common algorithms include **AdaBoost**, **Gradient Boosting**, and **XGBoost**.\n",
        "\n",
        "7. **Impact on Errors**:\n",
        "\n",
        "   * **Bagging**: Bagging mainly reduces **variance**, making it effective for models that are prone to overfitting (e.g., decision trees).\n",
        "   * **Boosting**: Boosting primarily reduces **bias**, making it effective for models that are underfitting and improving the accuracy of weak models.\n",
        "\n",
        "8. **Parallelization**:\n",
        "\n",
        "   * **Bagging**: Since the models are trained independently, bagging can be easily **parallelized** for faster computation.\n",
        "   * **Boosting**: Boosting cannot be fully parallelized because models are trained sequentially, where each model depends on the previous one.\n",
        "\n"
      ],
      "metadata": {
        "id": "_NBD4XU8hrYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Practical"
      ],
      "metadata": {
        "id": "IwdFp-_OibPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.  Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy?\n"
      ],
      "metadata": {
        "id": "_pkp21Kyiim2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Tree as the base estimator\n",
        "bagging_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNL1q1yUkPXs",
        "outputId": "d75afb57-7777-494f-94d7-80113fd8e077"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.  Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores?\n"
      ],
      "metadata": {
        "id": "Z81-V9kpkVEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the feature importance scores\n",
        "feature_importances = rf_clf.feature_importances_\n",
        "print(\"Feature Importance Scores:\")\n",
        "for feature, importance in zip(data.feature_names, feature_importances):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4to2wqrpkcpG",
        "outputId": "eae7f2fd-2da1-485e-f4e0-023a23697cef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 97.08%\n",
            "Feature Importance Scores:\n",
            "mean radius: 0.0323\n",
            "mean texture: 0.0111\n",
            "mean perimeter: 0.0601\n",
            "mean area: 0.0538\n",
            "mean smoothness: 0.0062\n",
            "mean compactness: 0.0092\n",
            "mean concavity: 0.0806\n",
            "mean concave points: 0.1419\n",
            "mean symmetry: 0.0033\n",
            "mean fractal dimension: 0.0031\n",
            "radius error: 0.0164\n",
            "texture error: 0.0032\n",
            "perimeter error: 0.0118\n",
            "area error: 0.0295\n",
            "smoothness error: 0.0059\n",
            "compactness error: 0.0046\n",
            "concavity error: 0.0058\n",
            "concave points error: 0.0034\n",
            "symmetry error: 0.0040\n",
            "fractal dimension error: 0.0071\n",
            "worst radius: 0.0780\n",
            "worst texture: 0.0188\n",
            "worst perimeter: 0.0743\n",
            "worst area: 0.1182\n",
            "worst smoothness: 0.0118\n",
            "worst compactness: 0.0175\n",
            "worst concavity: 0.0411\n",
            "worst concave points: 0.1271\n",
            "worst symmetry: 0.0129\n",
            "worst fractal dimension: 0.0069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree?\n"
      ],
      "metadata": {
        "id": "hWYlsi84klDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target variable (house values)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Create and train a single Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set for both models\n",
        "y_pred_rf = rf_regressor.predict(X_test)\n",
        "y_pred_dt = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE) for both models\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Print the MSE for both models\n",
        "print(f'Mean Squared Error of Random Forest Regressor: {mse_rf:.4f}')\n",
        "print(f'Mean Squared Error of Decision Tree Regressor: {mse_dt:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGh5Lcj8kp34",
        "outputId": "d96961b4-59e3-4a76-96ec-8322d46ca085"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of Random Forest Regressor: 0.2565\n",
            "Mean Squared Error of Decision Tree Regressor: 0.5280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier?\n"
      ],
      "metadata": {
        "id": "NOWhZBh_lPdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset (optional, just to match general workflow)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier with OOB scoring enabled\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, bootstrap=True, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the Out-of-Bag score\n",
        "print(f'OOB Score: {rf_clf.oob_score_:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnDItNYVlVTM",
        "outputId": "2951947f-8271-4f16-ba71-db6ee47b9fde"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.9548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a Bagging Classifier using SVM as a base estimator and print accuracy?\n"
      ],
      "metadata": {
        "id": "jH5n47H5lg4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with SVM as the base estimator\n",
        "bagging_svm = BaggingClassifier(estimator=SVC(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NhHOWLLlmvh",
        "outputId": "08b9ba7c-323e-4bc3-8ec2-f60eee9dbc46"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Train a Random Forest Classifier with different numbers of trees and compare accuracy?\n"
      ],
      "metadata": {
        "id": "RAB1tYO7l1mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define different numbers of trees to compare\n",
        "tree_counts = [1, 5, 10, 50, 100]\n",
        "print(\"Number of Trees | Accuracy\")\n",
        "\n",
        "# Train and evaluate for each tree count\n",
        "for n in tree_counts:\n",
        "    clf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{n:>15} | {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4mYsPeol89A",
        "outputId": "239eec03-6197-402f-aeaa-11293bd2165d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Trees | Accuracy\n",
            "              1 | 1.0000\n",
            "              5 | 1.0000\n",
            "             10 | 1.0000\n",
            "             50 | 1.0000\n",
            "            100 | 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score?\n"
      ],
      "metadata": {
        "id": "ATDS_0JgmF-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create Bagging Classifier with Logistic Regression\n",
        "bagging_lr = BaggingClassifier(estimator=LogisticRegression(max_iter=1000), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train model\n",
        "bagging_lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for AUC\n",
        "y_prob = bagging_lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute and print AUC score\n",
        "auc_score = roc_auc_score(y_test, y_prob)\n",
        "print(f'AUC Score: {auc_score:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FgqIYNynMWu",
        "outputId": "d05b5a34-03af-415c-9daa-a95a66dcd17e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score: 0.9978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a Random Forest Regressor and analyze feature importance scores?\n"
      ],
      "metadata": {
        "id": "doA5ToC7oBuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = rf_regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "# Analyze feature importance\n",
        "importance = rf_regressor.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importance\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNoNg_8VpaVt",
        "outputId": "02b96e76-47ed-4133-b992-104b74042b2d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.2565\n",
            "\n",
            "Feature Importances:\n",
            "      Feature  Importance\n",
            "0      MedInc    0.526011\n",
            "5    AveOccup    0.138220\n",
            "7   Longitude    0.086124\n",
            "6    Latitude    0.086086\n",
            "1    HouseAge    0.054654\n",
            "2    AveRooms    0.047188\n",
            "4  Population    0.031722\n",
            "3   AveBedrms    0.029995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.  Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n"
      ],
      "metadata": {
        "id": "c5aj5HJDpwp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Classifier with Decision Tree\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_acc:.4f}\")\n",
        "print(f\"Random Forest Accuracy:     {rf_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "259Ygn7yqJHt",
        "outputId": "c6baf4c7-1e4c-44a1-e79c-243a05a74bf0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 1.0000\n",
            "Random Forest Accuracy:     1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV?\n"
      ],
      "metadata": {
        "id": "wVDAF2Lfq7Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Create the model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model prediction\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Print best params and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Test Accuracy: {:.4f}\".format(accuracy_score(y_test, y_pred)))\n"
      ],
      "metadata": {
        "id": "21Lsh01UsKaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a Bagging Regressor with different numbers of base estimators and compare performance?\n"
      ],
      "metadata": {
        "id": "xQAiY86fsOgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Try different numbers of estimators\n",
        "estimators_list = [1, 5, 10, 50, 100]\n",
        "print(\"Estimators | MSE\")\n",
        "\n",
        "for n in estimators_list:\n",
        "    model = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=n,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"{n:10} | {mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArtdqPzRsS56",
        "outputId": "9218e5a7-6ed0-4bdd-ca99-2a25b7e2fcf4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimators | MSE\n",
            "         1 | 0.5583\n",
            "         5 | 0.3168\n",
            "        10 | 0.2862\n",
            "        50 | 0.2579\n",
            "       100 | 0.2568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Train a Random Forest Classifier and analyze misclassified samples?\n"
      ],
      "metadata": {
        "id": "MP-1qkv8tPix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Analyze misclassified samples\n",
        "misclassified = X_test[y_test != y_pred]\n",
        "true_labels = y_test[y_test != y_pred]\n",
        "predicted_labels = y_pred[y_test != y_pred]\n",
        "\n",
        "# Display misclassified samples\n",
        "print(\"\\nMisclassified Samples:\")\n",
        "for i in range(len(misclassified)):\n",
        "    print(f\"Sample: {misclassified[i]}, True: {target_names[true_labels[i]]}, Predicted: {target_names[predicted_labels[i]]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqyOD5DmtfZ2",
        "outputId": "cd39baa3-8be2-4b84-df0c-ae2b88e258ba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        19\n",
            "  versicolor       1.00      1.00      1.00        13\n",
            "   virginica       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "\n",
            "Misclassified Samples:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier?\n"
      ],
      "metadata": {
        "id": "yyCGxzBTthVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Bagging Classifier with Decision Trees\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Compare results\n",
        "print(f\"Decision Tree Accuracy: {dt_acc:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "P3W-e0sPtnGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a Random Forest Classifier and visualize the confusion matrix?\n"
      ],
      "metadata": {
        "id": "lGr1spuatxxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "target_names = data.target_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Random Forest Classifier - Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "viSR-6Lit5kc",
        "outputId": "5196e161-9e38-43f8-f859-122afed6cc96"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "36.  Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy?\n"
      ],
      "metadata": {
        "id": "StoBlkNbuDMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base learners\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(kernel='linear', random_state=42)),\n",
        "    ('lr', LogisticRegression(random_state=42))\n",
        "]\n",
        "\n",
        "# Create Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "\n",
        "# Train the model\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSmCcLxoufSh",
        "outputId": "5690d42c-f7f7-4684-9b5f-f1f4078b5bb0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "37.Train a Random Forest Classifier and print the top 5 most important features?\n"
      ],
      "metadata": {
        "id": "v6kv_fzWuiEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display feature importances\n",
        "features_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the features by importance in descending order and print the top 5\n",
        "top_5_features = features_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "print(top_5_features)\n"
      ],
      "metadata": {
        "id": "NaqHYmbgu5I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score?\n"
      ],
      "metadata": {
        "id": "XHNR3a0tvCDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train Bagging Classifier with Decision Trees as base estimators\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Precision, Recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "cZ32NH1mvITa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39.  Train a Random Forest Classifier and analyze the effect of max_depth on accuracy?\n"
      ],
      "metadata": {
        "id": "gI4w1tdwvbrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# List of different max_depth values to test\n",
        "max_depth_values = [None, 2, 3, 5, 7, 10, 15, 20]\n",
        "\n",
        "# Store accuracies for each max_depth\n",
        "accuracies = []\n",
        "\n",
        "# Loop through each max_depth value, train the model, and evaluate accuracy\n",
        "for depth in max_depth_values:\n",
        "    rf = RandomForestClassifier(max_depth=depth, n_estimators=100, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Plot the effect of max_depth on accuracy\n",
        "plt.plot(max_depth_values, accuracies, marker='o', linestyle='-', color='b')\n",
        "plt.title('Effect of max_depth on Random Forest Accuracy')\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(max_depth_values)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print accuracies\n",
        "for depth, acc in zip(max_depth_values, accuracies):\n",
        "    print(f\"max_depth = {depth}, Accuracy = {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "2RtiGtagvjip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40.  Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance?\n"
      ],
      "metadata": {
        "id": "UOgMP7RRvzR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor with DecisionTreeRegressor as base estimator\n",
        "bagging_dt = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "\n",
        "# Train Bagging Regressor with KNeighborsRegressor as base estimator\n",
        "bagging_knn = BaggingRegressor(base_estimator=KNeighborsRegressor(), n_estimators=50, random_state=42)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_dt = bagging_dt.predict(X_test)\n",
        "y_pred_knn = bagging_knn.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error for both models\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean Squared Error (Decision Tree as base estimator): {mse_dt:.4f}\")\n",
        "print(f\"Mean Squared Error (KNeighbors as base estimator): {mse_knn:.4f}\")\n"
      ],
      "metadata": {
        "id": "L9TMwO3XwNXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score?\n"
      ],
      "metadata": {
        "id": "w_7nc0PGwOWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Binarize the target labels for multi-class ROC-AUC calculation\n",
        "lb = LabelBinarizer()\n",
        "y_bin = lb.fit_transform(y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the test set\n",
        "y_prob = rf.predict_proba(X_test)\n",
        "\n",
        "# Calculate ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
        "\n",
        "# Print ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "uIfkz_PbwkT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a Bagging Classifier and evaluate its performance using cross-validatio?\n"
      ],
      "metadata": {
        "id": "2dviM7p-w4eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Create a Bagging Classifier with DecisionTree as base estimator\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Evaluate using cross-validation\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring='accuracy')  # 5-fold cross-validation\n",
        "\n",
        "# Print cross-validation scores and the mean score\n",
        "print(f\"Cross-validation scores: {cv_scores}\")\n",
        "print(f\"Mean cross-validation accuracy: {np.mean(cv_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "ed_Q6FNCw-Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43.  Train a Random Forest Classifier and plot the Precision-Recall curv?\n"
      ],
      "metadata": {
        "id": "KaNB1ZX0xMbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Convert multi-class to binary for Precision-Recall curve (choose one class for binary classification)\n",
        "# We'll classify class 0 against the rest for simplicity\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the test set\n",
        "y_probs = rf.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Compute precision and recall\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.plot(recall, precision, marker='.', color='b')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HfElKY5ZxRrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy?\n"
      ],
      "metadata": {
        "id": "rZCa8BoJxdRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base models (Random Forest and Logistic Regression)\n",
        "base_learners = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=200, random_state=42))\n",
        "]\n",
        "\n",
        "# Create Stacking Classifier with Logistic Regression as the final estimator\n",
        "stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
        "\n",
        "# Train the Stacking Classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "-UQXiQ1MxjgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance?\n"
      ],
      "metadata": {
        "id": "Twe6NdgLxrqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_boston()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different levels of bootstrap samples (e.g., 50%, 70%, and 100%)\n",
        "bootstrap_samples = [0.5, 0.7, 1.0]\n",
        "errors = []\n",
        "\n",
        "# Train a Bagging Regressor with different levels of bootstrap samples\n",
        "for sample in bootstrap_samples:\n",
        "    # Create a Bagging Regressor with DecisionTree as the base estimator\n",
        "    bagging_regressor = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=50,\n",
        "        max_samples=sample,  # Adjust bootstrap sample size\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "    # Calculate the mean squared error (MSE)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    errors.append(mse)\n",
        "\n",
        "# Plot MSE vs. Bootstrap sample size\n",
        "plt.plot(bootstrap_samples, errors, marker='o')\n",
        "plt.title('MSE vs. Bootstrap Sample Size')\n",
        "plt.xlabel('Bootstrap Sample Size')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print the errors for each sample size\n",
        "for sample, error in zip(bootstrap_samples, errors):\n",
        "    print(f\"Bootstrap sample size: {sample}, MSE: {error:.4f}\")\n"
      ],
      "metadata": {
        "id": "efWP9giix20Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FTUv4501jDpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WV2o3tAjWvFK"
      }
    }
  ]
}